[
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/ansible-playbooks-beyond-the-basics/",
	"title": "Ansible Playbooks Beyond the Basics",
	"tags": ["ansible", "ansible-for-devops", "playbook"],
	"description": "",
	"content": "playbook과 우리가 이전에 사용한 간단한 playbook organization은 많은 일반적인 use case를 커버할 수 있다. system administration이 필요로 하는 폭에 대해 얘기하자면 Ansible에는 우리가 알아야 할 수많은 기능들이 있다. 우리는 play를 어떻게 더 세분화하여 실행하는지, simplicity와 usability를 위해 어떻게 task와 playbook을 구성하는지, infrastructure를 더 자신있게 다룰 수 있도록 해주는 다른 advanced playbook topic에 대해 다루어 볼 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/jenkins/install/",
	"title": "Jenkins Install",
	"tags": ["jenkins", "install"],
	"description": "",
	"content": "Configure docker-compose.yaml 다음과 같이 docker-compose.yaml 파일을 적절한 디렉토리에 생성합니다.\nversion: \u0026#39;2\u0026#39; services: jenkins: image: \u0026#39;jenkins/jenkins:lts\u0026#39; ports: - \u0026#39;38080:8080\u0026#39; - \u0026#39;38443:8443\u0026#39; - \u0026#39;50000:50000\u0026#39; volumes: - \u0026#39;jenkins_data:/var/jenkins_home\u0026#39; volumes: jenkins_data: driver: local driver_opts: type: none device: $PWD/jenkins_data o: bind jenkins_data는 jenkins가 사용할 데이터들입니다. 이를 local에 폴더로 만들어줍니다.\nmkdir jenkins_data Start Jenkins 이제 docker-compose 명령어를 통해 실행합니다.\nsudo docker-compose up -d http://$IP:38080 으로 접속할 수 있습니다. 로컬에 설치하셨다면 http://localhost:38080으로 접속하면 됩니다.\nJenkins 세팅 처음에 다음과 같은 화면을 볼 수 있습니다.\n여기서 /var/jenkins_home/secrets/initialAdminPassword의 내용을 아래 칸에 입려하라고 지시합니다.\n저희는 앞서 $PWD/jenkins_data를 /var/jenkins_home에 binding했으므로, $PWD/jenkins_data/secrets/initialAdminPassword를 확인하면 됩니다. 또는 docker container에 직접 접속해서 해당 path로 이동 후 값을 확인해도 됩니다.\n이제 다음과 같은 화면이 뜰 것입니다.\n여기서는 왼쪽의 Install suggested plugins를 선택해줍니다.\n다음과 같이 설치가 진행됩니다.\n위의 plugin 설치과정이 완료되고 나면 다음과 같은 admin 계정 생성에 관한 화면이 나옵니다.\n알맞게 입력을 한 뒤 저장하면 접속 URL을 작성하라고 합니다. default로 적어져 있으니 확인후 변경이 필요하면 바꾸면 됩니다.\n여기까지 하면 기본 설정은 다 끝났습니다. Jenkins가 준비될 때까지 잠시 기다리면 home 화면이 뜹니다.\nReference  https://hub.docker.com/_/jenkins/  "
},
{
	"uri": "http://kimmj.github.io/english/himym/season1/episode01/",
	"title": "Episode01",
	"tags": [],
	"description": "",
	"content": "Generated by Language Learning with Netflix\n   Eng Kor     OLDER TED: Kids, I'm gonna tell you an incredible story. 얘들아, 아빠가 굉장한 얘기를 해 주마   The story of how I met your mother. 어떻게 네 엄마를 만났는지 말이야   We being punished for something? OLDER TED: No. - 저희 벌받는 거예요? - 아니   Yeah, is this gonna take a while? 오래 걸려요?   OLDER TED: Yes. Twenty-five years ago, before I was Dad, I had this whole other life. 응. 25년 전, 아빠가 되기 전에 완전히 다른 삶을 살았어   It was way back in 2005. 오래전, 2005년이었어   I was 27, just starting to make it as an architect 스물일곱의 난 막 건축가로 자리 잡고   and living in New York with Marshall, my best friend from college. 대학 때 절친 마셜과 뉴욕에서 살고 있었지   My life was good. And then Uncle Marshall went and screwed the whole thing up. 잘 살고 있었는데 마셜 삼촌이 일을 다 망쳤어   Will you marry me? 나랑 결혼해 줄래?   Yes. Perfect. And then you're engaged, 그래, 완벽해! 이제 약혼했으니   you pop the champagne, you drink a toast, 샴페인 터뜨리고 건배하고   you have sex on the kitchen floor. 부엌에서 섹스하겠네   Don't have sex on our floor. Got it. - 부엌에선 하지 마 - 알았어   Thanks for helping me plan this out, Ted. 테드, 도와줘서 고마워   Dude, are you kidding? It's you and Lily. 무슨 소리야? 너하고 릴리 일이잖아   I've been there for all the big moments of you and Lily. 너희의 중요한 순간마다 난 늘 함께였어   The night you met, your first date, other first things. 첫 만남, 첫 데이트 또 첫 번째 그것까지   Yeah, sorry, we thought you were asleep. 미안해 너 자는 줄 알았어   It's physics, Marshall. 물리 법칙 몰라?   If the bottom bunk moves, the top bunk moves, too. 아래 침대가 흔들리면 위층 침대도 흔들려   My God, you're getting engaged tonight. 세상에, 너 오늘 약혼하네?   Yeah. What are you doin\u0026rsquo; tonight? 응, 넌 오늘 밤에 뭐 할 거야?   OLDER TED: What was I doing? 아빤 뭘 했을까?   Here, Uncle Marshall was taking the biggest step of his life. 마셜 삼촌이 인생의 큰 변화를 맞이했을 때   And me? I'm calling up your Uncle Barney. 난 바니 삼촌에게 전화했단다   Hey, so you know how I've always had a thing for half-Asian girls? 내가 아시아계 혼혈 여자 좋아하는 거 알지?   Well, now I've got a new favorite. Lebanese girls. 요새 취향이 바뀌었어 레바논 여자야   Lebanese girls are the new half-Asians. 이제 레바논 여자가 내 취향이야   Hey, you wanna do something tonight? 오늘 밤에 한잔 어때?   Okay, meet me at the bar in 15 minutes. And suit up! 좋아, 15분 후 바에서 보자 정장 입어!   Hey. Where's your suit? 바니 정장 입으랬잖아   Just once, when I say \u0026ldquo;suit up,\u0026rdquo; I wish you'd put on a suit. 정장 입으라고 하면 한 번쯤 입어 주면 안 돼?   I did. That one time. 한 번 입었잖아   It was a blazer. 그건 블레이저였어   You know, ever since college it's been Marshall and Lily and me. 대학 때부터 난 늘 마셜, 릴리와 함께였는데   Now it's gonna be Marshall and Lily and me. 이제 걔들과 난 별개야   They'll get married, start a family. 둘은 곧 결혼할 테고   Before long, I'm that weird middle-aged bachelor their kids call \u0026ldquo;Uncle Ted.\u0026rdquo; 오래지 않아 난 노총각이 돼서 테드 삼촌 소리나 듣겠지   I see what this is about. 왜 그러는지 알겠다   Have you forgotten what I said to you the night we met? 우리 처음 만난 날 내가 한 말 잊었어?   Ted, I'm gonna teach you how to live. 테드, 내가 사는 법을 가르쳐 줄게   Barney. We met at the urinal. 난 바니야 오줌 눌 때 봤지   Oh, right. Hi. 그랬지   Lesson one, lose the goatee. It doesn't go with your suit. 첫째, 수염 깎아 정장하고 안 어울리니까   I'm not wearing a suit. Lesson two, get a suit. - 정장 안 입었는데 - 둘째, 정장을 살 것   Suits are cool. Exhibit A. 정장이 얼마나 멋진데. 봐, 증거 1호   Lesson three, don't even think about getting married till you're 30. 셋째, 서른 전에는 결혼 생각을 하지 말 것   Thirty. Right, you're right. 맞아, 서른이랬지   I guess it's just, your best friend gets engaged, you start thinkin\u0026rsquo; about that stuff. 절친이 약혼한다니까 괜히 그런 것 같아   I thought I was your best friend. Ted, say I'm your best friend. 내가 절친 아니었어? 테드, 그렇다고 말해   You're my best friend, Barney. Good. 네가 내 절친이야   Then as your best friend, I suggest we play a little game I like to call, 좋았어, 절친으로서 게임이나 하자. 뭐냐면~   \u0026ldquo;Have you met Ted?\u0026rdquo; - \u0026lsquo;테드 만나 봤어요?\u0026rsquo;   No, no, no, no, we're not playing \u0026ldquo;Have you met Ted.\u0026rdquo; 잠깐, 안 돼. 그런 게임 안 해   Hi, have you met Ted? 안녕, 테드 만나 봤어요?   Hi, I'm Ted. 안녕하세요, 테드예요   Yasmin. It's a very pretty name. - 야스민이에요 - 이름 참 예쁘네요   Thanks. It's Lebanese. 고마워요. 레바논 이름이죠   Hey. 왔어?   I'm exhausted. 너무 피곤해   It was finger-painting day at school 학교에서 핑거 페인팅 했는데   and a 5-year-old boy got to second base with me. 다섯 살짜리 꼬마가 내 가슴을 만졌어   Wow, you're cooking? 요리하는 거야?   Yes, I am. 그래   Are you sure that's a good idea after last time. 요리해도 괜찮겠어?   You looked really creepy without eyebrows. 지난번에 눈썹 다 타서 정말 징그러웠거든   I can handle this. I think you'll find I'm full of surprises tonight. 괜찮아, 오늘 밤엔 자기 놀랄 일이 많을걸?   So there's more surprises? Like what? 더 놀랄 일? 그게 뭔데?   OLDER TED: Marshall was in his second year of law school, 마셜은 로스쿨 2년 차라   so he was pretty good at thinking on his feet. 순발력이 뛰어났지   Boogedy boo! And that's all of them. 이게 다야   I'm gonna go cook. 가서 요리할게   I'm so happy for Marshall, I really am. 마셜을 진심으로 축하해요   I just couldn't imagine settling down right now. 난 정착하는 건 꿈도 못 꾸거든요   So do you think you'll ever get married? 결혼 생각은 있어요?   Well, maybe eventually. 언젠가는 하겠죠   Some fall day. Possibly in Central Park. 어느 가을날 센트럴 파크면 좋겠어요   Simple ceremony. We'll write our own vows. 스몰 웨딩에 서약서도 직접 쓰고   Band, no DJ. People will dance, I'm not gonna worry about it. DJ는 없을거고, 밴드도 부를 거예요 사람들은 알아서 춤추겠죠   Damn it, why did Marshall have to get engaged? 제길, 왜 마셜은 약혼하겠단 거야?   Yeah, nothing hotter than a guy planning out his own imaginary wedding, huh? 결혼식 상상이나 하다니 엄청나죠?   Actually, I think it's cute. Well, you're clearly drunk. - 귀여운데요? - 취하셨네요   One more for the lady! 여자분께 한 잔 더요!   Oh, hey, look what I got. 이것 좀 볼래?   Oh, honey, champagne. 자기야, 샴페인이네   Yeah. 그래   No, you are too old to be scared to open a bottle of champagne. 샴페인 병 따는 거 무서워할 나이는 지났잖아   I'm not scared. 안 무서워   Then open it. Fine. - 그럼 해 봐 - 알았어   Please, open it. 자기가 해 줘   Gosh, you are unbelievable, Marshall. 정말 못 말려   OLDER TED: There are two big questions a man has to ask in life. 인생에서 남자가 물어야 할 질문 두 가지가 있어   One you plan out for months, the other just slips out 하나는 몇 달을 계획하고 묻지만 다른 하나는   when you're half-drunk at some bar. 취하면 그냥 튀어나오지   Will you marry me? 나랑 결혼해 줄래?   You wanna go out sometime? 나중에 데이트할래요?   Of course, you idiot! 당연하지, 바보야!   I'm sorry, Carl's my boyfriend. 미안해요 칼이 남자 친구예요   What's up, Carl? 안녕하세요, 칼   I promised Ted we wouldn't do that. 부엌에선 안 하겠다고 약속했는데   Did you know there's a Pop-Tart under your fridge? 냉장고 밑에 팝타르트 있는 거 봤어?   No, but dibs. 아니, 내가 찜!   Where's that champagne? I wanna drink a toast with my fiancee. Oh. 샴페인 어디 있지? 내 약혼녀와 건배해야지   I don't know why I was so scared of this. It's pretty easy, right? 이걸 왜 무서워했지? 쉽잖아, 안 그래?   Why am I freaking out all of a sudden? This is crazy. I'm not ready to settle down. 갑자기 왜 이러지? 난 정착할 준비도 안 됐는데   How does Carl land a Lebanese girl? 칼은 어떻게 레바논 여자를 만났지?   The plan's always been don't even think about it until you're 30. 서른 전에는 결혼 생각 않기로 했는데   Exactly. The guy doesn't even own a suit. 맞아, 녀석은 정장도 한 벌 없는데   Plus, Marshall's found the love of his life. 마셜은 인생의 짝도 찾았잖아   Even if I was ready, which I'm not, 난 아직 준비도 안 됐지만   but if I was, it's like, \u0026ldquo;Okay, I'm ready. Where is she?\u0026rdquo; 설령 준비됐다 한들 나의 그녀는 어디 있는데?   OLDER TED: And there she was. 거기 그 여자가 있었단다   OLDER TED: It was like something from an old movie 옛날 영화를 보면   where the sailor sees the girl across the crowded dance floor 뱃사람이 군중 속에서 춤추는 여자를 보고   turns to his buddy and says, 친구에게 말하지   \u0026ldquo;See that girl? I'm gonna marry her someday.\u0026rdquo; 친구에게 말하지 \u0026lsquo;저 여자 보여? 저 여자랑 결혼할 거야\u0026rsquo;   Hey, Barney, see that girl? 바니, 저 여자 보여?   Oh, yeah. You just know she likes it dirty. 응, 음란한 걸 좋아하는 여자 같네   Go say hi. 가서 인사해   I can't just go say hi. I need a plan. 어떻게 그래 계획을 세워야지   I'm gonna wait until she goes to the bathroom, then I'll strategically place myself by the jukebox so that\u0026hellip; 화장실에 갔을 때 주크박스 옆에 서 있어야지   Hi, have you met Ted? 혹시 테드 만나 봤어요?   Hi. 안녕하세요   Let me guess. Ted. 맞혀 볼게요, 테드죠?   I'm sorry, Lily. 릴리, 정말 미안해   I'm so sorry. Take us to the hospital. 릴리, 정말 미안해 병원으로 가 주세요   Whoa, whoa, whoa. Did you hit her? 잠깐만, 여자를 때렸나?   Hit me? Please. This guy can barely even spank me in bed for fun. 설마요, 잠자리에서 재미로도 못 때리는걸요   He's all, like, \u0026ldquo;Oh, honey, did that hurt?\u0026rdquo; \u0026lsquo;안 아파?\u0026lsquo;라고 묻는 남자예요   And I'm, like, \u0026ldquo;Come on, let me have it, you pansy.\u0026rdquo; 오히려 제가 \u0026lsquo;어서 쳐, 호모야!\u0026lsquo;라고 해요   Wow, a complete stranger. No, no, no, it's okay. Go on. - 모르는 분께 실례를\u0026hellip; - 괜찮아요, 계속해요   So, these, uh, spankings, 그러니까, 때릴 때   are you in pajamas or au naturel? 잠옷 입고 해요? 다 벗고 해요?   So what do you do? A reporter for Metro News 1. 무슨 일 하세요? \u0026lsquo;메트로 뉴스 1\u0026rsquo; 기자예요   Oh. Well, kind of a reporter. 리포터라고 할 수 있죠   I do those dumb little fluff pieces at the end of the news. 뉴스 끝날 때 웃기는 단신을 다뤄요   You know, like, \u0026ldquo;Monkey who can play the ukulele.\u0026rdquo; 우쿨렐레를 연주하는 원숭이 같은 거요   But I'm hoping to get some bigger stories soon. 조만간 큰 건을 맡으면 좋겠어요   Bigger, like, uh, \u0026ldquo;Gorilla with an upright bass?\u0026rdquo; 더블베이스 연주하는 고릴라 같은 거요?   Sorry. You're really pretty. 미안해요 당신 정말 예쁘네요   Oh, your friends don't seem too happy. 친구들 기분이 별로인가 봐요   Yeah. The one in the middle just got dumped by her boyfriend. So tonight, every guy is \u0026ldquo;the enemy.\u0026rdquo; 가운데 애가 막 차여서 오늘 모든 남자가 적이죠   You know, if it'll make your friend feel better you could throw a drink in my face. I don't mind. 친구를 즐겁게 할 수 있다면 내 얼굴에 술 끼얹어도 돼요   She would love that. 친구가 좋아하겠어요   And it does look fun in the movies. 영화에서 보면 재미있어 보이던데   Hey, you wanna have dinner with me Saturday night? 토요일에 저녁 같이 할래요?   Oh, I can't. I'm going to Orlando for a week on Friday. 안 돼요, 금요일부터 일주일간 올랜도에 가거든요   Some guy's attempting to make the world's biggest pancake. 어떤 사람이 세계 최대 크기의 팬케이크 만든대요   Guess who's covering it? That's gonna take a week? - 누가 취재할까요? - 일주일이나요?   Yeah, he's gonna eat it, too. It's another record. 그걸 또 먹는다네요 또 다른 신기록이죠   Hey, what's takin\u0026rsquo; so long? 뭐가 이렇게 오래 걸려?   I know this is a long shot, but how about tomorrow night? 가능할지 모르겠지만 내일 저녁은 어때요?   Yeah. What the hell. 좋아요, 까짓거   Jerk! 멍청한 자식!   That was fun. 재미있었어요   De\u0026hellip; Wait for it \u0026hellip;nied. 역시나 거절당했지?   Denied. 거절당했어   We're goin\u0026rsquo; out tomorrow night. 내일 만나기로 했어   I thought we were playing Lazer Tag tomorrow night. 내일 레이저태그 하기로 했잖아   Yeah, I was never gonna go play Lazer Tag. 애초에 갈 생각 없었어   OLDER TED: The next night, I took her out to this little bistro in Brooklyn. 다음 날, 브루클린의 작은 식당에 데려갔어   That is one bad-ass blue French horn. 멋진 파란색 프렌치 호른이네요   Yeah. Sort of looks like a Smurf penis. 그렇네요 스머프 고추 같아요   OLDER TED: Son, a piece of advice. 아들아, 충고하마   When you go on a first date, you really don't want to say Smurf penis. 첫 데이트에서 \u0026lsquo;스머프 고추'는 말하지 마   Girls don't ordinarily like that. 보통 여자들은 안 좋아하거든   But this was no ordinary girl. 그런데 이 여자는 남달랐어   Lily. 릴리?   How long have you been sitting there? 언제부터 거기 있었어?   Stupid eye patch. 망할 안대 같으니   Mom, Dad, I have found the future Mrs. Ted Mosby. 엄마, 아빠 미래의 제 부인을 찾았어요   Marshall, how have I always described my perfect woman? 마셜, 내 이상형이 뭐랬지?   Ah. Let's see. She likes dogs? 보자, 개를 좋아해?   I've got five dogs. 개가 다섯 마리 있어요   She drinks Scotch? 위스키를 마셔?   I love a Scotch that's old enough to order its own Scotch. 아주 오래된 위스키를 좋아해요   Can quote obscure lines from Ghostbusters? \u0026lsquo;고스트버스터즈\u0026rsquo; 대사도 읊어야 해   \u0026ldquo;Ray, when someone asks you if you're a God, you say, \u0026lsquo;Yes!'\u0026rdquo; \u0026lsquo;레이, 누군가 네게 신이냐고 물으면 그렇다고 해\u0026rsquo;   And I'm saving the best for last. 그중에서도 최고는 말이야   Do you want these? I hate olives. 이거 드실래요? 난 올리브가 싫어요   She hates olives. Awesome! 올리브를 싫어한대! 완벽해!   The Olive Theory. 올리브 이론이네   The Olive Theory is based on my friends, Marshall and Lily. 올리브 이론은 제 친구 얘기인데   He hates olives, she loves them. And in a weird way, that's what makes them such a great couple. 한쪽은 싫어하고 한쪽은 좋아해서 멋진 커플이래요   Perfect balance. 완벽한 조화라나?   You know, I've had a jar of olives just sitting in my fridge forever. 우리 집 냉장고에 올리브 한 통 있어요   I could take them off your hands. 제가 치워 드리죠   They're all yours. 다 가지세요   Oh, it is on! 완전, 시작됐네!   It is on till the break of dawn. 이제 밤새워도 모자라겠는걸?   But, wait, it's only the break of 10:30. What happened? 잠깐, 이제 열 시 반인데 왜 벌써 왔어?   I gotta get one of those blue French horns for over my fireplace. 아까 본 프렌치 호른을 사서 벽난로 위에 걸어야겠어요   It's gotta be blue, it's gotta be French. 반드시 파란색이어야 하고 다른 건 안 돼요   No green clarinet? Nope. - 녹색 클라리넷은요? - 안 돼요   Come on, no purple tuba? 자주색 튜바도요?   It's a Smurf penis or no dice. 스머프 고추 아니면 안 돼요   There you are! 여기 있었네   We got a jumper. Some crazy guy on the Manhattan Bridge. 맨해튼 다리에 자살하려는 사람이 있어   Come on, you're covering it. 어서 와, 보도해야지   Um. All right, I'll be right there. 알았어요, 바로 갈게요   I'm sorry. 미안해요   I had a really great time tonight. 오늘 정말 즐거웠어요   Yeah, well. 네   So, did you kiss her? 그래서? 키스했어?   No, the moment wasn't right. 아니, 타이밍이 별로였어   Look, this woman could actually be my future wife. I want our first kiss to be amazing. 미래의 아내일지도 모르는데 첫 키스는 멋져야지   Oh, Ted, that is so sweet. 테드, 정말 다정하다   So, you chickened out like a little bitch. 그래서 겁먹고 꼬릴 내렸어?   What? I did not chicken out. 겁먹은 거 아니야   You know what? I don't need to take first kiss advice from some pirate who hasn't been single since the first week of college. 관두자 대학 입학부터 지금까지 커플인 해적한테 첫 키스 충고는 듣고 싶지 않아   Ted, anyone who's single would tell you the same thing. 싱글들도 똑같이 말할 거야   Even the dumbest single person alive. 세상에서 가장 멍청한 싱글도 그럴걸?   And if you don't believe me, call him. 못 믿겠으면 전화해 봐   Hey, loser, how's not playing Lazer Tag? 루저, 레이저태그 안 하니까 좋냐?   Because playing Lazer Tag is awesome. 이 게임 완전 끝내주는데   Oh, I killed you, Connor. Don't make me get your mom. 코너, 너 죽었어 네 엄마한테 이른다   Hey, listen, I need your opinion on something. 나 물어볼 게 있어   Okay, meet me at the bar in 15 minutes. And suit up! 좋아, 15분 뒤 바에서 봐 정장 입고!   So, these guys think I chickened out. What do you think? 얘네는 내가 겁먹었대 네 생각은 어때?   I can't believe you're still not wearing a suit. 넌 어떻게 된 게 아직도 정장을 안 입었냐?   She didn't even give me the signal. 아무런 신호도 없었어   What, is she gonna bat her eyes at you in Morse code? 그럼 눈으로 모스 부호라도 보내야 하냐?   \u0026ldquo;Ted, kiss me.\u0026rdquo; No, you just kiss her. \u0026lsquo;테드, 키스해 줘요\u0026rsquo; 그냥 키스하면 된다고!   Not if you don't get the signal. 신호도 안 보냈는데!   Did Marshall give me the signal? 마셜이 신호 보냈어?   No! I didn't, I swear. 천만에 절대 안 그랬어, 맹세해   But, see, at least tonight, I get to sleep knowing Marshall and me, never gonna happen. 최소한 오늘밤에 난 잠에 들며 마셜과 난 안된다는걸 알겠지   You should've kissed her. 키스했어야지   Oh, I should've kissed her. Well, maybe in a week when she gets back from Orlando. 키스를 놓치다니 일주일 뒤에 올랜도에서 돌아올 때는 어떨까?   A week? That's like a year in hot-girl time. She'll forget all about you. 일주일? 미녀에게 일주일은 1년이지. 넌 기억도 못 할걸?   Mark my words, you will never see that one again. 잘 들어 그 여자 다시 못 볼 거야   There she is. 저기 있네   Ooh. She's cute. Hey, Carl, turn it up. 저기 있네 귀엽다 칼, 소리 좀 올려 줘요   \u0026hellip;persuaded him to reconsider. 설득에 성공했습니다   At which point the man came down off the ledge, 그제야 남자는 난간에서 내려왔고   giving this bizarre story a happy ending. 자살 소동은 잘 마무리됐습니다   Reporting from Metro 1 News, back to you, Bill. The guy didn't jump. - 메트로 뉴스 1의\u0026hellip; - 남자가 안 뛰었군   I'm gonna go kiss her. 가서 키스해야겠어   Right now. Look, dude, it's midnight. 지금? 야 지금 한밤중이야   As your future lawyer, I'm gonna advise you 미래의 네 변호사로서 충고하지   that's freakin\u0026rsquo; crazy. I never do anything crazy. - 이건 미친 짓이야 - 난 미친 짓을 안 해왔어   I'm always waiting for the moment, planning the moment. 늘 때를 기다리고 계획만 짜고 있지   Well, she's leavin\u0026rsquo; tomorrow and this may be the only moment I'm gonna get. 내일 떠나니까 지금이 유일한 기회야   I gotta do what that guy couldn't. I gotta take the leap. 저 남자가 못 한 걸 해야 해 뛰어들어야 한다고   Okay, not a perfect metaphor 비유가 적절하진 않군   \u0026lsquo;cause for me it's fall in love and get married and for him it's death. 난 사랑에 뛰어드는 거지만 저 남잔 죽음이었으니까   Actually, that is a perfect metaphor. 사실 정확한 비유였어   By the way, did I congratulate you two? 너희들 약혼은 축하했던가?   I'm doing this. 가야겠어   Let's go. 우리도 가자   Word up. We're coming with you. - 당근이지 - 우리도 같이 갈게   Barney? 바니, 넌?   All right, but under one condition. 좋아, 하지만 조건이 있어   Look at you, you beautiful bastard, you suited up. 이 자식, 정장 입으니까 얼마나 멋지냐   This is totally going in my blog. 블로그에 올려야지   Stop the car. Uh, pull over right here. 잠깐만 차 좀 세워 주세요   I gotta do something. 할 일이 있어   Excuse me. Pardon me, just a sec. 실례합니다, 잠시만요   Enjoy your coffee. 커피 맛있게 드세요   Hey! 저기요, 이봐요!   Go, go, go! 빨리 출발하죠!   Everybody brings flowers. 보통은 꽃을 준비하는데..   Okay, moment of truth. Wish me luck. 이제, 중요한 순간이야 행운을 빌어 줘   Ted's gonna get it on with a TV reporter. 테드가 TV 리포터와 자다니   \u0026ldquo;This just in.\u0026rdquo; Okay. \u0026lsquo;방금 들어온 소식입니다\u0026rsquo; 괜찮았어?   Kiss her, Ted. Kiss her good. 키스해, 테드! 잘하라고   Kiss the crap out of that girl. 키스로 끝내 버리는 거야   Marshall, remember this night. 마셜, 오늘 밤을 기억해 둬   When you're the best man at our wedding and you give a speech, you're gonna tell this story. 내 결혼식 때 들러리로서 이 얘기를 해야 할 테니까   Why does he get to be the best man? I'm your best friend! 왜 마셜이 들러리야? 내가 네 절친이잖아!   OLDER TED: As I walked up to that door, a million thoughts raced through my mind. 집 앞으로 걸어갈 때 온갖 생각을 다 했지만   Unfortunately, one particular thought did not. 그만 한 가지를 깜빡했어   I've got five dogs. 개가 다섯 마리 있어요   Not good, not good. 안 좋아   No! Get back in there. - 안 돼 - 빨리 돌아가   You're wearing a suit! 정장도 입었잖아   ROBIN: Ted? 테드?   Hi. 반가워요   I was just, uh\u0026hellip; 그냥 저기\u0026hellip;   Come on up. 들어와요   MARSHALL: He's in. 들어갔어   So, Ranjit, you must have done it with a Lebanese girl? 저기, 란지트 레바논 여자하고 해 봤죠?   Okay, that's my Barney limit. 그래 이게 바니의 한계지   I'm gonna see if that bodega has a bathroom. 저 가게에 화장실 있겠지?   Actually, I'm from Bangladesh. 사실 난 방글라데시 출신이에요   The women hot there? 거기 여자들도 섹시해요?   Here's a picture of my wife. 우리 집사람 사진이에요   [WHISPERING] A simple \u0026ldquo;no\u0026rdquo; would have sufficed. 그냥 아니라고 했으면 충분했을텐데   She's lovely. 아름다우시네요   So, Ted, what brings you back to Brooklyn at 1:00 in the morning in a suit? 테드 새벽 한 시에 정장까지 입고 브루클린엔 무슨 일이죠?   I was just hoping to 그러니까\u0026hellip;   get those olives that you said I could have. 나한테 준다던 올리브 가져가려고요   Would you like those olives with some gin and vermouth? 올리브 안주로 칵테일 한잔할래요?   Are you trying to get me drunk? 나 취하게 하려고요?   For starters. 일단은요   So, Marshall, this Olive Theory based on you and Lily. 마셜, 올리브 이론이 - 너하고 릴리 얘기라며?   Yeah. 맞아   You hate olives. Lily loves them, you can't stand them. 넌 올리브를 싫어하고 릴리는 무척 좋아하지   Yeah, I hate olives. 그래, 올리브 싫어   Two weeks ago, Spanish bar on 79th Street. Dish of olives. You had some. What up? 2주 전 바에서 올리브 시켰을 때 너도 먹지 않았나?   You have to swear that this does not leave this cab. 이 얘기가 택시 밖으로 안새어 나간다고 맹세해   I swear. 맹세할게   I swear. 나도 맹세하지   On our first date, I ordered a Greek salad. 첫 데이트에 그리스 샐러드를 시켰는데   Lily asked if she could have my olives. 내 올리브를 먹어도 되느냐고 묻잖아   I said, \u0026ldquo;Sure, I hate olives.\u0026rdquo; 그래서 난 싫어한다고 먹으라고 했지   But you like olives. 너 올리브 좋아하잖아   Well, I was 18, okay? I was a virgin. 그때 난 열여덟이었고 총각이었어   Been waiting my whole life for a pretty girl to want my olives. 내 올리브를 먹어줄 미녀를 기다렸다고   Marshall, I'm gonna give you an early wedding present. 마셜, 결혼 선물 미리 줄게   Don't get married. 결혼하지 마   I think I like your Olive Theory. 당신 올리브 이론이 맘에 드네요   I think I like your new French horn. 당신 새 프렌치 호른이 맘에 드네요   I think I like your nose. 당신 코가 맘에 드네요   I think I'm in love with you. 당신을 사랑하는 것 같아요   ALL: What? 뭐라고?   BOTH: What? What? - 뭐라고요? - 뭐라고요?   Come on, man, you said your stomach's been hurting, right? 너 배가 아프다고 했지?   You know what that is? Hunger. You're hungry for experience. 그게 뭔지 알아? 배고픔이야. 넌 경험이 고픈 거라고   Hungry for something new. Hungry for olives. 뭔가 새로운 것이 고프고 올리브가 고픈 거라고   But you're too scared to do anything about it. 하지만 겁쟁이라 아무것도 못 하지   Yeah, I'm scared, okay? 그래, 나 겁쟁이야   But when I think of spending the rest of my life with Lily, 하지만 릴리와 함께 평생 살 생각을 하면   committing forever, no other women, 앞으로 한 여자에게만 헌신해야 한다고 해도   doesn't scare me at all. 하나도 겁 안 나   I'm marrying that girl. 릴리랑 결혼할 거야   Lily. 릴리   Lily, I like olives. 사실 나 올리브 좋아해   We'll make it work. 해결할 수 있을 거야   So, Orlando? You gonna hit Disney World? 올랜도에 가면 디즈니월드에도 가요?   You love me? Oh, God. 사랑한다고요?   I can't believe I said that. Why did I say that? 내가 그런말을 했다는게 안 믿겨지네요 왜 그랬을까요?   Who says that? 대체 누가 그런 소릴 하죠?   I should just go. 가야겠네요   Hold on. Wait a minute. 잠깐만요, 기다려요   I promised you these. 이거 주기로 했잖아요   Olives. Yeah. Thanks. I love you. 올리브네요, 고마워요 사랑해요   What is wrong with me? 대체 왜 이러는 거지?   Why are we still sitting here? Let's go. We can still make last call. 왜 안 가? 바 문 닫기 전에 가자   What do you say, Lil? \u0026ldquo;Yo-ho-ho, and a bottle of rum\u0026rdquo;? 어때, 릴리? 럼주로 나팔 한번 불어야지   \u0026lsquo;Cause you're a pirate? 해적답게 말이야   Okay, eye patch gone. 안대를 벗어야지   And we can't just abandon Ted. 테드는 어쩌고 그냥 가   If it doesn't go well up there, he's gonna need some support. 일이 잘 안되면 우리가 위로해줘야지   It's been, like, 20 minutes. 벌써 20분이나 됐어   Do you think they're doin\u0026rsquo; it? 하고 있을까?   You think they're doing it in front of the dogs? Doggie style. - 개 앞에서 하고있냐고? - 개처럼 후배위로?   I knew this girl in college, she had this Golden Retriever\u0026hellip; 대학 때 개를 키우던 여자가 있었는데\u0026hellip;   Okay, we can go to the bar. Just stop talking. 말은 그만하고 바에 가자   Hit it, Ranjit. 출발하죠   So, when you tell this story to your friends, could you avoid the word \u0026ldquo;psycho\u0026rdquo;? 친구들한테 내 얘길 하더라도 사이코란 표현은 피해 줘요   I'd prefer \u0026ldquo;eccentric.\u0026rdquo; 차라리 괴짜가 낫겠어요   Goodnight, psycho. 잘 가요, 사이코   Great. Um, how do I get to the F train? 망했네 F 기차는 어디서 타죠?   Oh. Um, two blocks, that way and take a right. 저쪽으로 두 블록 가서 오른쪽이에요   You know what? 그거 알아요?   I'm done being single. I'm not good at it. 싱글은 그만할래요 난 혼자 잘 못 지내요   Look, obviously, you can't tell a woman you just met you love her, but it sucks that you can't. 물론 처음 본 여자한테 사랑한다고 하면 안 되지만 꼭 안 될 건 없잖아요   I'll tell you something, though. If a woman, not you, just some hypothetical woman, 있잖아요, 어떤 여자가 당신 말고 가상의 여자요   were to bear with me through all this, I think I'd make a damn good husband 내 미숙함을 견뎌 준다면 난 정말 좋은 남편이 될 거예요   because that's the stuff I'd be good at. 난 그런 걸 잘해요   Stuff like makin\u0026rsquo; her laugh and bein\u0026rsquo; a good father 여자를 웃게 하고 좋은 아빠가 되고   and walking her five hypothetical dogs. Bein\u0026rsquo; a good kisser. 상상의 개 다섯 마리 산책시키고 키스 잘하는 것도요   Everyone thinks they're a good kisser. Oh, I've got references. - 다들 자기가 잘하는 줄 알죠 - 증명할 수 있어요   Goodnight, Ted. 잘 가요, 테드   And I'm a good handshaker. 악수도 잘해요   That's a pretty great handshake. 정말 멋진 악수군요   And that was it. I'll probably never see her again. 그게 끝이었어 다시는 못 보겠지   What? That was the signal. - 왜? - 그게 바로 신호였어   That long, lingering handshake. You should've kissed her. 악수를 오래 했잖아! 키스했어야지   There's no such thing as the signal. 세상에 신호 따윈 없어   But, yeah, that was the signal. 근데 그건 신호였어   Signal. 신호예요   Carl, thank you. 칼, 고마워요   There's something I gotta do. 샴페인 따는 건 내 몫이지   By the way, you should've kissed her. 그나저나, 키스했어야죠   Carl, you guys weren't there. 칼! 너희는 거기에 없었잖아   I am so turned on right now. 자기, 나 너무 흥분돼   Guys, trust me. I've seen the signal. That was not the signal. 신호가 뭔지 나도 아는데 그건 신호가 아니었대도   Yeah, Ted, we're not on you anymore. 알았으니까 네 얘긴 그만해   To my fiancee. To the future. - 내 약혼녀를 위해 - 미래를 위해   To one hell of a night. 끝내주는 밤을 위해!   That was not the signal. 그건 신호가 아니었어!   OLDER TED: I asked her about it years later, and, yeah, that was the signal. 몇 년 뒤에 물었더니 신호가 맞았다더구나   I could've kissed her. 키스했어야 했어   But that's the funny thing about destiny, it happens whether you plan it or not. 하지만 운명은 계획과 상관없이 일어나게 돼 있어   I mean, I never thought I'd see that girl again. 사실 다시는 못 볼 줄 알았거든   But it turns out, 근데 알고 보니   I was just too close to the puzzle to see the picture that was forming. 근데 알고 보니 퍼즐이 완성되기 바로 직전이었던 거야   Because that, kids, is the true story of how I met your Aunt Robin. 이게 내가 로빈 이모를 만나게 된 이야기란다   Aunt Robin? 로빈 이모요?   I thought this was how you met Mom. 엄마 얘기인 줄 알았는데   Will you relax? I'm getting to it. 조급해 마 그 얘기 할 거니까   Like I said, it's a long story. 아까 말했잖아 얘기가 길다고    "
},
{
	"uri": "http://kimmj.github.io/prometheus/install/",
	"title": "Install Prometheus",
	"tags": ["install", "prometheus"],
	"description": "",
	"content": "Prometheus Prometheus는 opensource monitoring system입니다. 음악을 하는 사람들이 많이 이용하는 사이트인 SoundCloud에서 개발된 오픈소스입니다.\nPromQL이라는 Query문을 사용하여 metric을 수집할 수 있습니다. 자세한 내용은 나중에 따로 포스트를 작성하도록 하겠습니다.\n이 문서에서는 docker-compose를 통해 간단하게 prometheus를 설치해볼 것입니다. 또한 prometheus와 뗄레야 뗄 수 없는 단짝 Grafana도 함께 설치할 것입니다.\nPrometheus의 설치 제가 docker-compose를 선호하는 이유는 너무나도 간단하게, dependency가 있는 어플리케이션을 설치할 수 있기 때문입니다. 아래에 예시에서도 잘 드러나있습니다.\n저는 monitoring/ 폴더 아래에 docker-compose.yaml이라는 이름으로 파일을 생성했습니다.\nversion: \u0026#39;3\u0026#39; networks: back-tier: services: prometheus: image: prom/prometheus restart: unless-stopped volumes: - ./config/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; networks: - back-tier grafana: image: grafana/grafana:6.5.3 ports: - 9080:3000 user: \u0026#34;0\u0026#34; # $(id -u) https://community.grafana.com/t/new-docker-install-with-persistent-storage-permission-problem/10896/5 depends_on: - prometheus volumes: - ./grafana/provisioning/:/etc/grafana/provisioning/ - ./grafana_data:/var/lib/grafana networks: - back-tier 위의 예시에서 services.grafana.user=\u0026quot;0\u0026quot;으로 설정이 되어있는 것이 보이실 것입니다. 이 부분을 $(id -u)의 결과값으로 변경하시면 됩니다. 관련된 내용은 옆에 주석에서도 확인이 가능합니다.\n이렇게 하고난 뒤, 설치하는 방법은 너무나도 간단합니다.\ncd monitoring docker-compose up -d 잠시 내리고 싶을땐 다음과 같이 하면 됩니다.\ndocker-compose down 설정상, Prometheus는 9090 포트를 통해서 expose 되어있습니다. 또한 Grafana는 9080 포트를 사용하고 있습니다. Grafana의 기본 ID/PW는 admin/admin입니다.\n설정 Prometheus를 사용하기 위해서는 prometheus.yml이라는 파일을 관리해주어야 합니다. 이곳에서 어느 서비스의 metric을 가지고 올지 설정할 수 있습니다.\n해당 파일은 docker-compose.yaml을 참조해 보았을 때, monitoring/config/prometheus.yml을 수정하면 된다는 것을 알 수 있습니다. 수정 후에는 docker-compose down을 통해 잠시 내렸다가 docker-compose up -d를 통해 올리면 수정사항이 반영됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "Kubernetes Install Install Kubernetes on bare-metal\n Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm     "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/overview/",
	"title": "Overview",
	"tags": ["kubernetes", "install", "overview"],
	"description": "",
	"content": "저의 vm으로 구성한 클러스터를 설명드리고자 합니다.\n Cloud Provider: Kubernetes on-prem (4 VMs)  1 for master (4GB Mem, 2 CPU) 3 for worker (each 8GM Mem, 4 CPU)   Kubernetes 1.17.0 Storage Class: Ceph OS: Ubuntu 18.04.2 Server Internal Network: VirtualBox Host-Only Ethernet Adapter (192.168.x.0/24) External Network: Bridge to adapter (192.168.y.0/24)  한번에 쳐야하는 명령어가 많기 때문에, tmux를 사용해서 여러개의 pane을 생성하고 각각에 대해 ssh로 접속하였습니다.\n"
},
{
	"uri": "http://kimmj.github.io/english/himym/season1/",
	"title": "Season1",
	"tags": [],
	"description": "",
	"content": "HIMWM Season1  Episode01     Episode02     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Spinnaker Installation spinnaker를 설치해 볼 것입니다.\n쉽지 않았던 여정들을 기록하려고 합니다.\n Overview    Install Prometheus     Federation      Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/overview/",
	"title": "Overview",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Overview of install Spinnaker 어떻게 Spinnaker를 설치 및 배포하는지 알아보도록 하겠습니다.\n가장 먼저 최소 사양을 확인해보도록 하겠습니다.\n링크 : https://www.spinnaker.io/guides/developer/getting-set-up/#system-requirements\n 램 18 GB CPU 4코어 Ubuntu 14.04, 16.04, 18.04  Spinnaker 자체가 클라우드 환경에만 배포가 가능하기 때문에, 아마도 \u0026ldquo;전체 클라우드를 합하여 저정도면 된다\u0026quot;를 의미하는 것 같습니다.\n설치 방법은 두가지로 나뉩니다.\n 테스트를 목적으로 Helm Chart를 통한 설치 실제로 사용할 목적으로 halyard를 통한 설치  저는 여기서 2번 halyard를 통한 설치를 해보려고 합니다.\n전체적인 프로세스를 먼저 설명드리자면 다음과 같습니다.\n halyard 설치 Cloud Provider(클라우드 제공자) 선택 배포 환경 선택 Storage Service 선택 배포 및 접속 config 백업하기  그리고 저는 다음과 같은 환경에서 테스트를 할 예정입니다.\n Cloud Provider: Kubernetes on-prem (4 VMs)  1 for master (4GB Mem, 1 CPU) 3 for worker (each 8GM Mem, 4 CPU)   Environment: Distributed installation on Kubernetes Storage Service: Minio Deploy and Connect: expose by NodePort OS : Ubuntu 18.04.2 Server  "
},
{
	"uri": "http://kimmj.github.io/ibiza/importance-of-record/",
	"title": "Importance of Record",
	"tags": ["record", "blog"],
	"description": "",
	"content": "기록의 중요성 몇개월 전부터 Spinnaker라는 툴을 가지고 일을 하기 시작했다.\n처음 halyard를 통해 deploy하기까지 꽤나 많은 시간을 소요했던 것으로 기억한다. 집에서 하는게 아니라 회사에서 구축을 해야했기 때문에, 프록시와 관련된 설정들이 너무나도 어려웠다. 특히 spinnaker의 docs가 제대로 되어있는 것도 아니기에, 문제가 하나 발생하면 이를 해결하는 데 너무나도 많은 시간이 걸렸다.\n하지만 나의 최대 실수는 바로 기록하지 않은 것이다.\n그렇게 많은 노력끝에 약 한달만에 첫 deploy를 할 수 있었는데, 이 때 했던 설정을 \u0026ldquo;나중에 정리해야지\u0026quot;라는 안일한 생각으로 지금까지 정리를 안하고 있었다.\n요즘들어 회사에서 spinnaker를 사용할 일이 잦아지고, 다른 시료에도 deploy하게되는 일이 많아졌는데 내가 기록을 해놓지 않음으로 인해서 많은 차질이 생기고 있다. 당장만 해도 deploy 자체를 할 수 없으며, 내가 예전에 해 두었던 pipeline 설정들이 다 날아가버려서 지금 복구하는 데 한숨만 나올 뿐이다.\n그 많은 정보들을 정리해두었다면, 로그를 남겨두었다면 이런 일은 발생하지 않았을수도 있다.\n오늘 난 기록의 중요성을 다시 한번 깨달았고, 차곡차곡 이 블로그에 쌓아두기로 마음먹었다. 하루에 하나씩 쓴다는 것은 어려운 일일지 모르지만, 그래도 최대한 자주 기록을 함으로써, 나의 성장에도 도움이 되고 다른 사람들의 암흑같은 여정에 한줄기 빛이 될 수 있으면 좋겠다.\n"
},
{
	"uri": "http://kimmj.github.io/ibiza/",
	"title": "Ibiza",
	"tags": [],
	"description": "",
	"content": "Ibiza 子曰\n學而時習之 不亦說乎\n배우고 때때로 익히니 이 또한 기쁘지 아니한가.\n Importance of Record     2020 Plan     Ibiza Project     2020 New Year Holiday Plans     Diary    20200331      "
},
{
	"uri": "http://kimmj.github.io/",
	"title": "Ibiza",
	"tags": [],
	"description": "",
	"content": "Ibiza  Ibiza    Importance of Record     2020 Plan     Ibiza Project     2020 New Year Holiday Plans     Diary    20200331       Harbor    Harbor 설치      Git    git-secret을 통한 github 파일 암호화     Gitignore 설정      Python    [번역]Python을 통해 이쁜 CLI 만들기      Docker    http를 사용하는 docker registry를 위한 insecure registry 설정     Docker를 sudo없이 실행하기     [docker-compose] container에서 다른 container로 접속하기      Jenkins    Jenkins Install     Workspace@2를 변경하기 - Workspace List 설정 변경      IaC    [번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials      CICD    Deploy Strategy      CSS    background image 어둡게 하기     Greater Than Sign      Prometheus    Install Prometheus     Federation      Kubernetes    Install    Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm      CKA Study    10 Kubernetes the Hard Way     09 Networking     08 Storage     07 Security     06 Cluster Maintenance     05 Application Lifecycle Management     04 Logging and Monitoring     03 Scheduling     02 Core Concepts      Concepts    Controllers Overview     Pods      [번역] 쿠버네티스에서의 Port, TargetPort, NodePort     Stern을 이용하여 여러 pod의 log를 한번에 확인하기      Hugo    Ibiza    Font Change      Hugo에 Google Analytics 적용하기     Hugo에 Comment 추가하기 (Utterance)     HUGO로 HTML이 되지 않을 때 가능하게 하는 방법      Spinnaker    Installation    Overview     Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment      CanaryAnalysis    Canary Analysis      Tips    Pipeline Expressions       Ansible    Ansible for DevOps    Chapter 1 - Getting Started With Ansible     Chapter 2 - Local Infrastructure Development: Ansible and Vagrant     Ad Hoc Commands     Ansible Playbooks     Chapter5    Ansible Playbooks Beyond the Basics     Handover     Environment Variables     Variables     if/then/when - Conditionals     Delegation Local Actions and Pauses     Prompts     Tags     Summary      Chapter6      Create Vm With Ansible Libvirt      Ubuntu    Tools    Tmux      Network    Netplan으로 static IP 할당받기      Ubuntu의 Login Message 수정하기     reboot 후에 tmux를 실행시켜 원하는 작업을 하기     oh-my-zsh에서 home key와 end key가 안될 때 해결방법     Ubuntu에서 Base64로 인코딩, 디코딩하기     Editor(vi)가 없을 때 파일 수정하기     열려있는 포트 확인하기     pipe를 사용한 명령어를 watch로 확인하기     watch를 사용할 때 alias 이용하기     password 없이 ssh 접속하기     SSH Tunneling 사용법     Gateway를 이용하여 SSH 접속하기     Hostname 변경하기     추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기     Ubuntu 설치 시 Boot Parameter를 수정하기     sudo를 password 없이 사용하기      English    HIMYM    Season1    Episode01     Episode02        "
},
{
	"uri": "http://kimmj.github.io/english/himym/season1/episode02/",
	"title": "Episode02",
	"tags": [],
	"description": "",
	"content": "Generated by Language Learning with Netflix\n   Eng Kor     TED: Okay, where was I? 어디까지 말했지?   You were telling us how you met Mom. In excruciating detail. Right. - 엄마를 어떻게 만났는지요 - 그것도 아주 자세하게요   So, back in 2005, when I was 27, 그래, 2005년에 난 스물일곱이었어   my two best friends got engaged, and it got me thinking, maybe I should get married. 절친 둘이 약혼하면서 나도 결혼을 생각하게 됐지   And then I saw Robin. She was incredible. 그러다 로빈을 봤어 정말 멋진 여자였지   I just knew I had to meet her. 만나고 싶단 생각뿐이었어   That's where your Uncle Barney came in. 그때 바니가 끼어들었어   I suggest we play a little game I like to call \u0026ldquo;Have you met Ted?\u0026rdquo; \u0026lsquo;테드 만나 봤어요?\u0026rsquo; 게임을 제안하지   Wait, no, no, no. We're not playing \u0026ldquo;Have you met Ted?\u0026rdquo; 잠깐 아냐아냐. 그런 게임 안 해   Hi. Have you met Ted? 혹시 테드 만나 봤어요?   So I asked her out. 그렇게 데이트를 했지   And I know this sounds crazy, but after just one date, I was in love with her 정신 나간 소리 같겠지만 데이트 한 번에 사랑에 빠졌어   which made me say something stupid. 그래서 멍청한 소리를 했지   I think I'm in love with you. What?! - 사랑하는 것 같아요 - 뭐라고요?   Oh, Dad. 세상에, 아빠   So then what happened? 그래서 어떻게 됐어요?   Nothing. 아무 일도 없었어   I mean, I'd made a complete fool of myself. 바보 같은 짓을 해 버렸으니   So, a week went by, and I decided not to call her. 일주일 후에도 전화하지 않기로 했지   So you're not gonna call her? 전화도 안 한다고?   You went from, \u0026ldquo;I think I'm in love with you\u0026rdquo; to \u0026ldquo;I'm not gonna call her\u0026rdquo;? 사랑한다고 해 놓고는 전화를 안 해?   I wasn't in love with her, okay? 사랑하는 게 아녔어   I was briefly in love with the abstract concept of getting married. 결혼이라는 개념을 아주 잠깐 사랑했던 거지   It had absolutely nothing to do with Robin. 로빈과는 상관없어   Robin. - 로빈 - 잘 지냈어요?   Look who I ran into. 우연히 만났어   Since when do you guys know each other? 둘은 언제부터 알았어요?   Oh, since about\u0026hellip; here. 여기쯤일 때요?   Lily recognized me from the news and\u0026hellip; 릴리가 뉴스에 나온 날 알아보고\u0026hellip;   Hello, sailor! 안녕, 총각!   They just got engaged. 얼마 전에 약혼했어요   Well, I should get back to the station. 난 가 봐야겠어요   See you, guys. 또 봐요   Nice seeing you, Ted. Yeah, you, too. Thanks. - 테드, 반가웠어요 - 나도요   What? Damn it! - 젠장! - 왜 그래?   I'm in love with her. - 나 로빈을 사랑해 - 아니야!   As your sponsor, I will not let you relapse. 망상에서 벗어나도록 상황을 정리해 줄게   You blew it, it's over, move on. 네가 망쳤어 다 끝났으니 잊어   I don't know, I just have this feeling she's the future Mrs. Ted Mosby. ( Lily squeaks ) 잘은 모르겠지만 로빈이 내 미래의 아내 같아   Lily, you squeaked? 릴리, 왜 끽끽거려?   She said something about me, didn't she? Come on, spill it, Red! 로빈이 내 얘기 했지? 어서 말해 봐!   Fine. 알겠어   So, what do we think of Ted? 테드를 어떻게 생각해요?   Ted's something else. 남다른 것 같아요   I'm gonna spin that as good. 좋은 뜻으로 해석할래   Lots of guys are something, I'm something else. 난 보통의 남자들과는 다르다고 말이야   Comes on a little strong. 너무 적극적이랄까   But, that's part of my charm. 그건 내 매력 중 하나지   But, that's part of his charm. 그게 테드의 매력이죠   Oh, totally. I mean, he's sweet, he's charming, 맞아요 다정하고 매력적이지만   he's just looking for something a little bit more serious than I am. 제 생각보다 더 진지한 관계를 원해서요   I mean, the most I can handle right now is something casual. 저는 가벼운 관계를 원하거든요   This just stays between us, right? 비밀로 해줄 거죠?   Are you kidding? This flapper? Fort Knox. 당연하죠 입 꼭 다물고 있을게요   She wants casual. Okay, I'll be casual. 가벼운 관계? 그럼 가볍게 굴어 주지   I'm going to be a mushroom cloud of casual. 가벼움의 극치를 보여 주겠어   You know why? 왜냐고?   Cause it's a game\u0026ndash; I want her to skip to the end and do the whole happily-ever-after thing. 이건 게임이거든 마지막으로 건너뛰어서 해피 엔딩을 맞게 해야지   But you don't get there unless you play the game. 그러려면 게임을 해야만 해   So, are you going to ask her out? 데이트 신청 할 거야?   Yeah\u0026hellip; No! 그럼\u0026hellip; 아니!   I can't ask her out, because if I ask her out, I'm asking her out. 그건 안 돼, 데이트하려면 물어봐야 하잖아   So, how do I ask her out without asking her out? 그럼 안 물어보고 어떻게 데이트할 수 있을까?   Did you guys get high? 너희 취했어?   \u0026ldquo;something\u0026rdquo; I don't ask her out. 생각났어 안 물어볼 거야   I invite her to our party next Friday. 다음 주 금요일 파티에 초대하면 돼   We're having a party next Friday? 우리 파티해?   We are now. Casual. 방금 정해졌어 가볍게 말이지   Yeah, cause nothing says \u0026ldquo;casual\u0026rdquo; like inviting a hundred people over just to mack on one girl. 퍽이나 가볍다, 여자 꾀려고 사람 불러 파티를 하다니   Oh, and Lily, that's my leg. 릴리, 그거 내 다리다   You waited five minutes to tell me that? 그걸 5분이나 지나서 말해?   All right, so call her up. 로빈한테 전화해   No, calling's not casual. I just got to bump into her somewhere. 전화하면 가볍지 않잖아 우연히 만나서 말해야지   Now, if only I knew her schedule, I could arrange a chance encounter. 일정만 알면 기회를 만들 수 있을 텐데   That's great, Ted\u0026ndash; you'll be the most casual stalker ever. 넌 세상에서 가장 가벼운 스토커가 될 거야   put that ring on her finger, Lily had been, well, extra affectionate. 마셜과 약혼한 뒤로 릴리의 애정 표현이 뭐랄까, 다소 과해졌단다   ( chuckling ): Baby, no. 자기야, 안 돼   I have a 25-page paper on constitutional law due Monday. 자기야, 안 돼 월요일까지 리포트 내야 해   I barely started. 아직 시작도 못 했어   Hey, I'm just sitting here, wearing my ring, 난 그냥 반지 끼고 앉아 있을 뿐이야   my beautiful ring. ( typing ) 정말 아름다운 반지야   Kind of makes wearing other stuff seem wrong. 다른 걸 걸치면 안 될 것 같아   Like my shirt. Kind of don't want to wear my shirt anymore. 셔츠 같은 거 이제 입기도 싫어졌어   Or\u0026hellip; my underwear. 속옷 같은 것도?   That's right, I'm not wearing any. 맞다, 속옷 안 입고 있지   No underwear? 속옷 안 입었어?   Not even slightly. 실오라기 하나도 안 걸쳤지   TED: Guys. 얘들아   Boundaries. 지킬 건 지키자   There she is. 로빈 나왔다   ROBIN ( on TV ): Thanks, Bill. 고마워요, 빌   I'm reporting from the Razzle Dazzle Supermarket on 75th and Columbus\u0026hellip; 저는 지금 콜럼버스 75번가의 \u0026lsquo;시끌벅적 슈퍼'에서\u0026hellip;   75th and Columbus. 콜럼버스 75번가래   Game on! 게임 시작이야!   \u0026hellip;where four-year-old Leroy Ellenberg has climbed inside a grab-a-prize machine and gotten stuck. 네 살 꼬마 리로이 앨런버그가 상품을 집으러 들어갔다가 기계 안에 갇혔습니다   And, all in the pursuit of a stuffed, purple giraffe. 자주색 기린 인형 때문이었다고 하네요   For Metro News 1, I'm Robin Trubotsky. 자주색 기린 인형 때문이었다고 하네요 이상, 메트로 뉴스 1의 로빈 셔바츠키였습니다   ENGINEER: We're clear. 됐어요   ROBIN: Thanks, Don. Whew. 고마워요, 돈   Ted. 테드   Robin, wow! What are the odds? 로빈! 이런 데서 다 만나네요   What are you doing here? 여기서 뭐 해요?   Oh, you know, just, uh, shopping for, uh, dip. I love dip. 소스 하나 살까 해서 왔어요 소스를 좋아하거든요   I mean, I don't love dip, I like dip\u0026hellip; as a friend, you know. 사랑까지는 아니지만 좋아하죠 친구처럼요   ( chuckles ) So, uh, hey, you, uh, reporting a news story or something? 여기서 뉴스 취재라도 했어요?   Yeah, kid stuck in a crane machine. How sweet of you to call it news. Wow. 애가 기계에 갇혔거든요 뉴스라고 해 줘서 고마워요   Kid in a crane machine. Mm-hmm. 인형 뽑기 기계에 갇히다니   You just had to have that toy, didn't you? 그렇게 갖고 싶었어?   Couldn't play the game like everyone else. 남들은 잘만 뽑던데   You're all sweaty! 땀 범벅이네요!   Cute kid. 귀여운 아이네요   Um, you know, it's so funny I should run into you. 이렇게 만나다니 정말 신기해요   We're, uh, we're having a party next Friday, if you feel like swinging by. 다음 주 금요일에 파티하는데 들러요   But, you know, whatever. 뭐, 안 되면 말고요   Oh, I'm going back home next weekend. It's too bad it's not tonight. 다음 주말에 집에 가요 오늘이었다면 좋았을 텐데   It is\u0026hellip; it's tonight. This Friday. Did I say next Friday? 오늘 밤, 이번 주 금요일요 내가 다음 주랬어요?   Sorry, I guess I've been saying next Friday all week. 이번 주 내내 다음 주라고 했네요   But, yeah, it's tonight, the, uh, the party's tonight. 어쨌든 파티는 오늘 밤이에요   But, you know, whatever. 부담은 갖지 말고요   Hey, am I interrupting anything? - 여보세요? - 내가 방해했어?   No, no, I'm just writing my paper. Hitting the books. 아니, 리포트 써 완전 열심히 쓰고 있지   Yeah, well, you and Lily might want to put some clothes on. 너랑 릴리 둘 다 옷 입고 있는 게 좋겠어   We're throwing a party in two hours. Okay, bye. 두 시간 후에 파티할 거야 그럼 끊는다   So, Gatsby, what are you gonna do when Robin shows up? 로빈 오면 어떻게 할 거야?   Okay, I got it all planned out. 다 계획을 짜 뒀어   She steps through the door\u0026ndash; and where's Ted? Not eagerly waiting by the door. 들어오면서 날 찾겠지만 난 근처에 안 있을 거야   No, I'm across the room at my drafting table, 들어오면서 날 찾겠지만 난 근처에 안 있을 거야 방 건너편 테이블에서   showing some foxy young thing all my cool architect stuff. 방 건너편 테이블에서 미녀들에게 건축 도구를 보여 주고 있어야지   So, Robin strolls over, and I casually give her one of these: 그럼 로빈이 다가올 테고 난 가볍게 \u0026lsquo;왔어요?\u0026rsquo; 하면 되는 거야   \u0026ldquo;Hey, what's up?\u0026rdquo; 난 가볍게 \u0026lsquo;왔어요?\u0026rsquo; 하면 되는 거야   She says, \u0026ldquo;Hey, nice place, et cetera, et cetera.\u0026rdquo; 로빈이 \u0026lsquo;집 좋네요'라고 하겠지 그럼 난 \u0026lsquo;편하게 있어요\u0026rsquo; 하고는   And then, I say, \u0026ldquo;Well, make yourself at home.\u0026rdquo; 그럼 난 \u0026lsquo;편하게 있어요\u0026rsquo; 하고는   And, I casually return to my conversation. 그럼 난 \u0026lsquo;편하게 있어요\u0026rsquo; 하고는 가볍게 미녀들과 대화를 이어가는 거지   Then, an hour later\u0026hellip; 가볍게 미녀들과 대화를 이어가는 거지 그리고 한 시간쯤 지나\u0026hellip;   \u0026ldquo;Oh, you're still here?\u0026rdquo; I say, like I don't really care, 아무렇지도 않은 듯 \u0026lsquo;아직 있었어요?\u0026rsquo; 하면   but it's a nice surprise. 기뻐하며 살짝 놀라겠지   And then, very casually: 기뻐하며 살짝 놀라겠지 그럼 난 정말 가볍게 \u0026lsquo;옥상 갈래요?\u0026lsquo;라고 할 거야   BOTH: The roof! Get her up to the roof, - 옥상이라니! - 옥상이라니! 옥상까지만 가면 다 잘 풀릴 거야   and the roof takes care of the rest. 옥상까지만 가면 다 잘 풀릴 거야   What's so special about the roof? 옥상에 뭐가 있길래?   Oh, the moon, the stars, the shimmering skyline. 달과 별이 있고 지평선도 반짝이는데   You can't not fall in love on that roof. 달과 별이 있고 지평선도 반짝이는데 사랑에 안 빠질 수 없지   We do it up there, sometimes. 우린 가끔 거기서도 해   Solid plan, my little friend. We're the same height. - 계획 확실하네, 땅꼬마 - 우리 키 같거든?   But, may I suggest one little modification. 내가 계획을 약간 수정해 줄게   BARNEY: That foxy young thing you were chatting up, take her up to the roof and have sex with her. 대화하던 미녀를 옥상에 데려가서 자   Crazy monkey style\u0026hellip; That's not the plan. - 정신 나간 원숭이처럼\u0026hellip; - 그건 계획이 아니야   BARNEY: Well, it should be the plan. 이게 계획이지   I mean, look at her. Ted, look at her. She's smoking! 내말은, 여자를 잘 봐 테드. 담배피고 있잖아!   Thank you! 고마워요!   Yeah\u0026hellip; But, she's not Robin! 그래\u0026hellip; 이 여자는 로빈이 아니잖아   Exactly! 바로 그거야!   Ted, let's rap. 테드, 생각해 봐   Statistic: At every New York party, 뉴욕의 모든 파티에는   there's always a girl who has no idea whose party she's at. 누구 파티인지도 모르고 오는 여자들이 있어   She knows no one you know, and you will never see her again. 서로의 지인도 모르고 다시 볼 일도 없지   Do you see where I'm going with this? 내가 무슨 말 하려는지 알겠어?   Barney, I don't think so. ( groans ) 바니, 내 생각은 달라   Scoping. 정찰 중   ( imitates sonar beeping ) Scoping. 정찰 중   Man, you're a dork. 너 정말 띨띨하다   Target acquired! 목표물 확보   Now it's time we play a little game 그럼 \u0026lsquo;테드 만나 봤어요?\u0026rsquo; 게임을 시작하자고   I like to call \u0026ldquo;Have you met Ted?\u0026rdquo; Oh, come on, not this. 그럼 \u0026lsquo;테드 만나 봤어요?\u0026rsquo; 게임을 시작하자고 제발 이러지 마   Hi. Have you met Ted? No. - 테드 만나 봤어요? - 아뇨   Hi. Hi. - 안녕하세요 - 안녕하세요   Do you know Marshall? Lily? WOMAN: No. - 마셜이나 릴리 알아요? - 아뇨   Hmm. Do you know anyone at this party? 파티에 아는 사람 있어요?   I work with Carlos. Excuse me. - 카를로스 동료예요 - 잠시만요   Anyone know a Carlos? 카를로스 아는 분 계신가요?   No. No. 카를로스 아는 분 계신가요? - 아니 - 몰라   On a silver platter. Bon appétit. 다 차려진 밥상이야 맛있게 먹어   I don't think so. 난 됐어   Your loss, her gain. 네 손해지, 여잔 득 봤고   Excuse me. 잠시만요   It's magical up there. 옥상에 가실래요? 정말 멋지거든요   Sure. TED: Wait, wait. Hey, hey, I got that roof reserved. - 그래요 - 잠깐, 옥상은 내가 가야지   Dude, Robin's not coming. 로빈은 안 와   Hey, she's going to show up! 아니야, 곧 올 거야   She'll show up. 곧 오겠지   TED: She didn't show up. 로빈은 안 왔어   At least it was a great party. 그래도 파티는 재미있었어   I ate, like, four whole cans of dip. 난 소스만 네 통 먹었어   You always know what to say, old friend. 역시 넌 무슨 말을 해야 할지 아네   It's Robin. 로빈이야   No, no, not right away\u0026ndash; got to seem casual. 받아 봐 바로 받으면 안 돼 가벼워 보여야지   Hello 여보세요?   I'm so sorry I missed your party. 파티에 못 가서 미안해요   Who is this? Meredith? 누구죠? 메러디스?   Robin. 로빈이에요   Oh, Robin! 로빈이군요!   Hey! Yeah, I, uh, guess you never showed up, did you? 아 네! 파티에는 안 왔더군요   No, I got stuck at work. But, they finally got that kid out of the crane machine. 계속 일했거든요 그래도 애는 기계에서 빼냈어요   Did he get to keep the purple giraffe? 자주색 기린 인형은 가져갔어요?   Yeah, they let him keep all the toys. 네, 아이한테 인형을 다 줬어요   He was in there a long time, and little kids have small bladders. 애가 오래 갇혀 있느라 쉬를 못 참았거든요   \u0026ldquo;something\u0026rdquo; was tonight. 파티를 오늘 했으면 좋았을걸   It is\u0026ndash; the party's tonight. 그게\u0026hellip; 오늘도 파티해요   Yeah, uh\u0026hellip; it's a two-day party, \u0026lsquo;cause that's just how we roll. 이틀에 걸쳐서 해요 우린 그렇게 놀죠   Uh, so, if you want to swing by, you know, it's casual. See ya. 그러니 오고 싶으면 편안하게 와요, 끊어요   So, that was Robin. 로빈이었어   What are you doing to me, man?! I got a paper to write! I know! Sorry! It's terrible! - 나 리포트 써야 해! - 미안! 어쩔 수 없었어   I'll buy more dip! Ted! Ted, wait! - 소스 더 사 올게! - 테드, 기다려!   Get French onion! 프렌치 양파 맛으로 사!   I got a paper to write. 쟤 너무하지 않아? 리포트는 어쩌라고   Okay, fine. But, it's got to be, like, super-quick, 알았어 근데 빨리 끝내야 해   and no cuddling after. 끝나고 안아 주는 것도 없어   I'm the luckiest girl alive. 난 제일 운 좋은 여자야   You were so right about the roof! 네 말이 맞았어 옥상 최고더라!   The roof! The roof is on fire, Ted! 정말이지 옥상은 정열이 불타는 곳이야   That girl from last night I took her back to my place, 어제 그 여자를 집에 데려갔다가   then this morning, took her outside, spun her around a couple times and sent her walking. 아침에 밖에서 몇 바퀴 돌고 걸어서 집에 보냈지   She will never find her way back, and there she is. 절대 못 찾아\u0026hellip; 올 줄 알았는데 여기 있네   How did she get here? Did you invite her? 어떻게 왔지? 설마 네가 초대했어?   She said she works with Carlos. Who's Carlos? - 난 모르는 여자야 - 카를로스 동료라잖아 카를로스가 누구야?   I don't know any Carlos. 난 누군지 몰라   Hi, you! You're back! 안녕, 다시 왔네요!   I sure am. 당연하죠   Come on, sweetie, I need a drink. 자기, 나 술이 필요해   \u0026ldquo;Sweetie\u0026rdquo;? Really? 자기라고?   Come on, I got that roof reserved. 얍삽한 놈들! 오늘 옥상은 내가 찜했어   All right. 알겠어   So, it's over between me and works-with-Carlos girl. 카를로스 동료인 여자와는 이제 끝났어   Whoa! That was fast. 그렇게 빨리?   Yeah. I was trying to think, what's the quickest way to get rid of a girl you just met? 방금 만난 여자를 떼어 낼 가장 빠른 방법을 생각했지   I think I'm in love with you. What?! - 당신을 사랑해요 - 뭐라고요?   Thanks, bro. Glad I could help. - 고마워 - 도움이 됐다니 기쁘네   What the\u0026hellip; No, no, no. Come on. 뭐야, 안 돼!   Sorry, Ted. 미안하다   Great. What am I going to do when Robin shows up? 이제 로빈 오면 어떡해   She'll show up. 곧 올 거야   She didn't show up. 로빈은 안 왔다   We threw two parties. Everybody had fun. 파티도 두 번이나 했고 다들 즐겁게 놀았어   Everybody wanged, everybody chunged. 신나게 춤추고 놀았지   Now, the kid has got to get to work, and the kid is not to be disturbed. Repeat after me. 난 이제 리포트 써야 해 방해받기 싫으니까 따라 해   I will not have sex with Marshall. 난 마셜과 안 자겠습니다   BOTH: I will not have sex with Marshall. 난 마셜과 안 자겠습니다 - 난 마셜과 안 자겠습니다 - 난 마셜과 안 자겠습니다   It's Robin. 로빈이야   Hi, Ted. Amanda? - 여보세요? - 테드, 나예요   Oh, Denise! Sorry, you totally sounded like Amanda. 어맨다? 데니즈! 미안해요 어맨다랑 비슷해서   Oh, Robin. Hi. - 로빈이에요 - 로빈이었군요   I totally wanted to come. I got stuck at work again. I feel like I live there. 파티에 정말 가고 싶었는데 직장에 매여 있었어요 진짜 거기 사는것같아요   I'm sorry I missed your party, again. 또 못 가서 미안해요   Hey, ain't no thing but a chicken wing, mamacita. 괜찮아요 걱정 마요, 우리 아기   Who am I? 나 왜 이러지?   I guess there's no chance your two-dayer turned into a three-dayer? 아마도 3일 연속 파티하는 일은 없겠죠?   The party continues tonight. 당연히 있죠 오늘까지 파티해요   Yeah. Uh, last night, people were like, \u0026ldquo;Keep it going, bro. Party trifecta.\u0026rdquo; 어제 사람들이 계속 3일 연속 파티하라고 했죠   Wow! Okay, well, I'll be there. 정말요? 오늘은 꼭 갈게요   Great! See you tonight. 그래요! 오늘 밤에 봐요   So, that was Robin. 로빈 전화였어   So, I threw a third party for Robin\u0026hellip; on a Sunday night. 그래서 로빈을 위해 일요일 밤에 세 번째 파티를 했어   Well, this is lame. 시시하네   Lame\u0026hellip; or casual? 시시한 거야? 편안한 거야?   Lame. 시시해   Or casual? 가볍지 않아?   Hey, law books. Ready for a little 15 minute recess? 우리 자기, 공부하다가 쉬러 나온 거야?   Sorry, baby, I got to work. I need all my blood up here. 미안해, 나 할 일이 많아 피를 머리로 보내야 해   Has anybody seen An Introduction to Contract Tort and Restitution Statutes from 1865-1923? 1865-1923 계약 위반 관련 법률 개론서 본 사람 없어?   Anybody seen a big-ass book? 무식하게 큰 책 못 봤어?   ALL ( muttering ): No. 못 봤어   WOMAN: Hello, Barney. 바니, 안녕   Of course. 그럼 그렇지   You look well. 좋아 보이네요   Is it weird they invited both of us? 우리 둘 다 초대하다니 이상하죠?   Who? 대체 누구죠?   Who invited you? No one even knows who you are! 누가 초대했어요? 아무도 당신을 모르는데   I understand you're hurt, but you don't have to be cruel. 상처받은 건 알겠지만 잔인할 것까진 없잖아요   Carlos was right about you. 카를로스 말이 맞았네요   Who is Carlos?! 대체 그게 누구죠?   Hey, where the hell is my\u0026hellip;? 대체 그게 누구죠? 내 책 본 사람\u0026hellip;   Okay\u0026hellip; 좋아   An Introduction to Contract Tort and Restitution Statutes from 1865-1923 1865-1923 계약 위반 관련 법률 개론서는   is not a coaster! 잔 받침이 아니야!   Ted, I'm jeopardizing my law career so you can throw not one, not two, but three parties for some girl that you just met who's probably not even going to show up! 테드, 난 경력 망칠 각오 하고 파티를 열게 도와줬어 그것도 오지 않을 여자를 위해서 세 번씩이나 말이야!   I mean, where is she, Ted, huh? Where's Robin? 그 여자 어디 있어? 로빈은 왔냐고!   Hi. Hi, Robin. 안녕하세요, 로빈   So, you threw all these parties for me? 나 때문에 파티를 계속한 거예요?   Oh, you thought that\u0026hellip; no! 절대 아니니까 그렇게 생각하지 말아요   I\u0026hellip; okay, yes. You got me. 난\u0026hellip; 그래, 맞아요   One of the reasons I threw these parties was so that I could introduce you to, um, this guy. 내가 파티를 했던 이유는 내 친구를 소개해주려고요 바로 얘요   Uh, I figured, you know, since it didn't work out between us and now we can just laugh about it\u0026hellip; 우리 둘이 잘 안된 건 그냥 가볍게 웃어넘기고\u0026hellip;   Anyway, Robin, this is\u0026hellip; 어쨌든 로빈, 이 친구는   Carlos. 카를로스예요   She's still talking to Carlos. 아직도 카를로스랑 얘기해   I can still win this. I-it's not over. 아직 가능성 있어 안 끝났다고   Okay, buddy. Time for the tough talk. 그래 알았어 냉정하게 얘기해줄게   Robin seems great, but let's look at the facts. 로빈은 멋진 여자지만 현실을 봐   You want to get married. 넌 결혼하고 싶고   And right now, there's a million women in New York looking for exactly you. 뉴욕에는 이런 널 원하는 수천 명의 여자가 있어   But Robin ain't one of them. 하지만 로빈은 아니야   She's not just one of them. She's the one. 로빈은 달라 내 인연이라고   Yeah, well, the one is heading up to the roof. 네 인연이 지금 옥상으로 가고 있는데?   What are you going to do? 어쩔 거야?   It's a game. I got to just keep playing it. 어쩌긴 뭘 어째 그냥 게임을 계속해야지   ( rock music playing ) \u0026ldquo;시작\u0026rdquo;   Ted\u0026hellip; 테드   Hey, Carlos, can you give us a minute? 카를로스 자리 좀 비켜줄래요?   Hey, no sweat, hombre. 그러죠   See ya. 다음에 봐요   Robin\u0026hellip; 로빈   look, I didn't throw this party to set you up with Carlos, 친구를 소개하려고 파티한 게 아니에요   or the one before that, or the one before that. 어제 파티도 그 전에 한 파티도요   I threw these parties because I wanted to see you. 로빈을 보고 싶어서 파티를 열었어요   Well, here I am. 그래서 왔잖아요   There's something here, look, unless I'm crazy. 뭔가 있는 것 같아요 미친 건지도 모르죠   You're not crazy. 안 미쳤어요   I don't know, Ted. I mean, we barely know each other 테드, 잘 모르겠어요 서로 잘 알지도 못하는데   and you're looking at me with that look. And, it's like\u0026hellip; 당신이 날 보는 시선은 마치\u0026hellip;   Like, like what? - 어떤데요? - 마치\u0026hellip;   Like, \u0026ldquo;Let's fall in love and get married and have kids and drive them to soccer practice.\u0026rdquo; - 어떤데요? - 마치\u0026hellip; 당장 결혼하고 애 낳아서 축구 교실에 보내자는 것 같아요   I'm not going to force sports on them unless they're interested. 애들에게 강요하진 않을 거예요   It's a great look. But you're looking at the wrong girl. 멋진 눈빛이지만 내게는 아니에요   No, I'm not. Yes, you are. - 아니에요 - 맞아요   I don't want to get married right now, maybe ever. 지금 당장은 결혼하고 싶지 않아요 어쩌면 난 평생 안 할지도 몰라요   And, if we got together, 그런데도 우리가 사귄다면   I'd feel like I'd either have to marry you or break your heart, and\u0026hellip; 결혼을 해야 하든가 상처를 주게 되겠죠   I just couldn't do either of those things. 난 그렇게는 못 하겠어요   Just like you can't turn off the way you feel. 당신이 감정을 주체할 수 없듯이요   Click. Off. Let's make out. 클릭, 대화 종료 우리 키스나 하죠   What? 뭐라고요?   That was the off switch. And I turned it off. 방금 전원 버튼을 끈 거예요   I mean, look, sure, yes, I want to fall in love, get married, blah, blah, blah. 그래요, 난 사랑에 빠지고 결혼도 하고 싶어요   But, on the other hand\u0026hellip; 하지만 지금은 당신과 내가\u0026hellip;   you, me, the roof. 옥상에 있잖아요   There's no off switch. 전원 버튼은 없어요   There is an off switch. And it's off. 있어요 지금은 꺼져 있고요   No, it's not. 없는데\u0026hellip;   Yes, it is. 정말 있어요   No, it's not. 없어요   Yes\u0026hellip; it is. 있는데 꺼졌어요   No, it's not. 없네요   There's no off switch. 당신이 맞았어요 전원 버튼은 없어요   God, I wish there was an off switch! 세상에! 있었으면 좋겠네요   Me, too. 나도 그래요   So, um\u0026hellip; 그럼\u0026hellip;   What do we do now? 우린 이제 어떡하죠?   We could be friends. 친구는 어때요?   I know it sounds insincere when people say that, but\u0026hellip; we could. 이런 말 진심이 아닌 것처럼 들릴 수도 있겠지만 가능해요   I don't know, Robin. 난 모르겠어요   I've made such a jackass of myself here. We start hanging out, every time I see you 너무 얼간이 짓을 해서 앞으로 만날 때마다   it'll be like, \u0026ldquo;Oh, that's right. I'm a jackass.\u0026rdquo; 내가 얼간이 같을 거예요   You're not a jackass. 얼간이 아니에요   Look, I'm sorry. I only moved here in April and I'm always working 미안해요, 막 이사 온 데다 계속 일만 해서   and I just haven't met a lot of good people so far. 아직 사람을 많이 못 만나 봤어요   But I understand. 하지만 이해해요   Well, uh, maybe in a few months, after it's not so fresh, 몇 달 후에 기억이 좀 흐려지면   we could all, uh, you know, get a beer. 맥주 한잔하는 거 어때요?   Yeah. That sounds good. 좋아요, 그거 괜찮네요   I'll see you, Ted. 안녕, 테드   Or, you know, now. 아니면 지금이라도   We could all get a beer now. 맥주 마시는 건 어때요?   I'd like that. 좋아요   My friends are going to love you\u0026hellip; like you, you know, as a friend. 내 친구들 모두 사랑할\u0026hellip; 좋아할 거예요, 친구로서   Jackass. 바보 같긴   That's just a recipe for disaster. They work together! 믿기지 않아 화를 자초하는군 회사 동료라면서 회사 동료라면서   Are you jealous? 질투 나?   What does Carlos have that I don't? 내가 쟤보다 뭐가 모자라지?   A date tonight. 데이트 상대?   Stop the tape. Rewind. Play it again. 테이프를 되감아 볼까요?   ( imitates tape rewinding ) A date tonight. 데이트 상대?   I'm not sure I like her. 이 사람 별로야   Hey, don't you have a paper to write? 너 리포트 쓴다며?   Dude, you're talking to the kid. I know it. - 지금 누구한테 묻는 거야 - 알겠어   I'm going to knock back this beer. I'm going to knock back one more beer. 일단 맥주 한 잔 하고 한 잔 더 마신 다음에   I'm going to go home. I'm going to write a 25-page paper. 집에 가서 리포트 25장 쓸 거고   I'm going to hand it in and I'm going to get an \u0026ldquo;A.\u0026rdquo; 내일 제출해서 A를 받을 거야   My name is Rufus and that's the trufus. 나는 말하는 대로 이루어지거든   TED: He got a B-minus. 마셜은 B-를 받았다   But still, 25 pages in one night, B-minus? The kid was good. 그래도 하룻밤에 25장 쓰고 B-를 받다니 굉장한 거다   At least let me buy you a beer. Come on, I'll buy everyone a beer. 내가 한잔 살게 모두한테 쏘지   I'll help carry. 같이 들어 줄게   You know something, Ted? What? - 너 그거 알아? - 뭐를?   You are a catch. 넌 매력적이야   You're going to make some girl very happy. 언젠가 한 여자를 행복하게 할 거야   And I am going to help you find her. 내가 그 여자 찾게 도와줄게   Well, good luck. 그래, 잘해 봐   I mean, maybe New York's just too big a town. I mean, there's millions of people in this city. 뉴욕은 너무 큰 도시잖아 거주자만 수백만 명이지   How, in all this mess, is a guy supposed to find the love of his life? 이런 복잡한 곳에서 어떻게 운명의 상대를 찾겠어   I mean, where do you even begin? 뭐부터 해야 하냐고   Have you met Ted? 저기요, 테드 만나 봤어요?    "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/handover/",
	"title": "Handover",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "chapter 4에서 Ubuntu LAMP server 예제는 Apache를 재시작하는 간단한 handler를 사용하였고 Apache의 configuration에 영향을 주는 특정한 task들이 notify: restart apache라는 옵션으로 handler에게 noti를 줬었다.\nhandlers: - name: restart apache service: name=apache2 state=restarted tasks: - name: Enable Apache rewrite module apache2_module: name=rewrite state=present notify: restart apache 어떤 상황에서는 여러개의 handler에 notify를 해야할 상황이 있을 수도 있다. 또는 어떤 handler는 다른 추가적인 handler에게 notify를 해야할 수도 있다. 이 두가지 모두 Ansible에서는 쉽게 할 수 있다. 하나의 task에서 여러 handler에게 notify를 하려면 notify 옵션을 list로 작성하면 된다.\n- name: Rebuild application configuration command: /opt/app/rebuild.sh notify: - restart apache - restart memcached handler가 다른 handler에게 noti를 주려면 notify 옵션을 handler에 추가하면 된다. handler는 기본적으로 notify 옵션으로 호출되는 glorified task이다. 하지만 스스로 task처럼 행동하기 때문에 다른 handler와 연계할 수 있다.\nhandlers: - name: restart apache service: name=apache2 state=restarted notify: restart memcached - name: restart memcached service: name=memcached state=restarted handler를 사용할 때 몇가지 고려해야할 사항들이 더 있다.\n handler는 task가 handler에게 notify를 할 때만 실행된다. 만약 handler를 notify하는 task가 when condition이나 다른 이유로 생략되었다면 handler는 실행되지 않는다. handler는 play가 끝날때 딱 한번만 실행된다. 이 동작을 override하여 playbook 중간에 handler가 실행되기를 원한다면 meta module을 사용하면 그렇게 할 수 있다. (e.g. - meta: flush_handlers) handler에게 noti를 주기 전에 play가 특정 host(또는 모든 host)에서 fail이 나면 handler는 절대 동작하지 않는다. playbook이 실패했을 때에도 무조건 handler가 동작하기를 원한다면 meta module로 해당 task를 playbook에서 분리하거나 playbook을 실행할 때 command line flag인 --force-handlers를 사용하면 된다. handler는 playbook이 동작하는 동안 어떤 호스트라도 접속이 안되는 경우 동작하지 않는다.  "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/getting-started-with-ansible/",
	"title": "Chapter 1 - Getting Started With Ansible",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "Ansible and Infrastructure Management On snowflakes1 and shell scripts 보통은 SSH를 통해서 접속하여 필요한 작업을 하고 접속을 종료한다. 이 때, 어떤 것들은 기록되고 어떤 것들은 기록되지 않는다. 결국 관리자가 똑같은 작업을 여러 서버에 해야하는 책임소지가 있다.\n서버가 동작 중일 때 몇가지 변경사항이 생기고 적용할 방법이 쉽다면 문제가 되진 않을 것이다. 그러나 불행하게도 대부분은 그렇지 않다.\n이 때 기존과 똑같은 서버를 만드려고 한다면 정말 많은, 쓸데없는 시간을 소비하게 된다.\nshell script로 보완을 하려고 하지만, 모든 edge case를 커버하기란 어려운게 현실이다.\nConfiguration management CFEngine, Puppet, Chef같은 서비스들이 이런 Infrastructure를 구성하는 툴로 사용되고 있었다. 하지만 command line configuration이 아닌 다른 방법을 새로 익혀야 한다. Puppet이나 Chef는 Ruby나 기타 커스텀 언어들에 대한 이해가 필요하다.\n반면 Ansible은 command line을 그대로 실행하게 한다. 따라서 기존의 스크립트를 이용할 수 있다. 이를 idempotent2 playbook으로 전환할 수 있다. command line에 익숙한 사람들에게는 훨씬 좋은 옵션이 될 것이다.\nAnsible은 변경사항들을 모든 서버에(default) push하고 추가적인 소프트웨어를 서버에 설치할 필요가 없다. 따라서 memory footprint, 관리를 위한 추가적인 daemon같은 것은 없다.\nIdempotence 멱등성은 여러번 동작을 해도 동일한 결과가 나온다는 속성이다.\n이는 configuration management tool의 중요한 기능으로 몇번을 실행하더라도 동일한 설정이 유지되어야 한다는 것을 의미한다. 많은 shell script가 한 번 이상 실행되면 의도하지 않은 결과를 발생시키는데, Ansible은 추가적인 설정 없이 동일한 결과가 나오게 한다.\n사실 대부분의 Ansible modules와 commands는 idempotent이고, 그렇지 않은 것은 Ansible에서 언제 command가 동작해야 하는지, 변경되거나 실패한 command의 구성이 어떤지를 제공하여 모든 서버에 대해 idempotent configuration을 유지하도록 도와준다.\nInstalling Ansible Ansible은 Python에만 dependency가 있다. Python이 있으면 pip로 설치가 가능하다.\nMac 환경이면 더 쉽다.\n Homebrew 설치 (홈페이지에서 설치) Python 2.7.x 설치 (brew install python) Ansible 설치 (sudo pip install ansible)  brew install ansible로 설치해도 되고 이 경우 update 시 brew를 사용하면 된다.\nWindows를 사용한다면 조금은 복잡하다. 둘 중 하나를 선택해서 설치하면 된다.\n Linux Virtual Machine을 사용해서 설치하기 Cygwin 환경에서 Ansible이 동작하도록 하기  Linux를 사용한다면 이미 Ansible이 설치되어 있을 수 있다.\npython-pip와 python-devel이 있다면 pip를 통해 Ansible을 설치할 수 있다. 이 때 \u0026ldquo;Development Tools\u0026quot;가 이미 설치되어 있어 gcc, make 등이 설치되어있음을 가정한다.\nsudo pip install ansible Fedora같은 시스템은 yum 패키지를 통해 설치하는 것이 가장 쉽다. RHEL/CentOS는 Ansible 설치 전에 EPEL의 RPM을 설치해야한다.\nyum -y install ansible Ubuntu에서의 가장 간단한 설치 방법은 apt package를 통해 설치하는 것이다.\nsudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible Ansible이 설치되고 나면 ansible --version으로 제대로 설치되어 있는지 확인한다.\nCreating a basic inventory file Ansible은 inventory file(보통, 서버들의 리스트)을 사용하여 서버와 통신한다. IP 주소를 domain name과 매칭시키는 hosts 파일처럼 Ansible inventory file은 서버(IP address이나 domain name)을 group에 매칭시킨다. Inventory file은 이보다 더 많은 것을 할 수 있지만 지금은 간단하게 하나의 서버에 대한 파일만 생성할 것이다. /etc/ansible/hosts(Ansible inventory file의 default location)에 파일을 다음과 같이 추가한다.\nsudo mkdir /etc/ansible sudo touch /etc/ansible/hosts 이제 파일을 수정한다. 이 때 sudo 권한으로 수정해야 한다.\n[example] www.example.com example은 우리가 관리하게 될 서버의 그룹이고, www.example.com은 그 그룹에 속하게 된다. ssh 포트가 22가 아니라면 www.example.com:2222처럼 하면 된다. Ansible이 ssh configuration을 확인하여 지정된 포트를 불러오지 않기 때문에 포트가 다를 경우 반드시 명시해 주어야 한다.\nRunning your first Ad-Hoc Ansible command Ansible과 inventory file을 설치하였으니, command를 실행시켜 잘 되는지 확인해볼 수 있다.\nansible example -m ping -u [username] 여기서 [username]은 ssh 접속할 때 사용하는 user를 입력하면 된다. 잘 된다면 ping의 결과들이 보일 것이고 안된다면 -vvvv 옵션을 추가해서 세부 결과를 확인할 수 있다. ssh username@www.example.com이 성공한다면 위의 명령어도 당연히 성공해야 한다.\nAnsible은 passwordless authentication을 가정한다. 따라서 ssh에 패스워드를 입력한다면, ssh-copy-id를 통해 패스워드 입력을 없앨 수 있다.\nansible example -a \u0026#34;free -m\u0026#34; -u [username] 위처럼 memory usage를 확인할 수 있다. 또한 df -h로 disk usage도 확인이 가능하다. 이런 방법으로 서버들에 에러가 없는지 확인할 수 있다.\nSummary configuration management와 Ansible에 대해 학습하였고, 이를 설치하고 서버에 대해 이야기하며 Ansible을 통해 서버에서 command를 실행시켜보았다.\n  문서화 하지 않고 구성하였기 때문에, 한번 구성하고 나면 동일하게 설정하기 힘든, 눈처럼 녹아버리는 서버 \u0026#x21a9;\u0026#xfe0e;\n 멱등의. 아무리 여러번해도 결과가 같음. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/install-kubeadm/",
	"title": "Install Kubeadm",
	"tags": ["install", "kubeadm"],
	"description": "",
	"content": "kubeadm은 Kubernetes cluster의 설정들을 관리하는 툴입니다.\nPrerequisites 먼저, 몇가지 전제사항이 있습니다.\n  모든 노드의 MAC 주소와 product_uuid가 달라야 합니다. ifconfig -a와 sudo cat /sys/class/dmi/id/product_uuid를 통해 알 수 있습니다.\n  network adapter를 확인합니다. 하나 이상의 network adapter가 있고, Kubernetes component들이 default route로 통신이 불가능하다면 IP route를 설정하여 Kubernetes cluster 주소가 적절한 adapter를 통해 이동할 수 있도록 해주는 것이 좋습니다.\n  iptables를 사용하는지 확인 Ubuntu 19.04 이후버전부터는 nftables라는 것을 사용한다고 합니다. 그러나 이는 kube-proxy와 호환이 잘 안되기 때문에 iptables를 사용해야한다고 하고 있습니다.\n다음과 같이 설정할 수 있습니다.\nsudo update-alternatives --set iptables /usr/sbin/iptables-legacy sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy sudo update-alternatives --set arptables /usr/sbin/arptables-legacy sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy   필수 포트 확인\nControl-plane node(s)\n   Protocol Direction Port Range Purpose Used By     TCP Inbound 6443* Kubernetes API server All   TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd   TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 10251 kube-scheduler Self   TCP Inbound 10252 kube-controller-manager Self    Worker node(s)\n   Protocol Direction Port Range Purpose Used By     TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 30000-32767 NodePort Services** All    여기서 * 표시가 있는 것은 수정이 가능한 사항이라고 합니다. (API server의 port, NodePort Service로 열 수 있는 port의 범위)\n  container runtime 설치 여기에서는 Docker를 사용할 것입니다. Docker 홈페이지에는 script를 통해 최신 버전의 Docker를 다운로드 받는 방법을 제공합니다.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh   kubeadm, kubelet, kubectl 설치 Kubernetes에 필요한 세가지 툴을 설치하도록 하겠습니다. 간단하게 설명드리자면 kubeadm은 Kubernetes를 관리하는 툴이라고 생각하면 되고 kubelet은 명령을 이행하는 툴이라고 생각하면 됩니다. kubectl은 Kubernetes cluster와 통신하는 툴입니다.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 마지막에 apt-mark를 통해 kubelet, kubeadm, kubectl의 버전을 고정시켰습니다.\n  이렇게 kubelet, kubeadm, kubectl을 설치하였으면 마지막으로 swap 영역을 제거할 것입니다. Kubernetes는 swap 영역이 없는 것이 필수 항목입니다. 따라서 다음의 명령어로 swap 영역을 삭제합니다.\nsudo swapoff -a 이는 현재 세션에서만 동작하고 재부팅시 해제됩니다. 따라서 /etc/fstab을 열고 swap 부분을 주석처리합니다.\n#/swapfile none swap sw 0 0 Reference https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/\nhttps://kubernetes.io/docs/concepts/cluster-administration/networking/\nhttps://docs.projectcalico.org/v3.11/getting-started/kubernetes/\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/install-halyard/",
	"title": "Install Halyard",
	"tags": ["spinnaker", "install", "halyard", "proxy"],
	"description": "",
	"content": "halyard란? halyard는 Spinnaker를 배포할 때 사용하는 CLI 툴입니다.\nhalyard는 Spinnaker 관련 설정들의 validation, 배포한 환경 백업, 설정 추가 및 변경에 사용됩니다.\n설치 방법 선택하기 총 2가지 방법으로 halyard를 설치할 수 있습니다.\n Debian/Ubuntu나 macOS에 직접 설치하기 Docker 사용하기  Spinnaker Docs에서는 실제 Production 환경이라면 직접 설치하는 방법을, 그게 아니라 간단하게 사용하려면 docker를 사용해도 된다고 하고 있습니다.\n그리고 한가지의 옵션이 더 있습니다.\n 인터넷이 되지 않는 환경 (프록시나 방화벽 등으로 halyard를 통한 설치가 어려운 경우)  이 글을 작성하고 있는 환경은 인터넷이 잘 되는 환경입니다. 그리고 두가지 모두 시도해 보도록 하겠습니다.\nDebian/Ubuntu나 macOS에 직접 설치하기 공식 Docs에서 halyard는 다음과 같은 환경에서 동작한다고 말하고 있습니다.\n Ubuntu 14.04, 16.04 or 18.04 (Ubuntu 16.04 requires Spinnaker 1.6.0 or later) Debian 8 or 9 macOS (tested on 10.13 High Sierra only)  이제 직접 설치를 시작해보도록 하겠습니다.\n시작하기 전에, halyard를 설치하기 위해서는 root 계정이 아닌 계정이 필요합니다. 만일 root만 있다면 spinnaker를 위한 계정을 생성해 줍니다.\nadduser spinnaker 위처럼 생성한 계정에 sudoers 권한을 줍니다.\nadduser spinnaker sudo 최신 버전의 halyard 다운로드 Debian/Ubuntu:\ncurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/debian/InstallHalyard.sh macOS:\ncurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/macos/InstallHalyard.sh 설치 sudo bash InstallHalyard.sh 확인 hal -v 추가사항 . ~/.bashrc를 실행하여 bash completion 활성화\n여기서 proxy 환경이라면 halyard의 jvm에 proxy 옵션을 추가해주어야 합니다.\nvi /opt/halyard/bin/halyard을 통해 halyard의 jvm 옵션을 추가할 수 있습니다.\nDEFAULT_JVM_OPTS=\u0026#39;\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34; \u0026#34;-Dspring.config.additional-location=/opt/spinnaker/config/\u0026#34; \u0026#34;-Dhttps.proxyHost=\u0026lt;proxyHost\u0026gt; -Dhttps.proxyPort=\u0026lt;proxyPort\u0026gt;\u0026#34; \u0026#34;-Dhttp.proxyHost=\u0026lt;proxyHost\u0026gt; -Dhttp.proxyPort=\u0026lt;proxyPort\u0026gt;\u0026#34;\u0026#39; 위의 설정에서 다음과 같이 proxy를 추가해줍니다. 그 다음 halyard를 재시동합니다.\nhal shutdown hal config 아랫줄의 hal config는 의도적으로 halyard를 구동시키기 위함입니다.\ndocker로 halyard 사용하기 다음의 명령어는 공식 docs에서 제공하는 명령어입니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm \\  -v ~/.hal:/home/spinnaker/.hal \\  -d \\  gcr.io/spinnaker-marketplace/halyard:stable kubernetes로 배포하려 할 경우, kubectl 명령어에서 사용할 kubeconfig 파일이 필요합니다. 이 또한 -v 옵션으로 주어야 합니다. 그리고 그 kubeconfig 파일을 읽도록 설정해야 합니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm \\  -v ~/.hal:/home/spinnaker/.hal \\  -v ~/.kube:/home/spinnaker/.kube \\  -e KUBECONFIG=/home/spinnaker/.kube/config \\  -d \\  gcr.io/spinnaker-marketplace/halyard:stable 사실 5번째 줄의 -e KUBECONFIG=/home/spinnaker/.kube/config은 없어도 default로 들어가있는 설정입니다. 하지만 혹시나 위에서 /home/spinnaker/.kube가 아닌 다른곳을 저장공간으로 둔다면 아래의 설정도 바뀌어야 합니다.\n프록시 환경이라면 다음과 같이 JAVA_OPT를 추가해주어야 합니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm -d \\  -v ~/.hal:/home/spinnaker/.hal \\  -v ~/.kube:/home/spinnaker/.kube \\  -e http_proxy=http://\u0026lt;proxy_host\u0026gt;:\u0026lt;proxy_port\u0026gt; \\  -e https_proxy=https://\u0026lt;proxy_host\u0026gt;:\u0026lt;proxy_port\u0026gt; \\  -e JAVA_OPTS=\u0026#34;-Dhttps.proxyHost=\u0026lt;proxy_host\u0026gt; -Dhttps.proxyPort=\u0026lt;proxy_port\u0026gt;\u0026#34; \\  -e KUBECONFIG=/home/spinnaker/.kube/config \\  gcr.io/spinnaker-marketplace/halyard:stable 그래도 안된다면.. 아마도 관리가 엄격한 네트워크를 사용하고 계실 것이라고 예상됩니다. 저 또한 그랬으니까요.\nhalyard는 설정 및 버전등의 정보를 bucket (Google Cloud Storage)로 관리한다고 합니다. 따라서 이곳으로 연결이 되지 않는다면 validation, version list 등의 상황에서 timeout이 날 것입니다.\nSpinnaker에서는 이를 확인하기 위해 gsutil을 사용하여 bucket의 주소인 gs://halconfig에 연결할 수 있는지 확인해보라고 합니다.\n또는 curl을 이용해서도 확인이 가능합니다.\n먼저 gsutil은 google storage 서비스에 접속하는 CLI 툴입니다. Docs에서 설치방법을 확인하여 설치할 수 있습니다.\n설치가 완료되었다면 gsutil로 접속이 가능한지부터 확인합니다.\ngsutil ls gs://halconfig 두번째로 curl을 사용하는 방법입니다.\ncurl storage.googleapis.com/halconfig 결과물들이 나온다면 정상적으로 bucket에는 접속이 가능한 것입니다. hal config 명령어가 성공하지 않지만 bucket에 접속이 가능한 케이스는 아직 보지 못했습니다.\n우선 여기까지 해서 bucket에 접속이 불가능하다고 판단이 되면, 인터넷이 없는 환경에서 설치하는 방법을 고려해보아야 합니다. 또는 googleapis.com url로 proxy에서 사이트가 차단되었는지 확인하고, SSL도 해제할 경우 해결될 수도 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/english/himym/",
	"title": "HIMYM",
	"tags": [],
	"description": "",
	"content": "English How I Met Your Mother Script : https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=how-i-met-your-mother\n Season1    Episode01     Episode02    Ibiza    Font Change      Hugo에 Google Analytics 적용하기     Hugo에 Comment 추가하기 (Utterance)     HUGO로 HTML이 되지 않을 때 가능하게 하는 방법       "
},
{
	"uri": "http://kimmj.github.io/ibiza/2020-plan/",
	"title": "2020 Plan",
	"tags": ["blog", "hugo"],
	"description": "",
	"content": "2020 새해에는 몇가지 목표가 있다.\n 꾸준하게 이 블로그 운영하기 꾸준하게 영어공부 하기 (쉐도잉) 꾸준하게 운동하기 나만의 hugo blog 만들기 적금으로 목돈만들기 개인 공부 많이 하기 CKA 취득  적다보니 너무 많아진 감이 없지않아 있지만, 올해는 자기계발을 많이 할 수 있는 한해가 되었으면 한다.\n특히 지금은 누군가가 만든 블로그 테마를 사용하고 있지만 나중에는 내가 원하는 대로 커스터마이징이 가능하도록 나만의 블로그 테마를 만들고 싶다.\n이를 위해서는 무엇이 필요한지도, 어떤 기술 스택을 쌓아야 할지도 모르지만 일단 도전해보고자 한다.\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/environment-variables/",
	"title": "Environment Variables",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "Ansible은 다양한 방법으로 environment variable을 사용할 수 있도록 해준다. 첫번째로 remote user account에 대해 어떤 environment variable을 설정하고자 한다면 remote user의 .bash_profile에 다음과 같이 추가하면 된다.\n- name: Add an environment variable to the remote user\u0026#39;s shell lineinfile: dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value 그러면 다음에 실행되는 모든 task는 이 environment variable에 접근할 수 있다. (물론 shell module만 environment variable을 사용하는 shell command를 이해할 것이다!) environment variable을 나중 task에서 사용하려면 task의 register 옵션을 사용하여 environment variable을 variable에 저장하여 Ansible이 나중에 사용할 수 있도록 하는 것을 추천한다. 예를 들면 다음과 같다.\n- name: Add an environment variable to the remote user\u0026#39;s shell. lineinfile: dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value - name: Get the value of the environment variable we just added. shell: \u0026#39;source ~/.bash_profile \u0026amp;\u0026amp; echo $ENV_VAR\u0026#39; register: foo - name: Print the value of the environment variable. debug: msg=\u0026#34;The variable is {{ foo.stdout }}\u0026#34; Ansible이 remote user의 최신 environment variable을 사용하고 있는지를 확실하게 하기 위해 우리는 4번째 줄에 있는 source ~/.bash_profile을 사용한다. 어떤 상황에서는 task가 $ENV_VAR가 아직 정의되지 않은, 계속해서 동작중인 세션이나 quasi-cached SSH session을 사용하기 때문이다.\n(처음으로 debug module이 등장했다. 이는 나중에 다른 debugging techniques들과 함께 깊게 파볼 것이다.)\n왜 ~/.bash_profile인가? user의 home folder에는 .bashrc, .profile, .bash_profile같은 environment variable을 저장할 수 있는 많은 파일이 있다. 우리의 경우 environment variable이 pseudo-TTY shell session을 사용하는 Ansible에서 사용할 수 있기를 원하기 때문에 .bash_profile로 environment를 설정할 수 있다. shell session configuration과 이 dotfiles에 대해서는 여기를 통해 더 알아볼 수 있다. (Configuring your login sessions with dotfiles)[http://mywiki.wooledge.org/DotFiles]\n Linux는 /etc/environment에 추가된 global environment variable도 읽기 때문에 그곳에 추가해도 된다.\n- name: Add a global environment variable. lineinfile: dest=/etc/environment regexp=^ENV_VAR= line=ENV_VAR=value sudo: yes 어떤 경우에던지 server에 있는 environment variable을 lineinfile로 관리하는 것은 꽤나 간단하다. 만약 어플리케이션이 많은 environment variable을 필요로 한다면(대다수의 Java application의 경우처럼) lineinfile로 많은 아이템의 리스트를 작성하는 것보다 copy나 template으로 local file을 사용하는 것을 고려해볼 수 있다.\nPer-play environment variables 또한 특정 play에 대해 environment 옵션을 사용하여 하나의 play에 대해서만 environment를 설정할 수 있다. 예를 들어, 특정한 파일을 다운로드하기 위해 http proxy 설정이 필요하다고 해보자. 이는 다음과 같이 할 수 있다.\n- name: Download a file, using example-proxy as a proxy. get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: http_proxy: http://example-proxy:80/ 트록시나 다른 environment variable을 필요로 하는 task가 많은 경우에 특히 성가실 수 있는 일이다. 이러한 경우 environment를 playbook의 vars 섹션에(또는 variable file을 include하여) variable을 통해 전달할 수 있다.\nvars: var_proxy: http_proxy: http://example-proxy:80/ https_proxy: https://example-proxy:443/ [etc...] task: - name: Download a file, using example-proxy as a proxy. get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: var_proxy proxy가 시스템 전반적으로 설정되어야 한다면(대다수의 기업 방화벽을 사용하는 경우) /etc/environment 파일에 설정하는 것을 선호하는 편이다.\n# In the \u0026#39;vars\u0026#39; section of the playbook (set to \u0026#39;absent\u0026#39; to disable proxy): proxy_state: present # In the \u0026#39;tasks\u0026#39; section of the playbook: - name: Configure the proxy. lineinfile: dest: /etc/environment regexp: \u0026#34;{{ item.regexp }}\u0026#34; line: \u0026#34;{{ item.line }}\u0026#34; state: \u0026#34;{{ proxy_state }}\u0026#34; with_items: - { regexp: \u0026#34;^http_proxy=\u0026#34;, line: \u0026#34;http_proxy=http://example-proxy:80/\u0026#34; } - { regexp: \u0026#34;^https_proxy=\u0026#34;, line: \u0026#34;https_proxy=https://example-proxy:443/\u0026#34; } - { regexp: \u0026#34;^ftp_proxy=\u0026#34;, line: \u0026#34;ftp_proxy=http://examle-proxy:80/\u0026#34; } 이렇게 하는 방법은 proxy가 서버에서 enable 되어있더라도 설정할 수 있도록 하며 한번 play한 뒤 http, https, ftp proxy들을 설정한다. 이런 비슷한 방법으로 시스템 전역에 설정되어야 하는 environment variable들을 설정할 수 있다.\nremote environment variable들을 ansible command를 통해 테스트 할 수 있다. (ansible test -b shell -a 'echo $TEST') 이렇게 할 때 quotes를 통해 escape를 해야함에 주의를 해야한다. double quotes나 single quotes를 사용할 수도 있다. 그렇지 않으면 remote server의 environment variable이 아닌 local server의 것을 출력할 수도 있다.\n{{$ /notes $}}\n "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/local-infrastructure-development-ansible-and-vagrant/",
	"title": "Chapter 2 - Local Infrastructure Development: Ansible and Vagrant",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "Prototyping and testing with local virtual machines Ansible은 remote, local 가리지 않고 연결할 수 있는 서버면 모두 잘 동작한다. 일반적으로 테스트를 할 때, Ansible Playbook 개발 속도를 빠르게 하기 위해 로컬로 테스트한다. 로컬로 하는 것이 실제 환경에서 테스트하는 것보다 훨씬 안전하다.\n최근 트렌드는 단연 TDD이다. 따라서 Infrastructure에도 테스트는 필요하다.\n소프트웨어에 대한 변경사항은 수동 또는 자동적으로 이루어진다. 이러한 것들이 Ansible과 다른 개발, configuration management 툴과 함께 구현되어 테스트를 할 수 있도록 구현되어있다. 단순하게 로컬에서 테스트를 해본다고 한들, 하지 않는 것(cowboy coding)보다 수천배 낫다.\n cowboy coding: production 환경에서 직접 작업하며, 문서화나 코드의 변경점을 감싸지 않는 방법이며, roll back에 대한 대응책이 없다.\n지난 십년간 많은 가상화 툴들이 개발되었고, 이에따라 로컬에서 infrastructure emulation을 할 수 있게 되었다. 중요한 서버에 장애를 내지 않고도 여러 테스트를 마음껏 할 수 있다. 단순히 VM을 새로 만들면 되기 때문에 실제 application의 downtime은 존재하지 않을 것이다.\nVagrant와 VirtualBox로 테스트 인프라를 구축할 수 있고 개별적인 서버 구성을 할 수 있다.\n여기에서는 Vagrant와 VirtualBox로 Ansible을 테스트 할 새로운 서버를 만들 것이다.\nYour first local server: Setting up Vagrant 첫 local virtual server를 생성하기 위해 Vagrant와 VirtualBox를 다운로드 받고 Vagrantfile을 설정하여 virtual server에 대한 설정을 할 것이다.\n Vagrant와 VirtualBox를 OS에 맞게 설치한다.  Download Vagrant Download VirtualBox   Vagrantfile과 provisioning instructions를 저장할 폴더 생성 Terminal이나 PowerShell을 열고 해당 폴더로 이동 vagrant box add geerlingguy/centos7을 이용하여 CentOS 7.x 64-bit을 추가한다. vagrant init geerlingguy/centos7으로 방금 다운로드 받은 default virtual server configuration을 생성할 수 있다. vagrant up으로 CentOS를 부팅할 수 있다.  Vagrant는 이미 만들어진 64-bit CentOS 7 가상 머신을 다운로드 한다. 또는 원할 경우 커스텀 boxes를 생성할 수 있다. 이를 통해 VirtualBox에서 사용할 configuration 파일인 Vagrantfile을 생성하여 이를 가상 머신의 부팅에 사용한다.\n이 가상 서버를 관리하는 것은 상당히 쉽다.\n vagrant halt: VM 종료 vagrant up: VM을 실행한다. vagrant destroy: VirtualBox에서 완전히 머신을 삭제한다.  이 때 vagrant up을 하게 되면 다시 base box에서 재 생성을 하게 된다.   vagrant ssh: 가상 머신으로 ssh 접속. Vagrantfile이 위치한 폴더에서 입력하면 된다. vagrant ssh-config로 수동으로 접속하거나 다른 어플리케이션에서 접속할 수 있다.  Using Ansible with Vagrant Vagrant는 preconfigured boxes를 사용하여 간편하게 하지만 VirtualBoxGUI에서도 이와 비슷하게 할 수 있다. Vagrant는 다음과 같은 특징이 있다.\n Network interface management: port를 VM에 포워드할 수 있고, public network를 공유하거나 inter-VM과 host-only 통신을 위한 private networking을 사용할 수 있다. Shared folder management: VirtualBox에서 host와 VM간에 NFS나 VirtualBox의 native folder sharing을 통해 폴더 공유를 하도록 설정한다. Multi-machine management: Vagrant는 하나의 Vagrantfile로 여러개의 VM을 관리할 수 있다. 더 중요한 점은, 문서에도 나와있듯 역사적으로 복잡한 환경을 동작시키는 것은 이를 하나의 머신에 관리하도록 만들었다. 이는 프로덕션 셋업의 부정확한 모델로 많은 차이가 있다. Provisioning: 처음 vagrant up을 실행하면 Vagrant는 Vagrantfile에서 어떤 provider를 선택했든지, 자동으로 금방 생겨난 VM을 provision할 것이다. 또한 VM이 생성되고 난 후에도 vagrant provision 명령어를 통해 명시적으로 provisioning을 할 수 있다.  마지막 특징이 가장 중요하다. Ansible 또한 Vagrant의 provisoner 중 하나이다. vagrant provision을 하면 Vagrant는 Ansible에게 VM을 정의된 Ansible Playbook을 실행하도록 한다.\nVagrantfile을 열어서 다음을 추가한다. 마지막 end 전에 추가하면 된다. (Ruby syntax를 사용한다.)\n# Provisioning configuration for Ansible config.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.playbook = \u0026#34;playbook.yml\u0026#34; # Run commands as root. ansible.sudo = true end 이것이 Vagrant에서 Ansible을 사용하도록 하는 가장 기본적인 설정이다. 깊게 들어가면 더 다양한 옵션들이 있다. 지금은 기본 playbook을 사용할 것이다.\nYour first Ansible Playbook 이제 palybook.yml 파일을 생성할 것이다. Vagrantfile이 있는 파일과 동일한 위치에서 다음을 입력한다.\n--- - hosts: all tasks: - name: Ensure NTP (for time synchronization) is installed. yum: name=ntp state=installed - name: Ensure NTP is running. service: name=ntpd state=started enabled=yes playbook에 대해 간단히 알아보고 넘어가겠다. 이제 playbook을 VM에서 실행시켜본다. Vagrantfile과 playbook.yml은 동일한 위치에 있어야 한다. 그 다음 vagrant provision을 입력한다. 그러면 tasks에서 지정한 동작을 하고, 이에대한 status를 확인할 수 있다. 그 후 VM에서 동작한 것들에 대한 요약을 보여준다.\nAnsible은 방근 정의한 간단한 playbook을 파싱하고, 여러 명령어들을 ssh를 통해 실행하여 우리가 정의한 대로 서버를 설정한다. 한 줄씩 playbook을 확인해보자.\n--- 첫 줄은 뒤의 문서가 YAML 형식으로 작성되었음을 알려준다.\n- hosts: all Ansible이 playbook을 어느 host에 적용할지 알려준다. all을 사용한 이유는 Vagrant가 자체의 개별적인 Ansible inventory file(이전에 생성한 /etc/ansible/hosts와는 다른)을 사용하여 방금 정의한 Vagrant VM을 관리하기 때문이다.\ntasks: 뒤에 나오게 될 task들은 모든 host에서 실행될 것이다. (이 경우 우리의 VM)\n- name: Ensure NTP daemon (for time synchronization) is installed. yum: name=ntp state=installed 이 명령어는 yum install ntp와 동일하지만 좀 더 똑똑하다. ntp가 설치되어있는지 확인하고 안되어있으면 설치한다. 다음의 shell script와 동일하다.\nif ! rpm -qa | grep -qw ntp; then yum install ntp fi 그러나 위의 스크립트는 Ansible의 yum command만틈 robust하지는 않다. ntp가 아닌 ntpdate가 설치되어 있을 경우엔 어떻게 해야하나? 이 스크립트는 Ansible의 yum command와 단순 비교하기에는 무리가 있다.\n- name: Ensure NTP is running. service: name=ntpd state=started enabled=yes 마지막 단계는 ntpd service가 시작되었고 동작하는지 확인한다. 그리고 system boot 시 시작되도록 설정한다. 동일한 결과를 내는 shell script는 다음과 같다.\n# Start ntpd if it\u0026#39;s not already running. if ps aux | grep -v grep | grep \u0026#34;[n]tpd\u0026#34; \u0026gt; /dev/null then echo \u0026#34;ntpd is running.\u0026#34; \u0026gt; /dev/null else /sbin/service ntpd restart \u0026gt; /dev/null echo \u0026#34;Started ntpd.\u0026#34; fi # Make sure ntpd is enabled on system startup chkconfig ntpd on shell script가 얼마나 복잡한 지 확인할 수 있다. 그리고 여전히 Ansible만큼 robust하지는 않다. idempotency를 보장하려면 shell script에 많은 작업이 필요하다.\n더 간결하게 하여 Ansible의 name module을 사람이 읽을 수 있는 이름으로 각 command에 부여하면 결과적으로 다음과 같은 playbook이 완성된다.\n--- - hosts: all tasks: - yum: name=ntp state=installed - service: name=ntpd state=started enabled=yes  code와 configuration file처럼 Ansible에서의 documentation(function에 name을 부여하는 것, 복잡한 tasks에 대해 comments를 다는 것)은 절대적으로 필요한 것은 아니다. 이렇게 하면 사람이 읽을 수 있는 정보를 가지고 어떻게 playbook이 실행되는지 확인하기 쉽다.\n Summary 이제 workstatin이 infrasturcture-in-a-box가 되었다. 그리고 그 infrastructure가 코드로 잘 테스트되었음을 보장할 수 있다. 이 작은 예시에서 간단하면서도 강력한 Ansible playbook을 경험할 수 있었다. 나중에 Ansible playbook에 대해서 더 깊게 알아볼 것이다. 또한 Vagrant에 대해서도 다루어 볼 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-cloud-providers/",
	"title": "Choose Cloud Providers",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Spinnaker를 배포할 환경을 설정해 주어야 합니다. 여기에서는 제가 구축한 local kubernetes cluster를 사용할 것입니다.\n먼저 2가지가 필요합니다.\n kubeconfig 파일 kubeconfig 파일은 일반적으로 ~$HOME/.kube/config 파일을 의미합니다. 저는 local kubernetes cluster로 이동하여 해당 파일을 halyard를 위한 vm으로 복사하였습니다. kubectl CLI 툴  이제 hal config 명령어를 통해 kubernetes cluster를 추가합니다.\nhal config provider kubernetes enable CONTEXT=$(kubectl config current-context) hal config provider kubernetes account add wonderland \\  --provider-version v2 \\  --context $CONTEXT hal config features edit --artifacts true "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/create-a-single-control-plane-cluster-with-kubeadm/",
	"title": "Create a Single Control Plane Cluster With Kubeadm",
	"tags": ["kubeadm", "instll"],
	"description": "",
	"content": "이 문서에서는 Master 노드 한대로 클러스터를 구성하는 방법에 대해 알아보도록 하겠습니다.\n먼저, 파드 네트워크에 사용할 add-on을 선정합니다. 그런뒤 kubeadm init을 할 때 필요로 하는 사항이 있는지 확인해야 합니다.\n저는 Calico를 사용할 것입니다. Calico는 kubeadm init에서 --pod-network-cidr=192.168.0.0/16을 해주거나, 나중에 calico.yml 파일에서 적절하게 수정해주어야 한다고 합니다. 저는 Pod Network에 사용될 IP 대역을 10.1.0.0/16 대역을 사용하고자 합니다. 그러기 위해 kubeadm init을 --pod-network-cidr=10.1.0.0/16 옵션을 통해 실행할 것입니다.\nkubeadm init --pod-network-cidr=10.1.0.0/16 몇분 후 설치가 완료될 것입니다. 그러면 아래에 kubeadm join 이라면서 어떻게 다른 node들을 join 시키는지 설명이 되어 있습니다.\n클러스터를 사용하려면 다음과 같은 명령어를 통해서 kubectl에서 접근이 가능하도록 해야합니다.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 그 다음엔 각 worker node로 접속하여 설치시 나왔던 kubeadm join 명령어를 복사하여 입력합니다. 그런 뒤 master node로 접속하여 kubectl get nodes를 입력하고 결과를 확인합니다. 성공했을 경우 node들이 보일 것입니다.\n이제 calico를 설치하도록 합니다.\ncurl -o calico.yaml https://docs.projectcalico.org/v3.8/manifests/calico.yaml 이후 calico.yaml에서 CALICO_IPV4POOL_CIDR 부분을 수정해줍니다.\n- name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.1.0.0/16\u0026#34; Reference https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\n"
},
{
	"uri": "http://kimmj.github.io/ibiza/ibiza-project/",
	"title": "Ibiza Project",
	"tags": ["blog", "hugo"],
	"description": "",
	"content": "Ibiza Project는 나만의 블로그을 만들기 위한 프로젝트이다.\n기본적으로 hugo-theme-learn에서 시작하여, 나의 커스텀 파일들을 추가하여 내가 원하는 사이트를 만들 것이다.\n기간 : 2020.4.15 (작성일 기준 + 100일)\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/variables/",
	"title": "Variables",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "Ansible에서의 variable은 대부분의 다른 시스템에서의 variable과 비슷하다. variable은 항상 글자로 시작하고[A-Za-z] 그 다음부터는 _나 숫자들 [0-9]를 포함할 수 있다.\n사용가능한 variable name은 foo, foo_bar, foo_bar_5, fooBar을 포함하지만 표준은 모두 소준자를 사용하고 variable name에서 숫자를 제거하는 것이다(camelCase나 UpperCamelCase를 피한다).\n사용불가능한 variable name은 _foo, foo-bar, 5-foo-bar, foo bar가 있다.\ninventory file에서 variable의 값은 equal sign으로 할당된다.\nfoo=bar playbook이나 파일에 포함된 variable에서는 variable의 value가 colon을 통해 할당된다.\nfoo: bar Playbook variables task에서 사용할 variable을 정의하는 방법에는 여러가지가 있다.\nvariable은 ansible-playbook을 호출할 때 --extra-vars 옵션을 통해 command line을 통해서 전달될 수 있다.\nansible-playbook exampl.yml --extra-vars \u0026#34;foo=bar\u0026#34; JSON, YAML에 quote를 씌워 extra variable을 전달하거나 JSON, YAML 파일을 직접 전달할 수도 있다. @even_more_vars.json이나 --extra-vars \u0026quot;@even_more_vars.yml처럼 사용하면 된다. 하지만 여기서 아래에 적힌 다른 method를 사용하는 것도 좋을 수 있다.\nvariable은 vars section에서 playbook의 나머지 부분에 inline을 표함할 수 있다.\n--- - hosts: example vars: foo: bar tasks: # Prints \u0026#34;Variable \u0026#39;foo\u0026#39; is set to bar\u0026#34;. - debug: msg=\u0026#34;Variable \u0026#39;foo\u0026#39; is set to {{ foo }}\u0026#34; variable은 또한 별도의 파일에 작성하고 vars_files section에서 포함하게 할 수도 있다.\n--- # Main playbook file. - hosts: example vars_files: - vars.yml tasks: - debug: msg=\u0026#34;Variable \u0026#39;foo\u0026#39; is set to {{ foo }}\u0026#34; --- # Variables file \u0026#39;vars.yml\u0026#39; in the same folder as the playbook. foo: bar 모든 variable들이 YAML의 root level로 설정된 것을 인지해야 한다. standalone file로 포함되는 상황에서는 vars같은 heading이 필요하지 않다.\nvariable 파일은 상황에 따라 import되게 할수도 있다. 예를 들어 CentOS server에 대한 variable 세트가 있을수 있고(Apache service가 httpd이다) Debian server에서는 다른 세트를 가질 수 있다(Apache service가 apache2이다). 이러한 경우 vars_files를 include할 때 조건을 사용할 수 있다.\n--- - hosts: example vars_files: - [ \u0026#34;apache_{{ ansible_os_family }}.yml\u0026#34;, \u0026#34;apache_default.yml\u0026#34; ] tasks: - service: name={{ apache }} state=running 그러고 난 후 두 파일을 example playbook과 같은 디렉토리에 apache_CentOS.yml과 apache_default.yml로 저장한다. CentOS 파일에서는 apache: httpd로 설정하고 default 파일에서는 apache: apache2로 설정한다.\nremote server가 facter나 ohai가 설치되어 있는 한 Ansible은 server의 OS를 읽을 수 있게 되고 이를 variable로 변경(ansible_os_family)하여 resulting name을 가지고 vars file을 포함시킬 수 있다. Ansible이 파일의 이름을 그 이름으로 찾지 못한다면 두번째 옵션(apache_default.yml)을 사용할 것이다. 따라서 Debian이나 Ubuntu server에서 Ansible은 apache_Debian.yml이나 apache_Ubuntu.yml 파일이 없더라도 apache2를 서비스 이름으로 사용할 수 있을 것이다.\nInventory variables variable은 Ansible inventory file의 host definition이 있는 줄이나 group에 대해 정의한 것을 통해서도 추가될 수 있다.\n# Host-specific variables (defined inline). [washington] app1.example.com proxy_state=present app2.example.com proxy_state=absent # Variables defined for the entire group [washington:vars] cdn_host=washington.static.example.com api_version=3.0.1 더 많은 variable을 정의하고자 한다면, 특히 한 두개 이상의 호스트에 variable을 적용하려고 한다면 inventory file을 사용하는 것은 성가신 일일 수 있다. 사실 Ansible의 documentation에서는 inventory에 variable을 저장하지 않는 것을 권장한다. 대신에 group_vars와 host_vars YAML variable file을 특정한 위치에 놓으면 Ansible이 이를 개별 호스트와 inventory에 정의된 group에 적용하게 된다.\n예를 들어 app1.example.com 호스트에 variable 설정을 적용하려 한다면 app1.example.com이라는 이름으로 빈 파일을 생성하고 이를 /etc/ansible/host_vars/app1.example.com에 위치한뒤 vars_files YAML에 넣고싶은 variable을 추가하면 된다.\nfoo: bar baz: qux variable들을 전체 washington group에 대해 적용하고 싶으면 /etc/ansible/group_vars/washington에 비슷한 파일을 생성하면 된다(washington은 사용하고싶은 group으로 변경하면 된다).\n이 파일들을 같은 이름으로 playbook 디렉토리의 host_vars나 group_vars 디렉토리 안에 놓아도 된다. Ansible은 /etc/ansible/[host|group]_vars 디렉토리에 있는 inventory에 정의된 variable을 먼저 사용한 뒤 playbook directory에 정의되어 있는 variable을 사용하게 될 것이다.\nhost_vars와 group_vars를 사용하는 것의 다른 대안책은 위에서 언급했듯이 conditional variable file을 사용하는 것이다.\nRegistered Variables command를 실행하고 나서 이 return code나 stderr, stdout을 가지고 다음 task를 진행할지 결정해야하는 상황이 많이 있을 것이다. 이러한 상황에서 Ansible은 register를 사용하여 runtime에 특정 command의 output을 variable로 저장하는 기능을 가지고 있다.\n이전 챕터에서 register를 통해 forever list command의 ouput을 가지고 Node.js app을 시작해야하 하는지 결정할 때 사용했다.\n- name: \u0026#34;Node: Check list of Node.js apps running.\u0026#34; command: forever list register: forever_list changed_when: false - name: \u0026#34;Node: Start example Node.js app.\u0026#34; command: forever start {{ node_apps_location }}/app/app.js when: \u0026#34;forever_list.stdout.find(\u0026#39;{{ node_apps_location }}/app/app.js\u0026#39;) == -1\u0026#34; 이 예시에서 우리는 Python에 내장된 string function(find)을 사용하여 app의 위치를 검색하고 존재하지 않을경우 Node.js app을 시작한다.\nregister에 대해서는 이번 챕터의 뒷부분에서 알아보도록 하자.\nAccessing Variables 간단한 variable(Ansible에 의해서 수집되는 inventory 파일 또는 playbook이나 variable file에 정의된 것들)은 {{ variable }}과 같은 문법을 사용하여 task 안에서 사용할 수 있다. 예를 들면 다음과 같다.\n- command: /opt/my-app/rebuild {{ my_environment }} command가 동작하면 Ansible은 my_environment의 내용을 {{ my_environment }}로 대치할 것이다. 그러면 결과적으로 command는 /opt/my-app/rebuild dev처럼 될 것이다.\n사용할만한 많은 variable들은 array(또는 리스트)로 구조화될 수 있고 foo array에 접근하는 것은 information에 대해 충분한 정보를 제공하지 않는다(Ansible이 with_items처럼 전체 array를 사용해야하는 상황을 제외하곤).\n아래와 같이 variable의 리스트를 정의했다고 해보자.\nfoo_list: - one - two - three 해당 array에 있는 첫번째 아이템에 다음과 같이 접근할 수 있다.\nfoo[0] foo|first 첫번째 줄은 Python에서 array에 접근하는 문법(array의 첫번째 element, 0번째 index를 불러오기)과 동일하지만 두번째 줄은 Jinja2가 제공하는 filter를 사용한 것이다. 둘 다 유효한 문법이며 유용하며 둘 중 하나를 고르는 것은 전적으로 사용자에게 달려있다.\n더 크고 구조화된 array에 대해서(예를 들어 Ansible이 server에서 얻어온 서버의 IP 주소를 받아오는 것) 우리는 []이나 .을 이용하여 어떤 array의 key에도 접근할 수가 있다. 예를 들어 eth0 network interface에 대한 정보를 얻고싶으면 먼저 playbook의 debug를 이용하여 전체 array를 살펴보면 된다.\n#In your playbook. task: - debug: var=ansible_eth0 TASK: [debug var=ansible_eth0] ***************************************** ok: [webserver] =\u0026gt; { \u0026#34;ansible_eth0\u0026#34;: { \u0026#34;active\u0026#34;: true, \u0026#34;device\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;ipv4\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.2.15\u0026#34;, \u0026#34;netmask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;10.0.2.0\u0026#34; }, \u0026#34;ipv6\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;fe80::a00:27ff:feb1:589a\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;64\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;link\u0026#34; } ], \u0026#34;macaddress\u0026#34;: \u0026#34;08:00:27:b1:58:9a\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;e1000\u0026#34;, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;promisc\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;ether\u0026#34; } } 이제 전반적인 variable에 대한 구조를 알게되었으니 다음의 테크닉을 통해 서버의 IPv4 주소만 얻어올 수 있다.\n{{ ansible_eth0.ipv4.address }} {{ ansible_eth0['ipv4']['address']}} Host and Group variables Ansible은 host별, group별 variable들을 쉽게 정의하거나 override할 수 있다. 앞에서 배운 것처럼 inventory file은 group과 host를 다음과 같이 정의할 수 있다.\n[group] host1 host2 host별 또는 group별로 variable을 정의하는 가장 쉬운 방법은 inventory file 안에서 직접 할 수 있다.\n[group] host1 admin_user=jane host2 admin_user=jack host3 [group:vars] admin_user=john 이 경우 Ansible은 default variable \u0026lsquo;john'을 {{ admin_user }}에 대해 사용하지만 host과 host2의 경우 hostname 옆에 정의된 admin user를 사용하게 될 것이다.\n이는 variable을 정의하거나 각 host별 또는 각 group별로 정의할 때 간편하고 잘 작동하지만 더 복잡한 playbook의 경우 host-specific한 variable을 몇가지(3+) 더 추가해야할 수도 있다. 이런 상황에서 유지보수와 가독성을 위해 다른 파일에 variable을 정의하는 것이 더 쉬울 것이다.\ngroup_vars and host_vars Ansible은 inventory file(또는 /etc/ansible/hosts에 있는 default inventory file을 사용한다면 /etc/ansible 내부)과 같은 위치에서 group_vars와 host_vars 디렉토리를 검색할 것이다.\n이 디렉토리 안에 inventory file에서 정의된 group name 또는 hostname을 따라 이름지어 YAML 파일을 저장하면 된다. 위의 예시에서 계속해서 이 specific variable들을 옮겨보도록 하자.\n--- # File: /etc/ansible/group_vars/group admin_user: john # File: /etc/ansible/host_vars/host1 anmin_user: jane default inventory file(또는 playbook의 root directory 바깥에 있는 inventory file)을 사용한다고 하더라도 Ansible은 playbook 안에 있는 group_vars와 host_vars 디렉토리 내부의 host, group variable file들도 사용한다. 이는 전체 playbook과 infrastructure configuration을 모든 host/group-specific configuration을 포함하는 source-control repository에 패키징할 때 편리하게 사용할 수 있다.\n또한 group_vars/all 파일을 정의하여 all group에 대해 적용할 수 있고 또한 host_vars/all 파일을 통해 all host에 대해 적용할 수도 있다. 하지만 보통은 playbook과 role 안에 sane default를 정의하는 것이 더 좋다(나중에 더 이야기해보도록 한다).\nMagic variables with host and group variables and information 특정한 host의 variable을 다른 host에서 얻어오고자 한다면 Ansible은 magic hostvars variable을 제공하여 (inventory file과 host_vars 디렉토리에 있는 YAML file 들에서) 정의된 host variable들을 불러올 수 있다.\n# From any host, returns \u0026#34;jane\u0026#34;. {{ hostvars[\u0026#39;host1\u0026#39;][\u0026#39;admin_user\u0026#39;] }} 이것 외에도 때때로 사용할 수 있는 variable들을 Ansible이 제공해준다.\n groups: inventory 안에 있는 모든 group name의 list group_names: current host가 속한 모든 그룹들의 리스트 inventory_hostname: inventory에 따른 현재 host의 hostname(이는 system이 제공하는 hostname인 ansible_hostname과는 다른 값일 수 있다). inventory_hostname_short: inventory_hostname의 첫번째 부분. play_hosts: 현재 play가 실행될 모든 host  Ansible의 official documentation에서 Magic Variables, and How To Access Information About Other Hosts를 참조하여 최신 정보와 사례를 확인해 보아라.\nFacts (Variables derived from system information) default로 Ansible playbook을 실행할 때마다 Ansible은 play를 하는 각 host의 information(facts)을 수집한다. 이전 chapter에서 playbook을 실행할 때마다 다음과 같은 것을 보았을 것이다.\n$ ansible-playboook playbook.yml PLAY [group] ********************************************************** GATHERING FACTS ******************************************************* ok: [host1] ok: [host2] ok: [host3] Fact는 playbook을 실행할 때 매우 유용하다. 우리는 특정 task를 수행할 때 또는 configuration file안의 특정 정보를 수정하기 위해 host IP address나 CPU type, disk space, OS information, network interface information같은 정보를 수집할 수 있다.\n사용할 수 있는 모든 fact에 대한 list를 얻으려면 ansible command에 setup module을 더해 사용하면 된다.\n$ ansible munin -b setup munin.midwesternmac.com | success \u0026gt;\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;167.88.120.81\u0026#34; ], \u0026#34;ansible_all_ipv6_addresses\u0026#34;: \u0026#34;2604:180::a302:9076\u0026#34;, [...] fact를 사용할 필요가 없고 playbook을 실행할 때 조금의 시간이라도 단축하고 싶으면(수십, 수백개의 서버에 대해 Ansible playbook을 돌릴 때 유용할 것이다) playbook에서 gather_facts: no를 설정하면 된다.\n- hosts: db gather_facts: no 저자가 사용하는 많은 playbook과 role은 ansible_os_family, ansible_hostname, ansible_memtotal_mb와 같은 fact를 사용하여 새로운 variable을 등록하거나 when과 함께 사용하여 특정한 task를 실행하도록 조건문을 걸어준다.\nFactor나 Ohai가 remote host에 설치되어 있다면 Ansible은 여기서 수집된 fact 또한 facter_와 ohai_ prefix를 통해 각각 include할 수 있다. Ansible을 Puppet이나 Chef와 함께 사용할 경우 이러한 system-information-gathering tool에 대해 이미 친숙할 것이고 Ansible에서도 편하게 이것들을 사용할 수 있을 것이다. 그렇지 않다면 Ansible의 Fact는 어떤 것을 하던지 충분할 것이고 Local Facts를 통해 더 유연하게도 할 수 있다.\n playbook을 비슷한 서버나 VM(예를 들어 모든 서버가 동일한 OS에서 실행하고 동일한 hosting provider인 경우)에서 실행한다면 fact는 거의 동일할 것이다. playbook을 다양한 set의 host(예를 들어 다른 OS들, 다른 virtualization stack, hosting provider일 경우)에 대해서 실행할 때 몇몇 fact는 예상하는 것과는 다른 information을 포함할 수 있다. Server Check.in에서 5개보다 많은 hosting provider, 다양한 hardware를 가진 server를 보유하고 있어 특히 새로운 서버를 추가할 때 ansible-playbook의 결과들을 모니터해야 한다.\n Local Facts (Facts.d) host-specific fact를 정의하는 다른 방법은 .fact file을 remote host의 특정한 디렉토리, /etc/ansible/facts.d/에 넣는 것이다. 이 파일들은 JSON이나 INI 파일이 될 수 있고 또는 JSON을 리턴하는 실행파일을 사용할수도 있다. 예를 들어 /etc/ansible/facts.d/settings.fact를 remote host에 생성하고 다음의 내용을 작성한다.\n[users] admin=jane,john normal=jim 그 다음 Ansible의 setup module을 사용하여 remote host에서 새로운 fact를 표시한다.\n$ ansible hostname -m setup -a \u0026#34;filter=ansible_local\u0026#34; munin.midwesternmac.com | success \u0026gt;\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_local\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: \u0026#34;jane,john\u0026#34;, \u0026#34;normal\u0026#34;: \u0026#34;jim\u0026#34; } } } }, \u0026#34;changed\u0026#34;: false } playbook을 사용하여 새로운 서버를 provision 하고, playbook 안에서 나중에 사용할 local fact를 생성하는 .fact file을 추가한다면 Ansible에게 다음과 같이 local fact를 불러오라고 명시적으로 알려주어야 한다.\n- name: Reload local facts. setup: filter=ansible_local  host_vars나 다른 variable definition 방법에 비해 local fact를 사용하는 것을 권장하긴 하지만 playbook이 각 host의 특정한 detail에 대해 의존하지 않도록 만드는 것이 훨씬 더 좋다. 때로 반드시 local fact(특히 facts.d에 있는 실행파일을 사용하여 local environment을 기반으로 fact를 정의하는 경우)를 사용해야 하지만 언제나 configuration을 중앙 repository에 두는 것이 더 좋은 방법이고 host-specific fact로부터 벗어나야 한다.\n setup module 옵션(filter 처럼)은 Windows host에 대해서는 사용할 수 없다는 것에 주의하라.\n Variable Precedence 5개의 다른 place에서 동일한 variable을 정의하였을 때어떤 variable이 사용되고 있는지에 대한 자세한 사항을 파고드는 경우는 드물 것이다. 그러나 만일을 대비하여 Ansible documentation에서는 다음과 같은 순서를 사용한다.\n command line을 통해 전달된 variable(-e option)이 항상 승리한다. inventory에 정의된 variable connection (ansible_ssh_user 등) 대부분의 모든 것 (command line switch, play에 정의된 variable, 내장된 variable, role variable 등) 나머지(non-connection) inventory variable들 local fact와 자동으로 발견된 fact들 (gather_facts를 통해) Role의 default variable (role의 defaults/main.yml 파일에 있는 것)  playbook, role을 만들고 inventory를 관리를 많이 하게되면 우리가 원하는 variable definition의 알맞은 조합을 찾게 될것이지만 play별, host별, run별로 variable을 setting하고 overriding하는데 드는 고통을 줄이기 위한 몇가지 사항들이 있다.\n Role(다음 챕터에서 이야기 해보도록 하자)은 role의 default variable을 통해 sane default를 제공해야 한다. 이 variable들은 어디에도 variable이 정의되지 않았을 때 사용될 것이다. playbook은 variable을 거의 정의하면 안된다(예를 들어 set_facts를 통해). 그 대신에 variable은 vars_files를 include 하거나 또는 inventory를 통해 사용되어야 한다. 오직 신뢰할 수 있는 host_specific 또는 group-specific variable들만 host나 group inventory에 정의되어야 한다. dynamic, static inventory source는 특정 playbook을 유지보수하는 데 잘 확인하지 않는 상황에서 특히 최소한의 variable을 포함해야 한다. command line variable(-e)는 반드시 가능한한 피해야한다. 이걸 주로 사용하는 하나의 예는 실행하는 task의 유지보수성이나 idempotence를 걱정할 필요가 없는 local testing이나 one-off playbook을 실행하는 것이다.  Ansible의 Variable Precedence documentation을 통해 더 자세한 사항과 예시를 확인하라.\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/ad-hoc-commands/",
	"title": "Ad Hoc Commands",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "지난 챕터에서는 Vagrant와 간단한 Ansible playbook으로 local infrastructure를 테스트해보았다. 이번에는 간단한 Ansible ad-hoc command를 사용하여 하나의 명령어로 다수의 remote server에 명령을 보낼 것이다.\n나중 챕터에서는 playbook에 대해 자세히 알아볼 것이지만, 지금은 어떻게 Ansible이 하나 이상의 서버에 대해 ad-hoc command로 빠르게 공통적인 일을 수행하는지, 데이터를 가져오는지에 대해 알아볼 것이다.\nConducting an orchestra 각 개인 administrator가 관리하는 서버의 수는 수년간 급격하게 증가했다. 특히 virtualization과 cloud application의 발전은 표준처럼 되었다.\n어느 때든 system administrator는 여러가지 업무가 있다.\n patch 적용과 yum, apt, 다른 package manager를 통한 update resource usage 확인 (disk space, memory, CPU, swap space, network) log file 확인 system user와 group 관리 DNS 설정, hosts 파일 관리 등 서버에 파일 업로드/다운로드 application 배포하기 또는 application 유지 server 재부팅 cron jobs 관리  최근에는 이런 작업들이 조금이나마 자동화 되어있다. 하지만 실시간 진단같은 문제에는 사람의 손길이 필요하긴 하다. 또한 multi-server 환경은 복잡하여 각 server에 접속하는 것은 좋은 솔루션이 아니다.\nAnsible은 admin이 ad-hoc command로 ansible 명령어를 사용하여 수백개의 서버에 동시에 명령을 전달할 수 있다. 챕터 1에서는 Ansible inventory file에 기록된 서버에 두개의 command를 실행했다. 이 챕터에서는 ad-hoc command와 multi-server 환경을 자세히 볼 것이다. 다른 Ansible의 강력한 기능들은 제쳐두고라도 이 챕터를 읽으면 더 효과적으로 server들을 관리할 수 있다.\nBuild infrastructure with Vagrant for testing 우리의 production server에 영향을 주지 않고 설명을 하기 위해 이 챕터의 나머지 부분에서는 Vagrant의 multi-machine capabilites를 사용하여 몇개의 server들을 설정해볼 것이다. 이것들은 Ansible을 통해 관리될 것이다.\n먼저 Vagrant에서 CentOS 7을 실행시키는 하나의 virtual machine을 사용할 것이다. 이 예시에서 우리는 Vagrantfile에 정의된 Vagrant의 기본 설정들을 사용할 것이다. 이 예시에서 우리는 Vagratn의 강력한 multi-machine management feature를 사용할 것이다.\n우린 3개의 VM을 생성할 것이다. (두개의 app server, 하나의 database server) 이정도면 Ansible의 server management 능력을 확인해볼 수 있을 것이다.\nlocal drive에 새로운 폴더를 생성하고 Vagrantfile을 생성한다. 이를 editor로 연 뒤 다음과 같이 입력한다.\n# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \u0026quot;2\u0026quot; Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.ssh.insert_key = false config.vm.provider :virtualbox do |vb| vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--memory\u0026quot;, \u0026quot;256\u0026quot;] end # Application server 1. config.vm.define \u0026quot;app1\u0026quot; do |app| app.vm.hostname = \u0026quot;orc-app1.dev\u0026quot; app.vm.box = \u0026quot;geerlingguy/centos7\u0026quot; app.vm.network :private_network, ip: \u0026quot;192.168.60.4\u0026quot; end # Application server 2. config.vm.define \u0026quot;app2\u0026quot; do |app| app.vm.hostname = \u0026quot;orc-app2.dev\u0026quot; app.vm.box = \u0026quot;geerlingguy/centos7\u0026quot; app.vm.network :private_network, ip: \u0026quot;192.168.60.5\u0026quot; end # Database server. config.vm.define \u0026quot;db\u0026quot; do |db| db.vm.hostname = \u0026quot;orc-db.dev\u0026quot; db.vm.box = \u0026quot;geerlingguy/centos7\u0026quot; db.vm.network :private_network, ip: \u0026quot;192.168.60.6\u0026quot; end end 이 Vagrantfile은 세 server를 정의하고 각각 고유한 hostname, machine name, IP 주소를 부여한 것이다. 간단하게 하기 위해 셋 모두 CentOS 7을 사용한다.\n터미널을 열고 Vagrantfile이 있는 폴더로 이동한다. 그 다음 vagrant up을 이용하여 세개의 VM을 생성한다. 이미 챕터 2에서 box를 다운로드 받았다면 5~10 내로 생성될 것이다.\n진행되는 동안 서버에 대한 Ansible을 설명하도록 하겠다.\nInventory file for multiple servers 관리하는 서버를 다루는 Ansible에 대해 이야기 할 것들이 많다. 하지만 대부분의 경우 서버를 시스템의 main Ansible inventory file(보통 /etc/ansible/hosts)에 추가하면 된다. 지난 챕터에서 파일을 생성하지 않았다면 돌아가서 파일을 생성해야한다. 또한 user가 해당 파일에 대해 read 권한이 있어야 한다.\n다음을 파일에 추가한다.\n# Lines beginning with a # are comments, and are only included for # illustration. These comments are overkill for most inventory files. # Application servers [app] 192.168.60.4 192.168.60.5 # Database server [db] 192.168.60.6 # Group 'multi' with all servers [multi:children] app db # Variables that will be applied to all servers [multi:vars] ansible_ssh_user=vagrant ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key  첫 번째 block은 우리의 application server들을 app group에 추가한다. 두 번째 block은 database server를 db group에 추가한다. 세 번째 block은 Ansible이 새 그룹 multi를 생성하고, child group으로 app과 db를 추가한 것이다. 네 번째 block은 multi group에 variables를 추가한 것이다. 이는 multi group과 그 내부의 모든 children 서버에 적용된다.  나중 챕터에서 variables, group definition, group hiearchy, Inventory file topics에 대해 알아볼 것이다. 여기서는 어떻게 Ansible이 간단히 서버 정보를 다루는지 확인하고 빠르게 이를 이용할 것이다.\n inventory file을 저장하고 Vagrant가 세개의 VM들을 성공적으로 build 하였는지 확인한다. Vagrant가 성공하고 나면 이들을 Ansible에서 관리할 것이다.\nYour first ad-hoc commands 가장 먼저 해야할 것은 서버 안에서 체크할 것이다. 제대로 설정이 되었는지 확인하고, 올바른 날짜 및 시간이 설정 되었는지(우린 time synchronization과 관련된 에러를 경험하고 싶지 않다!), 어플리케이션을 실행하기에 충분한 리소스가 있는지 확인해 보자.\n여기서 확인하는 것들은 production server에서 자동화 시스템으로 모니터링되어야 하는 것들이다. 재앙을 막는 가장 좋은 방법은 언제 올지 알고, 발생하기 전에 어떻게 문제점을 해결하는 것을 아는 것이다. Munin, Nagios, Cacti, Hyperic과 같은 툴을 사용하여 서버에서의 과거와 현재의 리소스 사용량을 확인할 수 있다. 인터넷에서 웹사이트나 웹 어플리케이션을 돌리고 있다면 Pingdom이나 Server Check in 같은 외부 모니터링 솔루션이 필요하다.\n Discover Ansible's parallel nature 먼저, Vagrant가 설정한 VM들이 올바른 hostname을 가지고 있는지 확인할 것이다. Ansible에서 -a argument를 \u0026ldquo;hostname\u0026quot;으로 줘서 모든 서버에 대해 hostname을 실행한다.\n$ ansible multi -a \u0026#34;hostname\u0026#34; Ansible은 이 명령어를 세 서버 모두에 대해 실행하고, 결과를 리턴받는다. (만일 Ansible이 하나의 서버에 도달하지 못할 경우 이는 해당 서버에 대해 error를 출력하지만 나머지 서버에 대해서는 계속해서 명령어를 수행한다.)\nAnsible이 No hosts matched나 다른 inventory 관련 에러를 리턴하면 ANSIBLE_HOSTS 환경 변수를 설정해보아라. export ANSIBLE_HOSTS=/etc/ansible/hosts를 하면 된다. 일반적으로 Ansible은 /etc/ansible/hosts 파일을 자동으로 읽지만 어떻게 Ansible을 설치했는지에 따라 명시적으로 ANSIBLE_HOSTS을 설정해야 할수도 있다.\n 명령어들이 각 서버에 대해서 예상했던 순서대로 실행되는 것은 아님을 확인할 수 있다. 명령어를 몇번 더 입력하여 순서를 확인해보자.\n기본적으로 Ansible은 명령어를 process fork를 이용해서 병렬적으로 실행한다. 따라서 명령어는 좀 더 빨리 완료될 수 있다. 몇개의 서버만 관리한다면 하나씩 실행하는 것에 비해 속도 체감이 별로 안들지만 5-10개의 서버만 관리한다고 하더라도 Ansible의 parallelism(기본적으로 활성화되어있다)을 이용하면 엄청난 속도 절감 효과가 있을 것이다.\nfork를 하나만 한다는 것을 의미하는 -f 1 argument를 이용해서 동일한 명령어를 다시 입력해보자. (일반적으로 각 서버에 대해 순서대로 실행할 때 쓴다)\n동일한 명령어를 계속 반복해도 항상 같은 순서대로 결과가 나올 것이다. 이걸 사용할 일은 거의 없겠지만 값을 늘리는 일은 그래도 더 많을 것이다. (-f 10이나 -f 25처럼 시스템과 네트워크 연결이 얼마나 감당 가능한지에 따라 수정할 수 있다) 이를 통해 수십 수백개의 서버에 대한 명령어 실행을 빠르게 할 수 있다.\n대부분의 사람들은 action의 target을 command/action 자체보다 더 앞에 배치한다. (\u0026ldquo;X 서버에서, Y 명령어를 실행해라\u0026rdquo;) 하지만 머랫속에서 반대로 작동한다면 (\u0026ldquo;Y 명령러를 실행해라. X 서버에서.\u0026quot;) target을 arguments 다음에 배치할 수도 있다. (ansible -a \u0026quot;hostname\u0026quot; multi) 이 둘은 완전히 일치하는 것이다.\n Learning about your environment 이제 우리는 Vagrant가 정상적으로 hostname을 설정한 것을 확인했다. 이제 다른것들을 확인해 보도록 하자.\n먼저, application을 위한 hard disk space가 충분한지 확인해보자.\n$ ansible multi -a \u0026#34;df -h\u0026#34; 192.168.60.6 | success | rc=0 \u0026gt;\u0026gt; Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 19G 1014M 18G 6% / devtmpfs 111M 0 111M 0% /dev tmpfs 120M 0 120M 0% /dev/shm tmpfs 120M 4.3M 115M 4% /run tmpfs 120M 0 120M 0% /sys/fs/cgroup /dev/sda1 497M 124M 374M 25% /boot none 233G 217G 17G 94% /vagrant 192.168.60.5 | success | rc=0 \u0026gt;\u0026gt; Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 19G 1014M 18G 6% / devtmpfs 111M 0 111M 0% /dev tmpfs 120M 0 120M 0% /dev/shm tmpfs 120M 4.3M 115M 4% /run tmpfs 120M 0 120M 0% /sys/fs/cgroup /dev/sda1 497M 124M 374M 25% /boot none 233G 217G 17G 94% /vagrant 192.168.60.4 | success | rc=0 \u0026gt;\u0026gt; Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 19G 1014M 18G 6% / devtmpfs 111M 0 111M 0% /dev tmpfs 120M 0 120M 0% /dev/shm tmpfs 120M 4.3M 115M 4% /run tmpfs 120M 0 120M 0% /sys/fs/cgroup /dev/sda1 497M 124M 374M 25% /boot none 233G 217G 17G 94% /vagrant 현재 충분한 만큼의 공간이 있는 것처럼 보인다. 우리의 application은 가벼운 편이다.\n두번째로 서버에 메모리가 충분한지 확인한다.\n$ ansible multi -a \u0026#34;free -m\u0026#34; 192.168.60.4 | success | rc=0 \u0026gt;\u0026gt; total used free shared buffers cached Mem: 238 187 50 4 1 69 -/+ buffers/cache: 116 121 Swap: 1055 0 1055 192.168.60.6 | success | rc=0 \u0026gt;\u0026gt; total used free shared buffers cached Mem: 238 190 47 4 1 72 -/+ buffers/cache: 116 121 Swap: 1055 0 1055 192.168.60.5 | success | rc=0 \u0026gt;\u0026gt; total used free shared buffers cached Mem: 238 186 52 4 1 67 -/+ buffers/cache: 116 121 Swap: 1055 0 1055 메모리는 약간 타이트하다. 하지만 우리는 localhost에 3개의 VM을 돌리고 있음을 감안해야한다.\n세 번째로 날짜 및 시간이 잘 맞춰졌는지 확인한다.\n$ ansible multi -a \u0026#34;date\u0026#34; 192.168.60.5 | success | rc=0 \u0026gt;\u0026gt; Sat Feb 1 20:23:08 UTC 2021 192.168.60.4 | success | rc=0 \u0026gt;\u0026gt; Sat Feb 1 20:23:08 UTC 2021 192.168.60.6 | success | rc=0 \u0026gt;\u0026gt; Sat Feb 1 20:23:08 UTC 2021 대부분의 어플리케이션은 각 서버의 시간 jitter에 약간은 견딜 수 있게 설계되어있지만 다른 서버들의 시간들을 가능한 가깝게 하는 것은 좋은 방법이고 이를 간단하게 할 수 있는것은 설정하는 것이 쉬운 Network Time Protocol을 사용하는 것이다. 나중에 이를 Ansible의 modules를 이용하여 힘쓰지 않고 프로세스를 진행해 볼 것이다.\n특정 서버에 대해서 모든 환경적인 자세한 부분(Ansible의 lingo에서는 facts라고 한다)을 확인하고 싶으면 ansible [host-or-group] -m setup을 입력하면 된다. 이는 매 분마다 서버에 대한 자세한 사항들을 보여준다. (file system, memory, OS, network interface 등이 포함된다)\n Make changes using Ansible modules 우리는 NTP daemon을 서버에 설치하여 시간을 동기화할 것이다. yum install -y ntp 명령어를 각 서버에서 실행하는 대신 ansible의 yum module을 이용해서 같은것을 해볼 것이다. (우리가 앞선 playbook 예제에서 했던 것과 같지만 이번에는 ad-hoc command를 사용할 것이다.)\n$ ansible multi -b -m yum -a \u0026#34;name=ntp state=installed\u0026#34; NTP가 이미 설치되어있기 때문에 세개의 \u0026ldquo;success\u0026rdquo; 메시지를 볼 수 있을 것이다. 이는 모든 작업이 순서대로 잘 되었음을 의미한다.\n-b 옵션은 Ansible이 sudo 권한으로 명령어를 실행할 수 있게 한다. 이는 우리의 Vagrant VM에서는 잘 작동하지만 sudo password를 요구로 하는 서버에 대해서는 -k 옵션도 지정하여 Ansible이 필요로 하는 sudo password를 입력해야한다.\n 이제 NTP daemon이 시작되었는지 확인하고 부팅 시 시작되도록 할 것이다. 우리는 service ntpd start 와 chkconfig ntpd on 두 명령어를 사용하기 보단 Ansible의 service module을 사용할 것이다.\n$ ansible multi -b -m service -a \u0026#34;name=ntpd state=started enabled=yes\u0026#34; 모든 서버에서 다음과 같은 메시지를 출력할 것이다.\n\u0026quot;changed\u0026quot;: true, \u0026quot;enabled\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;ntpd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;started\u0026quot; 동일한 명령어를 다시 쳐도 Ansible이 nothing has changed라고 리포트하는 것 빼곤 모든 결과는 같을 것이고 이 경우 changed 값은 false가 될 것 이다.\nAnsible의 module을 일반 shell command 대신 사용할 경우 Ansible이 제공하는 상화와 idempotency를 이용할 수 있다. shell command를 실행시키더라도 이를 Ansible의 shell이나 command module로 감싸줘야 하지만(ansible -m shell -a \u0026quot;date\u0026quot; multi처럼), ad-hoc으로 실행하는 것들은 Ansible module을 사용할 필요는 없다.\n마지막으로 체크해야하는 사항은 NTP server의 official time과 거의 일치하는지 확인하는 것이다.\n$ ansible multi -b -a \u0026#34;service ntpd stop\u0026#34; $ ansible multi -b -a \u0026#34;ntpdate -q 0.rhel.pool.ntp.org\u0026#34; $ ansible multi -b -a \u0026#34;service ntpd start\u0026#34; ntpdate 명령어를 실행하려면 ntpd service가 중지되어야 하기 때문에 service를 중지했고 명령어를 실행한 뒤 다시 서비스를 시작했다.\n나의 테스트에서 모든 세 서버에서 3/100초 내에 차이를 보여줬다. 내 목적에는 충분한 것이었다.\nConfigure groups of servers, or individual servers 우리의 가상 웹 어플리케이션은 Django를 사용할 것이며 따라서 우리는 Django와 그 dependency들을 설치해야한다. Django는 official CentOS yum repository에 없지만 우리는 Python의 easy_install을 통해서 설치할 것이다. (편리한 Ansible module이 있다.)\n$ ansible app -b -m yum -a \u0026#34;name=MySQL-python state=present\u0026#34; $ ansible app -b -m yum -a \u0026#34;name=python3-setuptools state=present\u0026#34; $ ansible app -b -m pip -a \u0026#34;name=django executable=pip3\u0026#34; 우리는 easy_install을 통해 설치한 django를 pip를 통해서도 설치할 수 있다. (Ansible의 easy_install module은 pip처럼 uninstall을 지원하 지않기 때문에 사용한다) 하지만 간단하게 하기 위해 easy_install을 사용하였다.\nDjango가 설치 되었고 제대로 동작하는지 확인해보자.\n$ ansible app -a \u0026#34;python3 -c \u0026#39;import django; print (django.get_version())\u0026#39;\u0026#34; 192.168.60.5 | CHANGED | rc=0 \u0026gt;\u0026gt; 3.0.3 192.168.60.4 | CHANGED | rc=0 \u0026gt;\u0026gt; 3.0.3 우리의 앱 서버가 정상적으로 동작하는 것처럼 보인다. 이제 우리는 database server에 대해 해볼 것이다.\n이 챕터에서 한 대부분의 설정은 Ansible playbook으로 하는 것이 더 좋다. (이 책의 뒷부분에서 더 자세히 다뤄볼 것이다.) 이 챕터는 Ansible을 통해 얼마나 쉽게 여러 서버를 관리할 수 있는지에 대해서 좀 더 집중할 것이다. 서버를 shell command를 통해 설정했다고 하더라도 Ansible은 굉장히 많은 시간을 줄여줄 수 있고 더 안전하고 효과적인 방법으로 모든 것들을 처리해줄 수 있다.\n Configure the Database servers 우리는 어플리케이션 서버를 Ansible의 main inventory에서 app group으로 정의하여 사용하였고 database server를 이와 비슷하게 db group으로 정의하여 설정할 것이다.\nMariaDB를 설치하고 이를 실행시키고 서버의 방화벽에서 MariaDB의 deafult port인 3306을 허용시키자.\n$ ansible db -b -m yum -a \u0026#34;name=mariadb-server state=present\u0026#34; $ ansible db -b -m service -a \u0026#34;name=mariadb state=started enabled=yes\u0026#34; $ ansible db -b -a \u0026#34;iptables -F\u0026#34; $ ansible db -b -a \u0026#34;iptables -A INPUT -s 192.168.60.0/24 -p tcp -m tcp --dport 3306 -j ACCEPT\u0026#34; 여기서 app server에서 database를 연결하려고 시도할 경우 연결이 불가능하지만, 연결할 필요가 없다. 왜냐면 MariaDB를 계속해서 셋업해야하기 때문이다. 전형적으로 이를 서버에 접속하여 mysql_secure_installation을 통해 한다. 그러나 운좋게도 Ansible은 MariaDB server를 mysql_* module로 제어할 수 있다. 이제 app server에서 MySQL로 접속할 수 있게 허용할 것이다. MySQL module은 MySQL-python module이 managed server에 설치되어있어야 한다.\n왜 MySQL이 아닌 MariaDB인가? RHEL 7과 CentOS 7는 MariaDB를 default supported MySQL-compatible database server로 지원한다. MariaDB와 관련된 몇몇 툴들은 MySQL* 네이밍 규칙을 가진 예전 것들을 사용하며 MySQL로 하더라도 MariaDB를 하는것과 비슷할 것이다.\n $ ansible db -b -m yum -a \u0026#34;name=MySQL-python state=present\u0026#34; $ ansible db -b -m mysql_user -a \u0026#34;name=django host=% password=12345 priv=*.*:ALL state=present\u0026#34; 여기서 Django application을 app server에 생성하거나 배포할 수 있어야 한다. 그 뒤 database server에 username=django, password=12345로 접속할 수 있을 것이다.\n여기서 사용한 MySQL configuration은 예시/개발 목적으로만 사용해야 한다. MySQL의 안전하게 하려면 test database의 삭제와 root user account에 password를 추가하고, 3306 port로 접근하는 IP address들을 제한하는 등 몇가지 더 할 일들이 있다. 이 중 몇몇은 이 책의 뒷부분에서 다루도록 하겠지만, 당신의 서버를 보호하는 것은 당신의 책임이다. 제대로 보호했는지 확실히 하라.\n Make changes to just one server 축하한다! 이제 Django와 MySQL을 실행시킬 작은 웹 어플리케이션 환경을 가지게 되었다. 앱 서버 앞단에 요청을 분배시켜줄 로드밸런서도 없어 충분하지는 않다. 하지만 서버에 접속하지 않고도 빠르게 이것들을 설정할 것이다. 더 인상깊은 점은 어느 ansible command도 다시 실행할 수 있고, 이것이 어떤 차이점도 만들지 않는다는 것이다. 이런 것들은 \u0026quot;changed\u0026quot;: false를 리턴할 것이고 기존의 설정이 변경되지 않았음을 알려준다.\n이제 local infrastructure가 동작하는 동안 로그를 보다보면 두 앱 서버중 하나의 시간이 NTP daemon이 충돌나거나 어떤 이유로 멈췄을 경우 다른 서버와 일치하지 않을 수 있다. 빠르게 다음의 명령어를 통해 ntpd의 상태를 체크해보자.\n$ ansible app -b -a \u0026#34;service ntpd status\u0026#34; 그리고 app server의 서비스들을 재시작한다.\n$ ansible app -b -a \u0026#34;service ntpd restart\u0026#34; --limit \u0026#34;192.168.60.4\u0026#34; 이 명령어에서 --limit argument는 명령어를 지정된 그룹 내에서 특정한 호스트에서만 실행하도록 제한하는 것이다. --limit은 정확한 string 또는 regular expression을 쓸 수 있다. (~를 prefix로 쓰면 된다) 위의 명령어는 .4 서버에서만 실행하고 싶다면 더 간단히 할 수 있다. (.4로 끝나는 IP 주소가 하나만 있다고 확신해야 한다) 다음의 명령어는 정확히 같은 동작을 한다.\n# Limit hosts with a simple pattern (asterisk is a wildcard). $ ansible app -b -a \u0026#34;service ntpd restart\u0026#34; --limit \u0026#34;*.4\u0026#34; # Limit hosts with a regular expression (prefix with a tilde). $ ansible app -b -a \u0026#34;service ntpd restart\u0026#34; --limit ~\u0026#34;.*\\.4\u0026#34; 이 예시에서 우리는 IP 주소를 hostname 대신에 사용하였다. 하지만 많은 실제 시나리오에서는 nyc-dev-1.example.com처럼 hostname을 많이 사용하게 된다. 따라서 정규식을 활용하는 것이 더 유용할 것이다.\n하나의 서버에 대해 명령어를 실행할때만 --limit 옵션을 주어라. 동일한 세트의 서버에 --limit 명령어를 자주 사용하게 된다면 이들을 inventory file에서 group으로 묶는 것을 고려해 보아라. 그 방법이 ansible [my-new-group-name] [command]를 사용할 수 있는 방법이고 타이핑을 줄일 수 있다.\n Manage users and groups 내가 Ansible의 ad-hoc command를 사용하는 일반적인 사용법은 user와 group 관리이다. 내가 user를 생성하며 home folder를 생성할지 말지와, 특정 유저를 특정 group에 추가하는 방법을 얼마나 많이 구글링했는지 모른다.\nAnsible의 user와 group module은 이런 것들을 어느 Linux 에서나 꽤 간단하고 표준적인 방법으로 할 수 있게 해준다.\n먼저, 간단히 server administrator를 위한 admin group을 app server에 추가하자.\n$ ansible app -b -m group -a \u0026#34;name=admin state=present\u0026#34; group module은 매우 간단하다. group을 state=absent로 하면 삭제가 되고 group id는 gid=[gid]로 설정할 수 있으며 group이 system group인지 나타내려면 system=yes를 설정하면 된다.\n이제 johndoe라는 user를 app server에 추가하고 생성한 group에 넣을 것이며 /home/johndoe에 home directory를 추가할 것이다. (Linux 배포판에서 default location이다)\n$ ansible app -b -m user -a \u0026#34;name=johndoe group=admin createhome=yes\u0026#34; 새로운 유저에 대해 자동으로 SSH key를 생성하고 싶다면 (존재하지 않는 경우) 같은 명령어를 generate_ssh_key=yes를 넣어 실행하면 된다. 또한 user의 UID를 uid=[uid]를 통해 설정할 수 있고, user의 shell을 shell=[shell]로 설정할 수 있으며 password를 password=[encrypted-password]로 설정할 수 있다.\naccount를 삭제하려면 어떻게 해야 할까?\n$ ansible app -b -m user -a \u0026#34;name=johndoe state=absent remove=yes\u0026#34; Ansible의 user module을 통해 useradd, userdel, usermod의 모든 것을 사용할 수 있으며 심지어 더 쉽다. 공식 User module 가이드에 더 자세한 설명이 있다.\nManage files and directories 또 다른 ad-hoc command의 일반적인 사용법은 remote file management이다. Ansible은 host에서 remote server로 파일을 복사하는 것, 디렉토리를 생성하는 것, 파일과 디렉토리의 권한과 소유권을 관리하는 것, 파일과 디렉토리를 삭제하는 것이 매우 쉽다.\nGet information about a file 파일의 권한, MD5, 소유자를 확인하려면 Ansible의 stat module을 사용하면 된다.\n$ ansible multi -m stat -a \u0026#34;path=/etc/environment\u0026#34; 이는 stat 명령어를 실행했을 때와 같은 정보를 보여주지만 JSON 형태로 출력해주어 좀 더 쉽게 parsing할 수 있게 된다. (또는 나중에 playbook에서 조건을 걸어 어떤일을 할지 말지 결정할 수 있게 한다)\nCopy a file to the servers 아마도 scp나 rsync를 통해 파일과 디렉토리를 remote server로 복사해왔을 것이다. Ansible은 rsync module을 가지고 있으며 대부분의 파일 복사관련 명령은 copy module로도 할 수 있다.\n$ ansible multi -m copy -a \u0026#34;src=/etc/hosts dest=/tmp/hosts\u0026#34; src는 파일이나 디렉토리가 될 수 있다. 마지막에 슬래쉬로 끝나게 되면 디렉토리의 내용들만 dest로 복사가 된다. 슬래쉬를 생략하면 내용과 디렉토리 그 자체가 dest로 복사된다.\ncopy module은 단일 파일 복사에 완벽하며 작은 디렉토리에서도 잘 작동한다. 수백개의 파일을 복사하려는 경우, 특히 서브 디렉토리가 많은 경우에는 Ansible의 unarchive module이나 synchronize module을 이용해서 복사하는 것을 고려해보면 좋을 것이다.\nRetrieve a file from the servers fetch module은 copy module과 거의 비슷하게 동작하지만 반대인 것이 다르다. 가장 주요한 차이점은 파일이 local dest의 디렉토리로 일치하는 host의 파일을 가져온다. 예를 들어 다음의 명령어는 hosts file을 서버에서 가져오는 것이다.\n$ ansible multi -b -m fetch -a \u0026#34;src=/etc/hosts dest=/tmp\u0026#34; Fetch는 default로 각 서버의 /etc/hosts 파일을 destination folder 안에 host의 이름을 추가하여 저장할 것이다. (우리의 경우 세 IP 주소이다) 따라서 db server의 hosts file은 /tmp/192.168.60.6/etc/hosts로 저장될 것이다.\nflat=yes라는 파라미터를 추가하고 dest를 dest=/tmp/로 설정하여(슬래쉬로 끝내기) Ansible이 file을 직접 /tmp 디렉토리로 fetch할 수 있다. 하지만 filename은 반드시 유일해야 이것이 동작하고 따라서 여러 호스트에서 파일을 가져올 때는 적합하지 않다. flat=yes는 하나의 호스트에서 파일을 가져올 때만 써야한다.\nCreate directories and files file module을 사용하여 파일과 디렉토리를 생성(touch 처럼)할 수 있고, 권한 관리와 파일과 디렉토리의 소유권을 관리, SELinux 속성 수정, symlink 생성을 할 수 있다.\n디렉토리를 생성하는 방법이다.\n$ ansible multi -m file -a \u0026#34;dest=/tmp/test mode=644 state=directory\u0026#34; symlink를 생성하는 방법이다.\n$ ansible multi -m file -a \u0026#34;src=/src/symlink dest=/dest/symlink owner=root group=root state=link\u0026#34; Delete directories and files state를 absent로 설정하여 파일이나 디렉토리를 삭제할 수 있다.\n$ ansible multi -m file -a \u0026#34;dest=/tmp/test state=absent\u0026#34; Ansible을 통해 원격 파일을 관리하는 방법은 매우 많다. 우리는 짧게 copy와 file module을 살펴보았지만, 다른 lineinfile, ini_file, unarchive같은 file-management module의 문서도 읽어보도록 해라. 이 책에서는 이런 module에 대해 나중 챕터에서 다루도록 하겠다. (playbooks와 함께.)\nRun operatioins in the background 몇몇 operation은 약간 시간이 걸린다(수 분에서 몇 시간까지 걸리기도 한다). 예를 들어, yum date나 apt-get update \u0026amp;\u0026amp; apt-get dist-upgrade를 할 경우, 서버에서 모든 패키지가 업데이트 되기까지 수 분이 걸릴 수 있다.\n이러한 상황에서 Ansible이 명령어를 asynchronous하게 실행하고 명령어가 끝났을 때 서버에서 결과를 가져오게 할 수 있다. 하나의 서버만 관리한다면 이는 그렇게 효과적이지는 않겠지만 많은 서버를 관리한다면 모든 서버에서 명령을 굉장히 빠르게 시작하고(특히 --forks 값을 늘리면 더욱 더 빨라진다) 이후에 서버에서 최신 상태를 polling할 수 있다.\n명령어를 background에서 실행하려면 다음의 옵션을 설정한다.\n -B \u0026lt;seconds\u0026gt;: job이 동작할 수 있는 최대 시간 (초) -P \u0026lt;seconds\u0026gt;: job 상태를 업데이트하기 위해 polling할 때 대기하는 시간 (초)  Update servers asynchronously, monitoring progress yum -y update를 모든 서버에서 시작하여 기다려 보자. -P 옵션을 쓰지 않으면 Ansible은 default로 10초마다 polling 한다.\n$ ansible multi -b -B 3600 -a \u0026#34;yum -y update\u0026#34; 조금 기다리면(VM을 설치한 host에 따라 오래 걸릴수도 있다) 다음과 같은 결과를 볼 수 있다.\n\u0026lt;job 763350539037\u0026gt; finished on 192.168.60.6 =\u0026gt; { \u0026#34;ansible_job_id\u0026#34;: \u0026#34;763350539037\u0026#34;, \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [ \u0026#34;yum\u0026#34;, \u0026#34;-y\u0026#34;, \u0026#34;update\u0026#34; ], \u0026#34;delta\u0026#34;: \u0026#34;0:13:13.973892\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2021-02-09 04:47:58.259723\u0026#34;, \u0026#34;finished\u0026#34;: 1, ... [more info and stdout from job] ... background 작업이 실행되는 동안 Ansible의 async_status module에 jid에 ansible_job_id를 넣어 작업 상태를 확인할 수 있다.\n$ ansible multi -m async_status -a \u0026#34;jid=763350539037\u0026#34; Fire-and-forget tasks 또한 장기간 동작하는 유지보수 스크립트를 돌리거나 완료되기까지 오래 걸리는 어떤 작업을 하면, 그 작업을 가만히 기다리고 싶지는 않을 것이다. 이런 경우에 -B를 높게 설정할 수 있다. (그렇게 해서 작업이 Ansible이 이를 죽이기 전에 끝내도록 할 수 있다.) 그리고 -P를 0으로 설정하여 Ansible이 명령을 실행하고 잊어버리게 할 수 있다.\n$ ansible multi -B 3600 -P 0 -a \u0026#34;/path/to/fire-and-forget-script.sh\u0026#34; jid를 통해 상태를 추적할 수 없지만 fire-and-forget 작업에는 유용하다.\n원격으로 추적이 불가능한 작업에 대해서는 task의 진행 경과를 log로 남기는 것, 실패 시 알람을 보내는 것은 좋은 방법이다. 특히, backup을 하는 background 작업이나 business-critical database 유지관리 작업에 유용할 것이다.\n 또한 Ansible의 playbook을 async와 poll 파라미터를 정의하여 background에서 asynchrnous로 동작할 수도 있다. 나중 챕터에서 playbook의 background 동작을 자세히 살펴볼 것이다.\nCheck log files 때로 application의 에러를 디버깅 할때 또는 다른 문제를 진단할 때 서버의 log 파일을 확인해야할 필요가 있다. 일반적인 log 명령(tail, cat, grep 등)은 ansible 명령어를 통해 할 수 있다. 여기엔 몇가지 경고가 있다.\n 지속적으로 파일을 모니터링하는 tail -f같은 것은 Ansible을 통해서는 할 수 없다. 왜냐면 Ansible은 명령이 완료되었을때의 결과만 출력하기 때문이고, file을 following하는 것은 Control+C를 입력하기 전까지는 완료되지 않기 때문이다. 언젠가 async module이 이 기능을 가질지 모르지만 현재로썬 가능하지 않다. Ansible에서 명령어를 통해 방대한 양의 데이터를 stdout을 통해 리턴받는 것은 좋은 생각이 아니다. 수 KB 이상의 파일을 cat하려 한다면 각 서버에 개별적으로 로그인 해야한다. Ansible을 통해 실행된 명령어의 결과를 redirect 하거나 filtering 하려면 Ansible의 default command module이 아닌 shell module을 써야 한다. (-m shell을 명령어에 추가한다.)  간단한 예제를 통해 각 서버에서 메시지 로그 파일의 끝의 몇 줄을 확인해보자.\n$ ansible multi -b -a \u0026#34;tail /var/log/messages\u0026#34; 경고 사항에서 언급했듯이 grep같은 것으로 메시지 로그를 필터링 하고싶으면 Ansible의 default command module이 아닌 shell을 사용해라.\n$ ansible multi -b -m shell -a \u0026#34;tail /var/log/messages | grep ansible-command | wc -l\u0026#34; 192.168.60.5 | success | rc=0 \u0026gt;\u0026gt; 12 192.168.60.4 | success | rc=0 \u0026gt;\u0026gt; 12 192.168.60.6 | success | rc=0 \u0026gt;\u0026gt; 14 이 명령어는 얼마나 많은 Ansible command가 각 서버에서 동작했었는지를 보여준다. (숫자는 다를 수 있다)\nManage cron jobs cron을 통한 주기적인 작업은 시스템의 crontab을 통해 할 수 있다. 일반적으로 서버에서의 cron job 설정을 변경하려면 서버에 접속하고 crontab -e를 cron job이 있는 계정에서 사용하여 간격과 작업을 입력한다.\nAnsible은 cron module을 통해 cron jobs를 관리할 수 있다. 매일 4 a.m.에 모든 서버에서 shell script를 실행하고 싶으면 다음과 같은 cron job을 추가하면 된다.\n$ ansible multi -b -m cron -a \u0026#34;name=\u0026#39;daily-cron-all-servers\u0026#39; hour=4 job=\u0026#39;/path/to/daily-script.sh\u0026#39;\u0026#34; Ansible은 지정하지 않은 값에 대해서는 *이라고 가정할 것이다. (유효한 값은 day, hour, minute, month, weekday이다) 또한 special_time=[value]를 사용하여 reboot, yearly, monthly같은 특정 시간을 설정할 수도 있다. job을 특정 유저로 하고싶으면 user=[user]를 사용하면 되고 현재 crontab을 백업하고 싶으면 backup=yes를 사용하면 된다.\ncron job을 제거하려면 어떻게 해야할까? 간단히 동일한 cron 명령어에다가 삭제하고 싶은 cron job 이름을 적고 state=absent를 사용하면 된다.\n$ ansible multi -b -m cron -a \u0026#34;name=\u0026#39;daily-cron-all-servers\u0026#39; state=absent\u0026#34; 또한 Ansible로 custom crontab 파일을 관리할 수 있다. 앞선 syntax와 동일하게 사용하지만 cron file의 location을 cron_file=cron_file_name으로 설정하면 된다. (cron_file_name은 /etc/cron.d에 위치한 cron file이다)\nAnsible은 Ansible-managed crontab 목록을 바로 위에 #Ansible: daily-cron-all-servers같은 comment를 남겨서 나타낸다. 이 crontab은 이대로 남겨두는 것이 제일 좋고, 항상 ad-hoc command 또는 Ansible의 cron module을 사용하는 playbook으로 관리해야 한다.\n Deploy a version-controlled application git checkout으로 업데이트를 하거나 새로운 코드를 서버에서 복사한 뒤 배포를 위해 명령어를 실행시키는 간단한 어플리케이션의 배포에서 Ansible의 ad-hoc mode가 도움이 될 수 있다. 더 복잡한 배포에서 Ansible playbook과 rolling update 기능(이후에 더 설명할 것이다)을 사용하여 zero downtime으로 배포를 성공적으로 할 수 있다.\n아래의 예시에서 하나 또는 두개의 서버에서 /opt/myapp 디렉토리에 있는 간단한 어플리케이션을 실행한다고 가정한다. 이 디렉토리는 중앙 서버 또는 GitHub같은 곳에서 clone한 git repository이고 어플리케이션의 배포와 업데이트는 clone을 업데이트 하고나서 /opt/myapp/scripts/update.sh에 있는 shell script를 실행시켜 진행된다.\n먼저, 모든 app 서버에서 application의 새로운 branch인 1.2.4로 git checkout을 하여 업데이트 한다.\n$ ansible app -b -m git -a \u0026#34;repo=git://example.com/path/to/repo.git dest=/opt/myapp update=yes version=1.2.4\u0026#34; Ansible의 git module은 branch, tag 또는 version parameter와 함께 특정한 commit을 지정할 수 있도록 한다. (이 경우 우리는 1.2.4 tag로 checkout하지만 prod같은 brach 이름으로 명령어를 실행하고자 한다면 Ansible은 이를 해줄 것이다) Ansible이 강제로 checked-out copy를 업데이트하도록 하려면 update=yes를 추가하면 된다. repo와 dest 옵션은 의미가 명확하다.\n그 다음 application의 update.sh shell script를 실행시킨다.\n$ ansible app -b -a \u0026#34;/opt/myapp/update.sh\u0026#34; ad-hoc command는 (위에서 본 예제와 같은)간단한 배포에 적합하지만 더 복잡한 어플리케이션 또는 복잡한 인프라를 필요로 할 경우 사용할 수 있는 Ansible의 더 강력하고 유연한 어플리케이션 배포 기능은 이 책의 뒷부분에 설명되어있다. 특히 Rolling Updates 섹션을 보아라.\nAnsible's SSH connection history Ansible의 가장 좋은 기능 중 하나는 추가적인 어플리케이션이나 daemon을 관리하는 서버에서 실행시키지 않아도 된다는 것이다. 대신 서버와 적절한 프로토콜로 통신을 하며 Ansible은 거의 모든 Linux 서버에서 동작하는 일반적인 관리를 위해 표준화되고 안전한 SSH 연결을 사용한다.\n안정적이고 빠르고 안전한 SSH 연결은 Ansible 통신 기능의 심장과도 같기 때문에 Ansible의 SSH 구현은 지난 몇년간 지속적으로 개선되어왔고 지금도 계속되고있다.\nAnsible의 SSH 연결 방법의 일반적인 것 중 하나는 Ansible이 연결을 통해 play나 command로 정의한 하나 또는 몇가지 파일을 원격 서버로 전송하고, play/command를 실행하고, 전송된 파일을 삭제하고, 결과를 리포트하는 것이다. 이런 이벤트 sequence는 나중의 Ansible에서는 변경될 수 있고 더 간단하고 직접적으로 변할 수 있다. (아래 Ansible 1.5를 보아라) 하지만 빠르고, 안정적이고 안전한 SSH 연결은 Ansible에서 무엇보다도 중요하다.\nParamiko 먼저 Ansible은 paramiko(Python에서 SSH2 implementaion을 한 open source)만을 사용한다. 하지만 단일 언어(Python)에 대한 단일 라이브러리로 paramiko의 개발은 OpenSSH의 개발을 따라잡지 못하고 있다. (거의 모든 곳에서 사용되는 SSH의 표준 구현이다) 그리고 OpenSSH보다 퍼포먼스와 보안이 약간은 떨어진다. 최소한 작성자의 관점에서는 말이다.\nAnsible이 계속해서 paramiko를 지원하며, 그리고 이를 (OpenSSH 5.6 또는 이후버전에서 option으로 지원하는)ControlPersist에서는 지원하지 않는 system의 default로 선택하며(RHEL 5/6 처럼) (ControlPersist는 서버의 SSH config에서 설정된 ControlPersist timeout이 될때까지 SSH 연결이 유지되도록 한다)\nOpenSSH (default) Ansible 1.3부터 Ansible은 default로 OpenSSH 연결을 사용하여 ControlPersist를 지원하며 서버에 연결하도록 하였다. Ansible은 이 기능을 0.5 버전부터 가지고 있었지만 1.3부터 default가 되었다.\n대부분의 local SSH configuration parameters(hosts, key files, 등)은 사용되지만 22 포트(default SSH Port)가 아닌 포트로 연결을 해야한다면 inventory file(ansible_ssh_port 옵션)에 포트를 지정하거나 ansible command를 이용해야 한다.\nOpenSSH는 paramiko보다 빠르고 더 믿을 수 있다. 하지만 Ansible을 빠르게 하는 방법은 여전히 더 존재한다.\nAccelerated Mode ad-hoc command가 그렇게 도움이 되지 않더라도 Ansible의 Accelerated mode는 playbook에 보다 더 좋은 퍼포먼스를 보여준다. 반복적으로 SSH를 통해 연결하는 것 대신, Ansible은 SSH를 처음에 연결하고, 처음 연결에 사용한 AES key를 사용하여 나머지 명령어와 통신하고 분리된 포트를 통해 전송한다(5099가 default지만 설정 가능하다).\naccelerated mode를 위해 필요한 추가적인 패키지는 python-keyczar 뿐이고 OpenSSH/Paramiko mode에서 사용가능한 대부분의 것은 sudo를 사용할 때 빼고 Accerlerated mode에서 사용 가능하다.\n sudoers 파일에 requiretty가 disabled 되어있다. (여기서 주석을 해제하거나 각 유저에 대해 Defaults:username !requiretty로 줄을 변경한다) sudoers 파일에서 NOPASSWD 설정을 하여 sudo password를 disable한다.  Accelerated mode는 OpenSSH에 비해 2~4배 더 빠른 성능(특히 파일 전송같은 것들에서)을 보여주고 playbook에서 accelerate: true를 설정하여 활성화 할 수 있다.\n--- - hosts: all accelerate: true 말할 것도 없이 accelerated mode를 사용하면 통신을 할 때 사용할 포트가 방화벽에서 뚫려있어야 한다. (5099 port가 default이며 accelerate 뒤에 이은 accelerate_port 옵션을 통해 지정한 포트 어느것도 될 수 있다.)\naccelerate mode는 Ansible의 통신을 가속화하는 비슷한 방법이지만 ZeroMQ가 controlled 서버에 설치(Ansible의 simple no-dependency와 no-daemon philosophy에 위배된다)되어야 하고 sudo command는 작동하지 않는, 지금은 deprecated된 Fireball mode에서 영감을 얻었다.\nFaster OpenSSH in Ansible 1.5+ Ansible 1.5부터 Ansible의 default OpenSSH 구현에 매우 큰 개선이 있었다.\n파일을 복사하는 대신 이를 원격 서버에서 실행하게 하고 이들을 지우는 새로운 OpenSSH 전송 방법은 SSH 통신을 통해 대부분의 Ansible module에 대한 명령어를 전송하고 실행할 것이다.\n이 연결 방법은 Ansible 1.5+에서만 사용할 수 있고 pipelining=True를 Ansible configuration file(나중에 좀 더 자세히 설명할, ansible.cfg)의 [ssh_connection] 섹션 아래에 추가하여 활성화 할 수 있다.\npipelining=True 설정 옵션은 /etc/sudoers의 Defaults requiretty 옵션을 제거했거나 주석처리 했다면 도움되지 않을 것이다. 대부분의 OS에서 이는 default configuration으로 설정되어 있지만, 이 세팅을 다시 한번 확인하여 fastest connection이 가능할지 확인해라.\n Mac OS X, Ubuntu, Cygwin을 사용한 Windows나 다른 대부분의 OS의 최신 버전으로 ansible과 ansible-playbook을 실행하는 host를 사용한다면, OpenSSH 5.6 이후 버전을 실행해야 Ansible의 SSH connection 세팅과 함께 사용되는 ControlPersist setting과 잘 동작한다.\n만일 Ansible이 실행되고 있는 호스트가 RHEL이나 CentOS를 가지고 있다면 OpenSSH 버전을 최슨으로 업데이트 하여 빠르고/지속 가능한 연결 방법을 사용할 수 있다. OpenSSH 5.6버전 이후는 전부 다 잘 작동한다. 이후의 버전들을 설치하려면 소스코드로부터 컴파일하거나 CentALT같은 다른 레파지토리를 사용하고 yum update openssh를 하면 된다.\n Summary 이 챕터에서 우리는 어떻게 local workstation에 test 목적을 위한 multi-server infrastructure를 Vagrant를 통해 구축하는지, 이를 설정하고 모니터링하고 인프라를 각 서버에 접속하지 않고도 관리하는 방법에 대해 배웠다. 또한 어떻게 Ansible이 원격 서버에 접속하는지에 대해 배웠고 어떻게 ansible 명령어가 많은 서버의 작업들을 빠르게 병렬적으로 수행할 수 있는지 하나씩 보았다.\n이제 우리는 Ansible의 basic과 익숙해지게 되었고, 더 효과적으로 우리만의 infrastructure를 관리할 수 있게 되었다.\n"
},
{
	"uri": "http://kimmj.github.io/ibiza/2020-new-year-holiday-plans/",
	"title": "2020 New Year Holiday Plans",
	"tags": ["holiday"],
	"description": "",
	"content": "설 연휴 계획 date : 2020-01-22T00:34:37+09:00\n- [ ] 연돈 가보기\n  블로그에 댓글기능 추가하기\n  블로그 레이아웃 구성\n 왼쪽 사이드바를 더 넓게.  - [ ] 오른쪽 여백 생성하여 가운데 정렬이 되도록\n- [ ] 배경 설정?\n 폴더 구분지을 prefix 추가 active 상태인 폴더는 다른 prefix로 임시 변경    연휴에도 할일 다 하기\n  Kubernetes localization 확인하기\n  "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-your-environment/",
	"title": "Choose Your Environment",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Spinnaker를 배포하는 방법에는 3가지가 있습니다. Kubernetes 환경에 배포하기, local debian으로 배포하기, local git으로 배포하기가 있습니다.\n여기에서는 Kubernetes 환경에 배포하기를 진행할 것입니다.\nACCOUNT=wonderland hal config deploy edit --type distributed --account-name $ACCOUNT 위와같이 설정하면 됩니다. ACCOUNT는 kubernetes cluster를 추가할 때 사용했던 이름을 사용하면 됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/",
	"title": "CKA Study",
	"tags": [],
	"description": "",
	"content": "Chapter X  10 Kubernetes the Hard Way     09 Networking     08 Storage     07 Security     06 Cluster Maintenance     05 Application Lifecycle Management     04 Logging and Monitoring     03 Scheduling     02 Core Concepts     "
},
{
	"uri": "http://kimmj.github.io/ibiza/diary/",
	"title": "Diary",
	"tags": [],
	"description": "",
	"content": "Diary  20200331     "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/conditionals/",
	"title": "if/then/when - Conditionals",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "많은 task가 특정한 상황에서만 동작해야 하는 조건을 가지고 있다. 몇몇 task는 idempotence가 내장된 module(yum이나 apt 패키지가 설치되어 있는 경우)을 사용하고 우리는 보통 이 task에 대해 추가적인 행동을 정의할 필요가 없다.\n하지만 많은 task에서 - 특히 Ansible의 command나 shell module을 사용하는 경우 - 이 module이 실행되고 나서 변화가 있는지 없는지에 따라 또는 실행했을때 실패되었던 경우, 언제 실행될지를 결정하는 추가적인 input을 필요로 하게 된다.\n우리는 이런 모든 주된 conditional behavior를 Ansible task에 적용할 수 있도록 할 것이고 또한 Ansible에게 언제 play가 끝나는지 또는 실패하는지를 알려줄수도 있다.\nJinja2 Expressions, Python built-ins, and Logic Ansible에서의 conditional에 대해서 이야기하기 전에 Jinja2(Ansible이 template과 conditional 모두에서 사용하는 문법), Python function(보통 built-ins라고 불린다)의 일부분에 대해서 짚고 넘어가는 것이 좋을 것이다. Ansible은 when, changed_when, failed_when과 함께 expressions와 built-ins를 사용하여 Ansible에게 가능한한 정확하게 설명할 수 있다.\nJinja2는 string(string), integer(42), float(42.33), list([1, 2, 3]), tuple(list와 비슷하지만 수정할 수 없는 것), dictionary({key: value, key2: value2}), boolean(true, false)과 같은 literal의 정의를 허용한다.\nJinja2는 또한 기본적인 사칙연산과 비교(==는 일치, !=는 불일치, \u0026gt;=는 이상)를 허용한다. logical operator는 and, or, not이 있고 괄호를 통해 그룹을 지을 수 있다.\n어느 프로그래밍 언어라도 친숙한 것이 있다면 Jinja2 expression의 기본적인 사용법은 금방 배울 수 있을 것이다.\n예를 들면 다음과 같다.\n# The following expressions evaluate to 'true': 1 in [1, 2, 3] 'see' in 'Can you see me?' foo != bar (1 \u0026lt; 2) and ('a' not in 'best') # The following expressions evaluate to 'false': 4 in [1, 2, 3] foo == bar (foo != foo) or (a in [1, 2, 3]) 또한 Jinja2는 주어진 오브젝트에 대해서 테스트할 수 있는 tests 세트를 제공한다. 예를 들어 foo variable을 특정한 서버의 그룹에서만 정의했다면 foo is defined를 conditional로 사용하여 variable이 정의가 되었을 경우 true를, 아니면 false를 리턴하게 할 수 있다.\n또한 undefined(defined의 반대), equalto(==와 비슷하게 동작하는 것), even(짝수일 경우 true를 리턴), iterable(오브젝트를 iterate할 수 있을 경우 true)과 같은 다른 체크들도 할 수 있다. 이 영역에 대해서는 이 책의 뒷부분에서 다룰 것이지만 지금은 Ansible conditional을 Jinja2 expression을 통해 많은 것을 할 수 있다는 것만 알면 된다.\nJinja2를 활용해서 해결할 수 없는 문제의 경우 Python의 built-in library function들(string.split, [number].is_signed()같은 것들)을 사용하여 variable을 조작하고 task가 실행할지 말지, 결과를 change로 할지 failed로 할지 등에 대해 결정할 수 있다.\n예를 들면 때때로 version string을 파싱하여 특정 프로젝트의 major version을 찾아내야하는 경우가 있다. software_version이 4.6.1로 설정되어있다고 가정하면 major version을 .으로 split 한 다음 array에서 첫번째 element를 사용하면 된다. major version이 4인 경우를 확인하기 위해 when을 사용하고 특정한 task를 실행하도록(또는 실행하지 않도록) 설정한다.\n- name: Do something only for version 4 of the software. [task here] when: software_version.split(\u0026#39;.\u0026#39;)[0] == \u0026#39;4\u0026#39; Jinja2 filter와 variable만 사용하는 것이 일반적으로는 가장 좋지만 좀 더 variable을 조작하기 위해서는 Python을 이용하는 것도 나쁘지 않다.\nregister Ansible에서 어느 play든 variable을 register할 수 있고 한번 register 하고 나면 그 variable은 뒤의 모든 task에서 사용할 수 있게 된다. register된 variable은 일반적인 variable이나 host fact처럼 사용할 수 있다.\n많은 경우에 우리는 shell command의 ouput(stdout 또는 stderr)를 필요로 하고 이를 다음과 같은 syntax를 통해 variable에 넣을 수 있다.\n- shell: my_command_here register: my_command_result 후에 우리는 stdout(string으로)을 my_command_result.stdout을 통해 접근할 수 있고 stderr를 my_command_result.stderr를 통해 접근할 수 있다.\nregistered fact는 많은 종류의 task에서 유용하게 사용되며 conditional(play가 실행되는 when과 how를 정의한 것)과 play의 어느 부분에서도 사용할 수 있다. 예를 들어 command가 10.0.4와 같은 version number string을 출력하고 이를 version으로 output을 register한다면 나중에 {{ version.stdout }}처럼 variable을 출력하여 체크해볼 수 있다.\n특정 registered variable에 대한 다른 속성들을 보고싶다면 playbook을 -v 옵션을 통해 실항하면 play ouput을 확인할 수 있다. 보통 changed(play의 결과가 변경되었을 경우), delta(play를 실행하는데 걸린 시간), stderr, stdout과 같은 value에 접근할 수 있다. 몇몇 Ansible module(stat과 같은)들은 registered variable보다 더 많은 정보를 가지고 있기 때문에 -v를 통해 그 안에 무엇이 있는지 확인하는 것이 좋다.\n when play에 추가할 수 있는 유용한 extra key중 하나는 when statement이다. 간단한 when의 사례를 보도록 하자.\n- yum: name=mysql-server state=present when: is_db_server 위의 statement는 is_db_server variable을 앞에서 boolean(true, false)으로 정의했다고 가정하고 value가 true이면 실행하고 false이면 건너뛰도록 한다.\nis_db_server variable을 database server에서만 정의했을 경우(variable이 아예 정의되지 않을 수도 있는 경우를 의미한다) 다음과 같이 task에 조건을 걸 수 있다.\n- yum: name=mysql-server state=present when: (is_db_server is defined) and is_db_server when은 variable이 이전 task에서 등록이 됐는지 확인하는 절차를 함께 넣으면 더욱 강력하다. 예를 들어 실행중인 어플리케이션의 상태를 체크하기 원하고 그 어플리케이션이 ready output을 보고했을 때만 play를 실행하고 싶은 경우가 있다.\n- command: my-app --status register: myapp_result - command: do-something-to-my-app when: \u0026#34;\u0026#39;ready\u0026#39; in myapp_result.stdout\u0026#34; 이 예시는 약간 부자연스럽긴 하지만 when이 task에서 보통 어떻게 쓰이는지에 대해서는 잘 묘사해주고 있다. when을 실제 playbook에서 어떻게 사용하는지 예시를 확인해보자.\n# From our Node.js playbook - register a command\u0026#39;s output, then see # if the path to our app is in the output. Start the app if it\u0026#39;s # not present - command: forever list register: forever_list - command: forever start /path/to/app/app.js when: \u0026#34;forever_list.stdout.find(\u0026#39;/path/to/app/app.js\u0026#39;) == -1\u0026#34; # Run \u0026#39;ping-hosts.sh\u0026#39; script if \u0026#39;ping_hosts\u0026#39; variable is true. - command: /usr/local/bin/ping-hosts.sh when: ping_hosts # Run \u0026#39;git-cleanup.sh\u0026#39; script if a branch we\u0026#39;re interested in is # missing from git\u0026#39;s list of branches in our project. - command: chdir=/path/to/project git branch register: git_branches - command: /path/to/project/scripts/git-cleanup.sh when: \u0026#34;(is_app_server == true) and (\u0026#39;interesting-branch\u0026#39; not in git_branches.stdout)\u0026#34; # Downgrade PHP version if the current version contains \u0026#39;7.0\u0026#39;. - shell: php --version register: php_version - shell: yum -y downgrade php* when: \u0026#34;\u0026#39;7.0\u0026#39; in php_version.stdout\u0026#34; # Copy a file to the remote server if the hosts file doesn\u0026#39;t exist. - stat: path=/etc/hosts register: hosts_file - copy: src=path/to/local/file dest=/path/to/remote/file when: hosts_file.stat.exists == false changed_when and failed_when when처럼 changed_when과 failed_when을 사용하여 특정 task가 changes 또는 failures일 경우 Ansible의 reporting에 영향을 줄 수 있다.\nAnsible이 주어진 command의 결과가 change인지 확인하는 것은 어렵기 때문에 comand나 shell module을 changed_when 사용하지 않고 사용하고자 한다면 Ansible은 항상 change를 report할 것이다. 대부분의 Ansible module은 result가 올바르게 changes로 되던 안되던 report를 하지만 이 동작을 changed_when을 통해서 override할 수도 있다.\nPHP Composer를 command로 사용하여 project dependency들을 설치할 때 Composer가 어떤것을 설치했는지 또는 언제부터 변경사항이 없는지를 알 때 유용하다. 다음은 이에 대한 예시이다.\n- name: Install dependencies via Composer. command: \u0026#34;/usr/local/bin/composer global require phpunit/phpunit --prefer-dist\u0026#34; register: composer changed_when: \u0026#34;\u0026#39;Nothing to install or update\u0026#39; not in composer.stdout\u0026#34; register를 사용하여 command의 result를 저장한 것을 볼 수 있고 그 다음 특정 string이 registered variable의 stdout에 있는지를 확인한다. Composer가 아무것도 안하면 Nothing to install or update를 출력할 것이고 우리는 그 string을 사용하여 Ansible에게 그 task가 change인지를 알려준다.\n많은 command-line ultility가 stdout 대신 stderr를 프린트하기 때문에 failed_when은 Ansible에게 언제 task가 실제로 failed되었는지, 그리고 이를 잘못된 방향으로 report하지 않도록 알려줄 수 있다. 다음은 Jenkins CLI command의 stderr를 파싱하여 Jenkins에서 실제로 우리가 요청한 command의 수행이 실패했는지를 확인해볼 수 있는 예시이다.\n- name: Import a Jenkins job via CLI. shell: \u0026gt; java -jar /opt/jenkins-cli.jar -s http://localhost:8080/ create-job \u0026#34;My Job\u0026#34; \u0026lt; /usr/local/my-job.xml register: import failed_when: \u0026#34;import.stderr and \u0026#39;already exists\u0026#39; not in import.stderr\u0026#34; 이 경우 우리는 command가 error를 리턴했을 때와 error가 already exists를 포함하지 않을 때에만 Ansible이 failure를 report하도록 하고 싶다. command가 job이 이미 존재하는 것을 stderr로 report하는지 아니면 그냥 stdout으로 report하는지에 대해서는 논란의 소지가 많지만 command가 Ansible에 어떤것을 하는지는 설명하기가 쉽다.\nignore_errors 때로 항상 실행되어야 하는 명령어가 있고 이 명령어들이 대부분 error를 발생한다. 또는 실행하는 스크립트에서 error가 에러를 곳곳에서 발생시키고 에러가 사실상 문제점을 나타내는 것이 아지만 짜증나게 하는 경우(그래서 결국 playbook이 실행을 멈추게 되는 경우)가 있다.\n이런 상황에서 ignore_errors를 task에 추가하며 Ansible은 특정 task를 실행하는 데 어떤 문제도 알아차리지 못할 것이다. 그러나 이를 사용하는 것은 항상 조심해야 한다. task에서 에러가 발생하느 경우 playbook이 실제로 문제점을 나타낸다면 fail을 하도록 하고 아닌 경우에도 동작할 수 있는 방법을 찾는것이 가장 좋다.\n"
},
{
	"uri": "http://kimmj.github.io/harbor/",
	"title": "Harbor",
	"tags": [],
	"description": "",
	"content": "Harbor  Harbor 설치     "
},
{
	"uri": "http://kimmj.github.io/git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "Git  git-secret을 통한 github 파일 암호화     Gitignore 설정     "
},
{
	"uri": "http://kimmj.github.io/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": "Python  [번역]Python을 통해 이쁜 CLI 만들기     "
},
{
	"uri": "http://kimmj.github.io/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Docker  http를 사용하는 docker registry를 위한 insecure registry 설정     Docker를 sudo없이 실행하기     [docker-compose] container에서 다른 container로 접속하기     "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/ansible-playbooks/",
	"title": "Ansible Playbooks",
	"tags": ["ansible", "ansible-playbooks"],
	"description": "",
	"content": "Power plays 다른 여느 configuration management solution처럼 Ansible은 configuration file을 설명하는데 메타포를 사용한다. 이를 playbooks라고 부르고 여기에는 특정한 서버나 특정 서버 그룹에서 실행되는 tasks(Ansible의 용어에서는 play)의 리스트가 있다. 미식 축구에서 팀은 게임에서 이기기 위해 사전 정의된 playbook을 플레이의 기반으로 실행하고 따른다. Ansible에서 우리는 playbook(서버가 특정한 configuration state로 가기 위해 실행해야 하는 스텝들의 리스트)을 작성하고 서버 위에서 play되도록 할 것이다.\nPlaybook은 configuration을 정의하는 상황에서는 자주 쓰이고 사람이 읽을 수 있는 간단한 문법을 가진 YAML로 작성되어있다. Playbook은 다른 playbook에 포함될 수 있고 특정 metadata와 옵션들은 다른 play를 하도록 할 수 있으며 또는 playbook이 서버마다 다른 시나리오로 동작하게 할 수도 있다.\nad-hoc 명령어는 그 자체로 Ansible을 powerful하게 만들어준다. playbook은 Ansible을 최고의 server provisioning과 configuration management tool로 만들어준다.\n대부분의 DevOps 사람들이 Ansible에 매료된 이유는 shell scripts를 쉽게 직접 ansible play로 변경할 수 있기 때문이다. 다음의 스크립트가 있다고 가정해보자. 이는 RHEL/CentOS 서버에 Apache를 설치하는 것이다.\n# Install Apache yum install --quiet -y httpd httpd-devel # Copy configuration files cp /path/to/config/httpd.conf /etc/httpd/conf/httpd.conf cp /path/to/config/httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf # Start Apache and configure it to run at boot service httpd start chkconfig httpd on shell script를 실행하려면(이 경우에 파일 이름은 shell-script.sh이다), command line에서 직접 호출하면 된다.\n# (From the same directory in which the shell script resides) $ ./shell-script.sh Ansible Playboook\n--- - hosts: all tasks: - name: Install Apache. command: yum install --quiet -y httpd httpd-devel - name: Copy configuration files. command: \u0026gt; cp /path/to/config/httpd.conf /etc/httpd/conf/httpd.conf - command: \u0026gt; cp /path/to/config/httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf - name: Start Apache and configure it to run at boot. command: service httpd start - command: chkconfig httpd on Ansible Playbook을 실행하려면(이 경우 파일 이름은 playbook.yml이다), ansible-playbook 명령어로 호출할 수 있다.\n# (From the same sirectory in which the playbook resides) $ ansible-playbook playbook.yml Ansible은 표준 shell command를 작성할 수 있고(수 십년간 사용해왔던 명령어들) 시간만 있다면 playbook을 사용하도록 빠르게 바꿀 수 있고 Ansible의 유용한 기능의 장점을 사용하여 configuration을 rebuild할 수 있어 강력한 툴이다.\n위의 playbook에서 Ansible의 command module을 사용하여 standard shell commands를 실행시켰다. 또한 우리는 각 play에 name을 부여하여 playbook을 실행했을 때 play가 사람이 읽을 수 있는 결과를 스크린 또는 로그에 띄울 수 있도록 한다. command module은 다른 트릭들을 가지고 있지만 지금 우리는 shell script가 Ansible playbook으로 귀찮은 작업 없이 바로 변환이 가능하고 확신한다.\n\u0026gt;는 command: module 바로 뒤에 와서 YAML이 \u0026ldquo;자동으로 다음의 indented line을 하나의 긴 string으로 줄바꿈은 스페이스로 분리하며 변환\u0026quot;하도록 해준다. 이는 어떤 케이스에선 readability를 향상시키는데 도움을 준다. 유효한 YAML 문법을 통해 configuration을 설명하는 방법에는 많은 것들이 있고 이런 방법들은 나중에 Appendix B - YAML Conventions and Best Practices 섹션에서 깊게 다루어 볼 것이다.\n이 책은 세가지 task-formatting 테크딕들을 살펴볼 것이다. 하나 또는 두개의 간단한 parameter가 있는 task는 Ansible의 shorthand syntax가 사용될 것이다(예: yum: name=apache2 state=installed). 더 긴 command 입력이 필요한 command나 shell의 대부분의 경우에는 위에서 언급한 \u0026gt; 테크닉을 사용할 것이다. 여러 parameter가 필요한 task들은 YAML object notation이 사용된다. 이는 각 줄의 key와 variable을 대치할때 사용된다.\n 위의 playbook이 정확히 shell script와 같은 동작을 하지만 이를 Ansible의 built-in modules로 어려운 일을 해결할 수 있다.\nRevised Ansible Playbook - Now with idempotence!\n--- - hosts: all become: yes tasks: - name: Install Apache. yum: name={{ item }} state=present with_items: - httpd - httpd-devel - name: Copy configuration files. copy: src: \u0026#34;{{ item.src }}\u0026#34; dest: \u0026#34;{{ item.dest }}\u0026#34; owner: root group: root mode: 0644 with_items: - { src: \u0026#34;/path/to/config/httpd.conf\u0026#34;, dest: \u0026#34;/etc/httpd/conf/httpd.conf\u0026#34; } - { src: \u0026#34;/path/to/config/httpd-vhosts.conf\u0026#34;, dest: \u0026#34;/etc/httpd/conf/httpd-vhosts.conf\u0026#34; } - name: Make sure Apache is started and configure it to run at boot. service: name=httpd state=started enabled=yes 이제 뭔가 되어가고 있다. 이제 이 playbook을 차근차근 밟아보자.\n 처음에 ---는 이 문서를 YAML syntax를 가지고 사용했다고 표시한 것이다. 마치 HTML의 맨 위에 \u0026lt;html\u0026gt;을 쓰는것 또는 PHP code block 제일 위에 \u0026lt;?php를 넣는것과 같다. 둘째 줄의 - hosts: all은 첫(이 경우는 유일하게) play를 정의한 것이고 Ansible이 알고있는 all hosts에서 play를 실행하도록 한다. 셋째 줄에서 become: yes는 Ansible이 모든 명령어를 sudo를 통해 실행하도록 하고 따라서 모든 명령어는 root 유저로 실행될 것이다. 네번째 줄에서 tasks:는 Ansible이 다음의 task 목록들을 이 playbook의 일부로 실행하도록 하는 것이다. 첫번째 task는 name: Install Apache...로 시작하고 name은 서버에서 어떤 동작을 하는 module은 아니다. 그냥 사람이 읽을 수 있는 description을 제공한다. Install Apache를 보는 것은 yum name=httpd state=installed를 보는 것보다 상대적으로 더 연관성이 있어 보인다. 그러나 name 줄을 완전히 지워버버려 아무 문제도 발생하지 않는다.  yum module을 사용하여 Apache를 설치하였다. yum -y install httpd httpd-devel을 사용하는 것 대신 Ansible에게 정확히 우리가 원하는 상태가 뭔지 알려주었다. Ansible은 items 배열에서 우리가 넣은 값들을 가져올 것이다. ({{ variable }}는 Ansible playbook의 variable을 참조한다) 우리는 yum이 state=present를 통해 우리가 정의한 패키지가 설치되어있다고 확신할 수 있지만 state=latest로 하면 최신 버전이 깔려있도록 할 수도 있고 state=absent를 하면 패키지가 설치되지 않은 상태가 되도록 할 수도 있다. Ansible은 with_items를 이용하여 간단한 리스트를 tasks에 주입할 수 있다. item의 list를 하단에 정의하면 각 라인은 play에 하나씩 전달될 것이다. 이 경우에 각 아이템은 {{ item }} variable로 대체되었다.   두번째 task는 다시 사람이 읽을 수 있는 name을 설정하는 것으로 시작한다. (원한다면 지울 수 있다.)  우리는 copy module을 사용하여 파일을 source(우리의 local workstation)에서 destination(관리중인 서버)로 복사할 것이다. 소유권과 권한(owner, group, mode)을 포함한 file metadata같은 더 많은 variable들을 전달할 수도 있다. 이 경우 변수 치환을 위해 multiple elements를 사용한 배열을 쓸 것이다. 이 때 { var1: value, var2: value } 문법을 사용하여 각 variable에 대한 element를 정의할 수 있다. (원하는 만큼 variable을 정의할 수 있고 또는 nested level로 variable을 정의할 수 있다.) play에서 variable을 언급하면 item의 variable에는 .을 통해 접근할 수 있고 따라서 {{ item.var1 }}을 통해 첫번째 variable에 접근할 수 있다. 우리의 예시에서 item.src는 각 item의 src에 접근하는 것이다.   세번째 task는 사람이 읽을 수 있는 포멧으로 정의된 이름을 사용한다.  우리는 service module을 사용하여 특정 service의 원하는 상태를 기술할 것이다. 이 경우에는 Apache의 http daemon인 httpd이다. 우리는 이것이 실행중이길 원하고 따라서 state=started로 설정하였고 또 시스템이 시작되었을 때 실행되길 원하기 때문에 enabled=yes로 설정하였다. (chkconfig httpd on과 동일하다.)    명령어의 리스트를 변환한 것의 가장 좋은 점은 Ansible이 계속해서 모든 우리의 서버의 상태를 추적한다는 것이다. playbook을 처음 실행하면 이는 Apache가 설치되었고 동작되었는지 확인하고, custom configuration이 제위치에 있는지 확인하여 서버를 provision한다.\n더 좋은 점은 두번째 실행했을 때 서버가 제대로 된 상태에 있다면 실제로 아무것도 하지도 않고 우리에게 아무것도 변경된 것이 없다고 말해준다. 따라서 이 짧은 playbook을 통해, 우리는 provision할 수 있고 Apache web server에 대해 적절한 configuration이 되었음을 확인할 수 있다. 게다가 playbook을 --check 옵션으로 실행하면(아래의 다음 섹션에서 확인할 수 있다) configuration이 우리가 playbook에서 정의한 것과 일치하는지를 서버에서 실제 task를 돌리지 않고도 검증할 수 있다.\nconfiguration을 업데이트하고 싶거나 다른 httpd 패키지를 설치하고 싶으면 file을 local에서 수정하거나 패키지를 with_items 리스트안에 넣고 다시 playbook을 실행하면 된다. 하나든 수천개의 서버든지 그 configuration은 우리의 playbook에 맞게 업데이트될 것이다. 그리고 Ansible은 우리에게 어떤 것이 변하였는지 알려줄 것이다. (각각의 production server에 ah-hoc change를 만들지는 않았을 것이다. 그렇지 않은가?)\nRunning Playbooks with ansible-playbook 위의 예시로 playbook을 실행한다면(all hosts로 실행하게 되어있다), playbook은 Ansible inventory file에 정의된 모든 host에 대해 실행될 것이다(챕터 1의 basic inventory file 예시를 확인해보아라).\nLimiting playbooks to particular hosts and groups hosts:를 바꿔서 playbook이 특정한 그룹이나 각각의 hosts에만 동작할 수 있도록 설정할 수 있다. 해당 값은 all hosts, inventory에 정의된 group, 여러 group들 (e.g. webservers, dbservers, 개별 서버(e.g. at1.example.com, 혼합된 host들로 설정할 수 있다. 또한 *.example.com과 같은 wildcard를 사용하여 최상위 도메인 중 매칭되는 모든 도메인으로 설정할 수 있다.\n또한 ansible-playbook 명령어를 통해 playbook이 실행될 hosts를 제한할 수 있다.\n$ ansible-playbook playbook.yml --limit webservers 이 경우(inventory file이 webserver group을 포함한다고 가정한다), playbook이 hosts: all로 설정되어 있거나 webservers group외의 hosts들을 포함한다고 하더라도 webservers에 정의된 hosts에서만 동작한다.\n또한 playbook을 특정 hosts에만 동작하게 할 수 있다.\n$ ansible-playbook playbook.yml --limit xyz.example.com 실행시키기 전에 playbook에 의해 영향을 받는 hosts들이 실제 어떤것인지 확인하고 싶으면 --list-hosts를 사용하면 된다.\n$ ansible-playbook playbook.yml --list-hosts 결과는 다음과 같을 것이다.\nplaybook: playbook.yml play #1 (all): host count=4 127.0.0.1 192.168.24.2 foo.example.com bar.example.com (count는 inventory에 정의된 서버의 숫자이고 그 아래는 inventory에 정의된 모든 hosts의 리스트이다.)\nSetting user and sudo options with ansible-playbook playbook의 hosts 안에 어떤 user도 정의되어 있지 않으면 Ansible은 특정 host에 대해 inventory file에서 정의한 user로 접속한다고 가정하며, 그 다음 local user account name으로 변경할 것이다. --remote-user (-u) 옵션을 통해서 원격 play에 사용될 remote user를 명시적으로 정의할 수 있다.\n$ ansible-playbook playbook.yml --remote-user=johndoe 어떤 상황에서는 원격 서버에서 sudo를 통해 명령어를 수행하기 위해 sudo password를 입력해야할 필요가 있을 것이다. 이런 상황에서는 --ask-sudo-pass (-K) 옵션이 필요할 것이다. 또한 --sudo를 사용하여 명시적으로 playbook 안의 모든 tasks를 강제로 sudo 권한으로 실행하게 할 수 있다. 마지막으로 --sudo-user (-U)옵션을 통해 sudo로 tasks를 실행할 sudo user(default는 root)를 지정할 수 있다.\n예를 들어, 다음의 명령어는 playbook을 sudo로 실행할 것이고, task에서 janedoe라는 sudo user를 쓸 것이고, Ansible은 sudo password를 입력하게 할 것이다.\n$ ansible-playbook playbook.yml --sudo --sudo-user=janedoe --ask-sudo-pass key-based 인증방식을 사용하여 서버에 접속하지 않는다면(챕터 1에서 그렇게 했을 때 security implication에 대한 경고사항을 읽어라), --ask-pass를 사용할 수 있다.\nOther options for ansible-playbook ansible-playbook 명령어는 다른 옵션들도 사용할 수 있다.\n --inventory=PATH (-i PATH): custom inventory file을 정의한다. (default는 default Ansible inventory file이고 보통은 /etc/ansible/hosts에 위치해 있다.) --verbose (-v)는 Verbose mode이다. (성공한 옵션들에 대한 output을 토함한 모든 output을 보여준다.) -vvvv를 통해 매 분마다 자세하게 볼 수 있다. --extra-vars=VARS (-e VARS): playbook에서 사용되는 variables를 \u0026quot;key=value, key=value\u0026quot;의 형태로 정의할 수 있다. --forks=NUM (-f NUM): fork하는 수이다(integer). 이 값을 5보다 크게하여 Ansible이 task를 동시에 실행하는 서버의 수를 늘릴 수 있다. --connection=TYPE (-c TYPE): 사용할 connection의 type이다. (default는 ssh이다. 가끔은 local로 playbook을 local machine에서 실행할 수 있고 또는 cron을 통해 원격 서버에서 실행할 수 있다.) --check: playbook을 Check Mode (\u0026ldquo;Dry Run\u0026rdquo;)으로 실행한다. playbook안에 정의된 모든 task는 모든 hosts에 대해 확인될 것이지만 실제로 실행하지는 않는다.  ansible-playbook을 최대한 활용하기 위해서 중요한 다른 옵션과 configuration variable이 있다. 그러나 여기 목록들로도 지금 챕터에서 우리의 서버와 가상 머신에 playbook을 실행하는데 충분한 것들이다.\n이 챕터의 나머지는 더 현실적인 Ansible playbook을 사용할 것이다. 이 챕터의 모든 예시는 Ansible for DevOps GitHub repository에 있고, 이를 clone하여 컴퓨터에 저장하고(또는 online으로 코드를 띄워서) 뒤의 단계를 더 쉽게 할 수 있다.\n Real-world playbook: CentOS Node.js app server 물론 여전히 옛날 Apache 서버에서 static web page를 포스트하려는 사람에게는 유용할지 모르겠지만, 첫번째 예시는 real-world 시나리오에 적합하지는 않다. 실제로 오늘날 production infrastructure를 관리하는 데 사용되는 것과 관련된 더 많은 것들을 할 수 있는, 좀 더 복잡한 playbook을 실행할 것이다.\n첫번째 playbook은 CentOS를 Node.js를 이용하여 설정할 것이다. 그리고나서 간단한 Node.js application을 설치하고 실행할 것이다. 서버는 매우 간단한 architecture로 되어있다.\n+-------------------------------------------------+ | | | Node.js Server/VM | | | +-------------------------------------------------+ | | | +-------------------------------------------+ | | | | | | | Custom Node.js application | | | | | | | +-------------------------------------------+ | | | | +-------------------------------------------+ | | | | | | | Node.js + NPM | | | | | | | +-------------------------------------------+ | | | | +-------------------------------------------+ | | | | | | | CeonOS 6.4 (Linux) | | | | | | | +-------------------------------------------+ | | | +-------------------------------------------------+ 시작하기에 앞서 우리의 playbook을 포함하는 YAML 파일(이 예시에서는 playbook)을 생성해야 한다. 간단하게 가보자.\n--- - hosts: all become: yes tasks: 먼저 playbook이 동작하게 될 hosts들의 set(all)를 정의한다. (playbook을 특정 groups와 hosts에 제한하는 섹션을 상단에서 확인하라.) 그리하여 뒤의 task들이 해당 hosts에서 실행할 것들이라고 정의한다.\nAdd extra repositories extra repositories(yum이나 apt)를 추가하는 것은 admin들이 서버에서 다른 작업을 하기 전에 특정한 패키지가 사용 가능한지 또는 base installation보다 최신 버전이 있는지 확인하는 작업이다.\n아래의 shell script에서 우리는 EPEL과 Remi repositories를 추가하여 Node.js나 필수 소프트웨어의 최신 버전을 받고 싶다. (이 예시는 RHEL/CentOS 6.x에서 동작한다고 가정한다)\n# Import EPEL GPG Key - see: https://fedoraproject.org/keys wget https://fedoraproject.org/static/0608B895.txt \\  -O /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 # Import Remi GPG key - see: http://rpms.famillecollet.com/RPM-GPG-KEY-remi wget http://rpms.famillecollet.com/RPM-GPG-KEY-remi \\  -O /etc/pki/rpm-gpg/RPM-GPG-KEY-rmi rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-remi # Install EPEL and Remi repos. rpm -Uvh --quiet \\  http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm rpm -Uvh --quiet \\  http://rpms.famillecollet.com/enterprise/remi-release-6.rpm # Install Node.js (npm plus all its dependencies). yum --enablerepo=epel install node 이 shell script는 rpm command를 사용하여 EPEL과 Remi repository GPG keys를 import한다. 그러면 repositoriy들이 추가되고 마침내 Node.js가 설치된다. 이는 간단한 deployment에서는 잘 작동하지만 이미 이전에 실행했었다면(즉, 두개의 repository와 그것의 GPG key가 추가되었다면) 이 모든 명령어를 다 실행하는 것(어떤것은 시간이 걸릴수도 있고 연결이 연결이 잘 안된다면 모든 스크립트가 멈출수도 있다)은 어리석은 짓이다.\n몇가지 단계를 스킵하고싶으면 GPG key를 추가하는 것을 스킵할 수 있고 단순히 command를 --nogpgcheck(Ansible에서는 yum module의 disabled_gpg_check parameter를 yes로 설정한다)으로 실행하기만 하면 된다. 하지만 이를 활성화 하는 것이 좋은 생각이다. GPG는 GNU Privacy Guard로, developer와 package distributor가 그들의 package에 서명하는 것이다. (따라서 이 패키지가 원작자가 만든 것이며 수정이나 변경이 없었음을 확인해준다) 정말 우리가 뭘 하고있는지 알기 전까진 GPG key 체크같은 security setting을 disable하지 말아라.\n Ansible은 좀 더 튼튼하다. 좀 더 verbose할지 몰라도 더 정형화된 방법으로 같은 동작을 수행하며 이해하기 쉽고 나중에 설명할 Ansible의 다른 기능들과 variable을 사용할 수 있다.\n- name: Import EPEL and Remi GPG keys. rpm_key: \u0026#34;key={{ item }} state=present\u0026#34; with_items: - \u0026#34;https://fedoraproject.org/static/0608B895.txt\u0026#34; - \u0026#34;http://rpms.famillecollet.com/RPM-GPG-KEY-remi\u0026#34; - name: Install EPEL and Remi repos. command: \u0026#34;rpm -Uvh --force {{ item.href }} creates={{ item.creates }}\u0026#34; with_items: - { href: \u0026#34;http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\u0026#34;, creates: \u0026#34;/etc/yum.repos.d/remi.repo\u0026#34; } - name: Ensure firewalld is stopped (since this is a test server). service: name=firewalld state=stopped - name: Install Node.js and npm. yum: name=npm state=present enablerepo=epel - name: Install Forever (to run our Node.js app). npm: name=forever global=yes state=present 한 단계씩 살펴보자.\n rpm_key는 매우 간단한 Ansible module로 RPM key를 URL이나 file, 이미 존재하는 key의 key id로부터 가져온다. 그리고 key가 present이거나 absent(state parameter)가 되도록 한다. 우리는 Fedora project의 EPEL과 Remi Repository 두 key를 import하고 싶다. Ansible이 built-in rpm module을 가지고 있지 않기 때문에 우리는 rpm 명령어를 사용할 것이다. 하지만 Ansible의 command module을 사용하여 우리는 두가지를 얻을 수 있다.  creates pamameter로 Ansible 명령어를 실행하지 않을 때를 알려준다. (이 경우 Ansible에게 rpm command가 성공하면 어떤 파일이 존재하게 되는지 알려준다) multidimensional array를 사용하여(with_items) URL을 정의하고 creates로 결과 파일을 체크한다.   Node.js가 없으면 yum은 Node.js를 설치(Node의 package manager인 npm도 함께)한다. 그리고 EPEL repo가 enablerepo parameter를 통해 검색될 수 있도록 한다. (disablerepo를 통해 명시적으로 repository를 disable할 수도 있다) NPM이 설치되었기 때문에 Ansible의 npm module로 Node.js utility를 설치할 것이고 forever로 우리의 app을 실행하고 계속 동작하도록 할 것이다. global을 yes로 설정하면 NPM은 forever node moulde을 /usr/lib/node_modules/에 설치할 것이고 따라서 모든 user가 Node.js app을 시스템에서 사용할 수 있을 것이다.  이제 Node.js app server 셋업의 시작점에 와있다. 이제 간단한 Node.js app을 셋업하여 80포트에서 HTTP request를 받아보자.\nDeploy a Node.js app 다음 단계는 간단한 Node.js app을 우리의 서버에 설치하는 것이다. 먼저, playbook.yml이 위치한 폴더에 새 폴더 app을 생성하고 정말 간단한 Node.js app을 생성할 것이다. 폴더 안에서 새 파일 app.js를 생성하고 다음의 내용을 입력한다.\n// Load the express module var express = require(\u0026#39;express\u0026#39;), app = express.createServer(); // Respond to requests for / with \u0026#39;Hello World\u0026#39;. app.get(\u0026#39;/\u0026#39;, function(req, res){ res.send(\u0026#39;Hello World!\u0026#39;); }); //Listen on port 80 (like a true web server). app.listen(80); console.log(\u0026#39;Express server started successfully.\u0026#39;); 문법이나 이것이 Node.js라는 것에 대해서는 걱정하지 말아라. 우리는 단지 빠르게 배포할 수 있는 예시가 필요한 것이다. 이 예시는 Python, Perl, Java, PHP 또는 다른 언어로 작성되지 않았다. Node는 매우 간단한 언어(JavaScript)이기 때문에 간단하고 가벼운 환경에서 동작할 수 있고 서버를 테스트하거나 생성하는 데 사용하는 쉽고 좋은 언어이다.\n이 작은 app이 Express(Node를 위한 http framework)에 의존성을 가지기 때문에, 우리는 NPM에게 app.js가 위치한 폴더와 같은 곳에 있는 package.json파일로 이 dependency에 대해 알려주어야 한다.\n{ \u0026#34;name\u0026#34;: \u0026#34;examplenodeapp\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Example Express Node.js app.\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jeff Geerling \u0026lt;geerlingguy@mac.com\u0026gt;\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;expres\u0026#34;: \u0026#34;3.x.x\u0026#34; }, \u0026#34;engine\u0026#34;: \u0026#34;node \u0026gt;= 0.10.6\u0026#34; } 이제 다음을 playbook에 추가하고 전체 app을 서버에 복사한다. 그리고 NPM이 필요한 dependency들을 다운로드 받도록 한다(이 경우 express).\n- name: Ensure Node.js app folder exists. file: \u0026#34;path={{ node_apps_location }} state=directory\u0026#34; - name: Copy example Node.js app to server. copy: \u0026#34;src=app dest={{ node_apps_location }}\u0026#34; - name: Install app dependencies defined in package.json. npm: path={{ node_apps_location }}/app 먼저, 우리는 file module을 통해 우리의 app이 설치될 곳에 디렉토리가 있는지 확인할 것이다. 각 command에 사용된 {{ node_apps_location }} variable은 playbook, inventory의 최상단의 vars section 또는 ansible-playbook으로 호출한 command line에서 정의할 수 있다.\n두번째로 우리는 전체 app 폴더를 Ansible의 copy command를 사용하여 복사할 것이다. 이는 똑똑하게 단일 파일과 디렉토리를 구별하며 scp나 rsync처럼 디렉토리 내부를 돌며 동작한다.\nAnsible의 copy module은 단일 또는 작은 그룹의 파일에 대해 잘 동작하고, 자동으로 디렉토리를 순회한다. 파일의 갯수가 수백개가 될 경우 또는 디렉토리 개위가 깊은 경우엔 copy는 교착상태에 빠질 것이다. 이러한 상황에서는 synchronize module로 전체 디렉토리를 복사할 수 있고 또는 unarchive로 archive를 복사하고 이를 서버에서 expand할 수 있다.\n 세번째로 app에 대한 path에 관한 추가적인 arguments 없이 npm을 다시 사용한다. 이는 NPM이 package.json 파일을 읽고 모든 dependency들이 존재하는지 확인하는 것이다.\n이제 거의 다 끝이 났다! 마지막 단계는 app을 실행하는 것이다.\nLaunch a Node.js app 우리는 이제 (이전에 설치한)forever을 통해 app을 실행시킬 것이다.\n- name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \u0026#34;forever start {{ node_apps_location }}/app/app.js\u0026#34; when: \u0026#34;forever_list.stdout.find(\u0026#39;{{ node_apps_location }}/app/app.js\u0026#39;) == -1\u0026#34; 첫 play에서 우리는 두가지 새로운 것을 한다.\n register는 새로운 variable forever_list를 생성하여 다음 play에서 언제 play를 실행해야하는지 결정하는 용도로 사용한다. register는 정의된 command에 variable name을 넣어서 output(stdout, stderr)를 보관한다. changed_when은 Ansible이 명시적으로 이 play가 서버에 변경사항을 줄 것이라고 말해준다. 이 경우 우리는 forever list command가 절대로 서버에 변경사항을 주지 않기 때문에 그냥 false를 설정한다. 이 경우 서버는 command가 실행될 때 전혀 변경되지 않을 것이다.  두번째 play는 실제로 forever를 이용하여 app을 시작한다. 우리는 node{{ node_apps_location }}/app/app.js를 호출해서도 app을 시작할 수 있지만 프로세스를 쉽게 컨트롤할 수 없고 Ansible이 이 play에 hanging이 되는것을 막기 위해서는 nohup이나 \u0026amp;을 써야한다.\nforever는 자신이 관리하는 Node app을 추적하고 우리는 forever의 list 옵션을 사용하여 동작중인 app의 리스트를 확인할 수 있다. 처음 이 playbook을 실행하면 list는 비어있을 것이다. 하지만 나중에 실행했을 때에는 app이 실행중이라면 우리는 이 app의 다른 인스턴스를 생성하고 싶지는 않다. 이런 상황을 피하려면 우리는 Ansible에게 언제 우리가 app을 시작하고 싶어하는지를 when을 통해 알려주면 된다. 특히 우리는 Ansible에게 app의 path가 forever list output에 없을 때에만 실행하라고 했다.\nNode.js app server summary 여기서 우리는 80 port로 오는 HTTP request에 대해 \u0026ldquo;Hello World!\u0026ldquo;를 출력하는 간단한 Node.js app을 설치하는 playbook을 완성하였다.\n이 playbook을 server에서 동작하게 하려면(우리의 경우 Vagrant 또는 수동으로 새로운 VM을 테스트용도로 생성하면 된다) 다음의 명령어를 사용한다(node_apps_location variable을 command를 통해 넘겨라)\n$ ansible-playbook playbook.yml --extra-vars=\u0026#34;node_apps_location=/usr/local/opt/node\u0026#34; 우리의 playbook이 서버를 configuring하고 app을 대포하는 것을 마쳤다면 브라우저를 통해(curl, wget을 사용해도 된다) http://hostname/에 접속해보아라. 다음과 같은 결과를 볼 수 있다.\n간단하지만 매우 강력하다. 우리는 50줄도 안되는 YAML로 Node.js application server 전체를 configure했다.\n전체 Node.js app server playbook은 이 책의 code repository인 https://github.com/geerlingguy/ansible-for-devops의 nodejs 디렉토리에서 확인할 수 있다.\nReal-world playbook: Ubuntu LAMP server with Drupal 이젠 Ansible playbook과 그것을 정의하는 YAML 문법에 친숙해져야 한다. 여기서 대부분의 예시는 CentOS, RHEL, Fedora 서버를 사용한다고 가정한다. Ansible은 다른 Linux나 BSD같은 시스템에서도 잘 동작한다. 다음의 예시에서 Drupal website를 실행하기 위해 우리는 전통적인 LAMP(Linux, Apache, MySQL, PHP)를 Ubuntu 12.04에 셋업할 것이다.\n+-----------------------------------------------------------+ | | | Drupal LAMP Server / VM | | | +-----------------------------------------------------------+ | | | +-------------------------------------------------------+ | | | | | | | | | | | Drupal (application) | | | | | | | | | | | +-------------------------------------------------------+ | | | | +--------------------------+ +--------------------------+ | | | | | | | | | PHP 5.4.x | | | | | | | | Mysql 5.6.x | | | | Apache 2.2.x | | | | | | | | | | | +--------------------------+ +--------------------------+ | | | | +-------------------------------------------------------+ | | | | | | | Ubuntu 12.04 | | | | | | | | (Linux) | | | | | | | +-------------------------------------------------------+ | | | +-----------------------------------------------------------+ Include a variables file, and discover pre_tasks and handlers 이 playbook에서 우리는 좀 더 playbook을 효과적으로 구성하는 것부터 시작할 것이다. command line을 통해 전달해주어야 할 필수 variable들을 정의하는 대신, 분리된 vars.yml파일에서 Ansible이 사용할 variables를 따로 관리하여 playbook을 실행하도록 하자.\n- hosts: all vars_files: - vars.yml 하나 이상의 variable file을 가지면 main playbook file을 깨끗하게 해줄 것이고 모든 configurable variable을 한곳에 정리할 수 있다. 이 때, 우리는 어떤 variable도 추가하지 않아도 된다. 우리는 vars.yml을 나중에 정의할 것이다. 지금은 빈 파일을 생성하고 playbook의 다음 section인 pre_task를 진행해보자.\npre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 Ansible은 main으로 실행할 task들이 실행되기 전과 이후에 pre_tasks와 post_tasks로 특정한 task를 실행할 수 있다. 이 경우에 우리는 playbook의 나머지가 실행되기 전에 apt cache가 업데이트 되어 우리의 서버에서 최신 버전의 패키지가 깔려있는지 확인을 하고 싶다. 우리는 Ansible의 apt module을 사용하여 지난 update 이후 3600초(1시간)보다 오래 경과했을 경우 cache를 업데이트 하라고 지시한다.\n그런식으로 한 다음에 우리는 handlers:라는 새로운 section을 추가할 것이다.\nhandlers: - name: restart apache service: name=apache2 state=restarted handlers는 그 그룹의 어느 task에서든지 notify 옵션을 추가하여 task 그룹 가장 마지막에 실행되는 특별한 종류의 task이다. handler는 task들 중 하나가 handler에게 server에 변경사항을 만들 것(그리고 실패하지 않았음)이란걸 알릴 때만 호출되고 task 그룹의 가장 마지막에만 알림을 받는다.\n이 hanlder를 호출하려면 notify: restart apache 옵션을 나머지 play에서 정의하면 된다. 우리는 이 handler를 정의하여 아래에 설명하듯 apache2 service를 configuration이 변경되고 나서 재시작할 수 있도록 한다.\nvariables처럼 handler와 task는 별도의 파일에 존재할 수 있고 playbook 안에서 이를 자그마하게 할 수 있다. (이에 대해서는 챕터 6에서 다루어 볼 것이다.) 하지만 간결하게 하기 위해 이 챕터에서의 예시들은 하나의 playbook file에 모두 보여진다. 우리는 다른 playbook organization 방법에 대해 나중에 토론해 볼 것이다.\n Basic LAMP server setup LAMP stack에 의존성을 가지는 application server를 구축하는 첫 단계는 실제 LAMP 쪽을 빌드하는 것이다. 이는 간단한 프로세스이지만 개별 서버에서 추가적인 약간의 작업이 필요하다. Apache, MySQL, PHP를 설치하고 싶지만 우리는 또한 다른 dependency도 필요하고 또한 extra apt repository에서만 사용할 수 있는 PHP의 특정 버전(5.5) 버전이 필요하다.\ntasks: - name: Get Software for apt repository management. apt: name={{ item }} state=installed with_items: - python-apt - python-pycurl - name: add ondrej repository for later versions of PHP. apt_repository: repo=\u0026#39;ppa:ondrej/php5\u0026#39; update_cache=yes - name: \u0026#34;Install Apache, MySQL, PHP, and other dependencies.\u0026#34; apt: name={{ item }} state=installed with_items: - git - curl - sendmail - apache2 - php5 - php5-common - php5-mysql - php5-cli - php5-curl - php5-gd - php5-dev - php5-mcrypt - php-apc - php-pear - python-mysqldb - mysql-server - name: Disable the firewall (since this is for local dev only). service: name=ufw state=stopped - name: \u0026#34;Start Apache, MySQL, and PHP.\u0026#34; service: \u0026#34;name={{ item }} state=started enabled=yes\u0026#34; with_items: - apache2 - mysql 이 playbook에서 우리는 각각 이름지어진 play에 간단한 prefix를 추가하기로 경정하였고 따라서 playbook의 progress가 실행될 때 더 쉽게 따라갈 수 있게 되었다. 일반적인 LAMP setup으로 시작할 것이다.\n 두개의 helper library를 설치할 것이다. 이것들은 python이 apt을 더 정확하게 관리할 수 있도록 한다. (python-apt, python-pycurl은 apt-repository module이 동작하는 데 필요하다) Ubuntu 12.04의 default apt repository가 PHP 5.4.x(또는 이후 버전)를 포함하지 않기 때문에 PHP 5.4.25(이글을 작성하는 시점에서)과 나머지 PHP package들을 포함하는 ondrej의 PHP5-oldstable repository를 설치한다. 우리의 LAMP server에 필요한 모든 package를 설치한다(Drupal을 실행하기 위한 php5 extension들도 설치한다). 테스트 목적을 위해 firewall을 모두 disable한다. production server나 어떤 server가 인터넷에 노출되어 있으면 22, 80, 443 또는 필요한 port만 허용하는 엄격한 firewall을 사용해야 한다. 모든 필요한 service를 시작하고 system boot 시 enable되도록 해야한다.  Configure Apache 다음 단계는 Apache가 Drupal과 잘 동작할 수 있도록 설정하는 것이다. 기본적으로 Ubuntu 12.04의 Apache는 mod_rewrite enabled가 되어있지 않다. 이를 해결하기 위해 우리는 sudo a2enmod rewrite 명령어를 입력해야 하지만 Ansible은 apache2_module module로 이를 간단하게 할 수 있다.\n추가적으로 VirtualHost entry를 추가하여 Apache에게 사이트 문서의 root가 어딘지 알려주고 사이트의 다른 옵션들을 알려주어야 한다.\n- name: Enable Apache rewrite module (required for Drupal). apache2_module: name=rewrite state=present notify: restart apache - name: Add Apache virtualhost for Drupal 8 development template: src: \u0026#34;templates/drupal.dev.conf.j2\u0026#34; dest: \u0026#34;/etc/apache2/sites-available/{{ domain }}.dev.conf\u0026#34; owner: root group: root mode: 0644 notify: restart apache - name: Symlink Drupal virtualhost to sites-enabled. file: src: \u0026#34;/etc/apache2/sites-available/{{ domain }}.dev.conf\u0026#34; dest: \u0026#34;/etc/apache2/sites-enabled/{{ domain }}.dev.conf\u0026#34; state: link notify: restart apache - name: Remove default virtualhost file. file: path: \u0026#34;/etc/apache2/sites-enabled/000-default\u0026#34; state: absent notify: restart apache 첫번째 명령어는 모든 필요한 Apache module들을 /etc/apache2/mods-available에서 /etc/apache2/mods-enabled로 symlink를 걸어 enable하는 것이다.\n두번째 명령어는 우리가 templates 폴더 안에서 정의한 Jinja2 template을 Apache의 sites-available 폴더로 올바른 owner와 permission을 가지고 복사하는 것이다. 추가적으로 새로운 VirtualHost로 복사하는 것은 Apache가 변경사항을 가져오기 위해 재시작되어야 한다는 뜻이므로 restart apache handler에게 notify를 한다.\nJinja2 template(filename 마지막에 .j2라고 적혀있다)인 drupal.dev.conf.j2를 확인해보자.\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin webmaster@localhost ServerName {{ domain }}.dev ServerAlias www.{{ domain }}.dev DocumentRoot {{ drupal_core_path }} \u0026lt;Directory \u0026quot;{{ drupal_core_path }}\u0026quot;\u0026gt; Options FollowSymLinks Indexes AllowOverride All \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; 이는 Apache VirtualHost definition의 거의 표준 형식이지만 우리는 거기에 Jinja2 template 변수들을 섞어넣었다. Jinja2 template에서의 변수를 프린트하는 문법은 Ansible playbook의 문법과 동일하다. - 두개의 bracket({)으로 변수의 이름을 감싼다. ({{ varialbe }})\n우리는 세가지 variable(drupal_core_version, drupal_core_path, domain)이 필요하여 이들을 전에 생성했던 vars.yml 파일에 추가해 주었다.\n# The core version you want to use (e.g. 6.x, 7.x, 8.0.x). drupal_core_version: \u0026#34;8.0.x\u0026#34; # The path where Drupal will be downloaded and installed. drupal_core_path: \u0026#34;/var/www/drupal-{{ drupal_core_version }}-dev\u0026#34; # The resulting domain will be [domain].dev (with .dev appended). domain: \u0026#34;drupaltest\u0026#34; 이제 Ansible이 이 template을 위치시키게 될 play에 도달했을 때 Jinja2 template은 variable name을 value 8.0.x와 drupaltest(또는 원하는 어떤 value)로 치환할 것이다.\n마지막 두 task는(line 12-19) 방금 생성한 VirtualHost를 enable하고 더이상 사용하지 않는 default VirtualHost definition을 삭제할 것이다.\n여기서부터 우리는 서버를 시작할 수 있지만 Apache는 우리가 정의한 VirtualHost가 아직 존재하지 않기 때문에({{ drupal_core_path }}에 디렉토리가 아직 없다) 에러를 던질 것이다. 이는 notify를 사용하는 것이 중요한 이유이다. - 이 세 step이 끝나고 Apache를 재시작하기 위해 play를 추가하는 대신에(처음 playbook을 실행할 때는 에러가 발생할 것이다) notify는 task의 main group의 다른 모든 단계가 끝날때까지 기다린 뒤(server 세팅이 끝나기까지 시간을 준다) Apache를 재시작한다.\nConfigure PHP with lineinfile file management와 ad-hoc task execution에 관해 설명할 때 우리는 book에서의 lineinfile에 대해 짧게 언급했었다. PHP의 configuration을 수정하는 것은 lineinfile의 simplicity와 usefulness를 설명하기에 매우 좋은 방법이다.\n- name: Enable upload progress via APC. lineinfile: dest: \u0026#34;/etc/php5/apache2/conf.d/20-apcu.ini\u0026#34; regexp: \u0026#34;^apc.rfc1867\u0026#34; line: \u0026#34;apc.rfc1867 = 1\u0026#34; state: present notify: restart apache Ansible의 lineinfile module은 간단한 task를 한다. 특정 text line이 파일에서 존재하는지(또는 존재하지 않는지) 확인한다.\n이 예시에서 우리는 APC의 rfc1867 option을 enable하여 Drupal이 APC의 파일 업로드 progress tracking을 사용하고 싶다. (이걸 할수 있는 더 좋은 방법들이 있지만 우리의 간단한 서버를 위해선 이걸로도 충분하다)\n먼저 dest parameter를 통해 lineinfile에게 파일의 위치를 알려주어야 한다. 그 다음 regular expression(Python-style)로 line이 어떻게 되어야 하는지를 정의한다. (이 경우 line은 apc.rfc1867로 시작해야 한다. - 우리는 period를 escape해야 하는데 이는 regular expression에서 special character이기 때문이다) 그 다음 lineinfile에게 정확히 어떻게 resulting line이 되어야 하는지를 알려준다. 마지막으로 이 line이 present해야 한다고 명시적으로 언급한다. (state parameter를 통해)\nAnsible은 regular expression을 가지고 매칭되는 line이 있는지 본다. 있다면 Ansible은 line이 line parameter와 매칭되는지 확인한다. 매칭되지 않는다면 Ansible은 line parameter에 정의된 line을 추가한다. Ansible은 추가하거나 line을 match line으로 변경해야하는 상황에서만 변경점을 report한다.\nConfigure MySQL 다음 단계는 MySQL의 default test database를 삭제하고 Drupal installation에 사용할 (이전에 정의한 domain의 이름을 가진)database를 생성하는 것이다.\n- name: Remove the MySQL test database. mysql_db: db=test state=absent - name: Create a database for Drupal. mysql_db: \u0026#34;db={{ domain }} state=present\u0026#34; MySQL은 default로 test라 이름지어진 database를 설치하고 MySQL에 포함된 mysql_secure_installation의 일부 데이터베이스를 삭제하는 것을 권장한다. MySQL을 configure하는 첫 단계는 이 database를 삭제하는 것이다. 그 다음 우리는 database를 {{ domain }}이란 이름으로 생성할 것이다 - database는 우리가 Drupal site에 사용할 domain가 동일하다.\nAnsible은 많은 데이터베이스를 out-of-the-box로 제공하여 기본으로 이용할 수 있다. (이 글을 작성하는 시점에는 MongoDB, MySQL, PostgreSQL, Redis, Riak가 있다) MySQL의 경우 Ansible은 MySQLdb Python package(python-mysqldb)를 사용하여 database server의 connection을 관리하고 default root 계정 credentials를 사용한다고 가정한다. (root가 username이고 passowrd는 없다) 이런 default를 그대로 두는 것은 명백히 잘못된 생각이다. production server에서 첫번째 단계중 하나는 root account의 password를 바꾸고 root account를 localhost로만 제한하고 불필요한 user를 제거하는 것이어야 한다.\n만약 다른 credentials를 사용한다면 Ansible이 MySQL에 접속할 때 Ansible playbook이나 variable files에 password를 기록하는 방법이 아닌 접속시 사용할 .my.cnf 파일을 remote user의 home directory에 추가할 수 있다. 아니면 Ansible playbook을 실행할 때 MySQL의 username과 password를 입력하게 할 수 있다. prompts를 사용하는 이 옵션은 이 책의 뒷부분에서 다룰 것이다.\n Install Composer and Drush Drupal은 Drush의 형태로 commandl-line을 가지고 있다. Drush는 Drupal과는 별개로 개발되고 있고 Drupal을 관리할 때 사용할 수 있는 CLI 명령어를 모두 제공한다. Drush는 대부분의 현대 PHP 툴처럼 Composer의 dependency들을 설명해주는 파일인 composer.json 파일로 정의된 external dependency들이 있다.\n여기서 우리는 단순히 Drupal을 다운로드하고 브라우저상으로 몇몇 setup을 수행하지만, playbook의 목적은 fully-automated가 되는 것이고, Drupal installation의 idempotent 속성을 부여하는 것이다. 따라서 우리는 Composer를 설치하고나서 Drush를 설치할 것이다.\n- name: Install Composer into the current directory. shell: \u0026gt; curl -sS https://getcomposer.org/installer | php creates=/usr/local/bin/composer - name: Move Composer into globally-accessible location. shell: \u0026gt; mv composer.phar /usr/local/bin/composer creates=/usr/local/bin/composer 첫번째 명령어는 composer.phar PHP application archive를 생성하는 Composer의 php-based installer를 실행시킨다. 이 archive는 (shell command에서는 mv를 이용하여) /usr/local/bin/compsoer로 복사되고 이를 통해 간단히 composer 명령어만 사용하여 Drush의 dependency들을 설치할 수 있다. 각 명령어는 /usr/local/bin/composer 파일이 존재하지 않을 때에만 동작하도록 설정되어있다(creates parameter를 통해).\n왜 command 대신에 shell을 사용하는가? Ansible의 command module은 host에서 명령을 실행할 때 많이 사용하는 option이다(Ansible의 module이 충분하지 않을 때). 그리고 이는 대부분의 시나리오에서 작동한다. 하지만 command는 remote shell /bin/sh를 통해 command를 실행할 수 없고 따라서 \u0026lt;, \u0026gt;, |, \u0026amp;같은 옵션과 $HOME같은 local environment variables가 작동하지 않는다. shell은 command의 ouput을 다른 command로 pipe를 연결해주고 local environment에 접속할 수도 있다.\nremote로 shell command를 실행시키는 것을 도와주는 module은 두가지가 더 있다. script는 shell scripts(하지만 shell script를 idempotent한 Ansible playbook로 바꾸는 것이 무조건 좋다)를 실행하고, raw는 raw commands를 SSH(다른 옵션을 사용하지 못할때만 사용해야한다)를 통해 실행한다.\nAnsible module을 모든 task에 사용하는 것이 가장 좋다. 만약 regular command line command를 사용해야만 한다면 command module을 먼저 시도해 보아라. 위에서 언급한 option들을 필요로 한다면 shell을 사용해라. script나 raw는 최대한 사용하지 말아야 하고 이 책에서는 다루지 않을 것이다.\n 이제 GitHub를 사용하여 가장 최근 버전의 Drush를 install할 것이다.\n- name: Check out drush master branch git: repo: https://github.com/drush-ops/drush.git dest: /opt/drush - name: Install Drush dependencies with Composer. shell: \u0026gt; /usr/local/bin/composer install chdir=/opt/drush creates=/opt/drush/vendor/autoload.php - name: Create drush bin symlink. file: src: /opt/drush/drush dest: /usr/local/bin/drush state: link 이 책의 앞부분에서 우리는 ad-hoc command를 통해 git repository를 clone하였다. 이번에는 git module을 사용하는 play를 정의하고 이를 통해 GitHub의 repository URL을 가지고 Drush를 clone할 것이다. 우리는 master branch를 사용할 것이기 때문에 repo(repository URL)과 dest(destination path) parameter를 넣어주어야 한다.\ndrush가 /opt/drush로 다운로드되고 나면, Composer를 사용하여 모든 필요한 dependency들을 설치한다. 이 경우 우리는 Ansible이 composer install를 /opt/drush 폴더에서 실행하길 원하고(이는 Composer가 자동으로 drush의 composer.json 파일을 검색하여 그렇게 된다) 따라서 우리는 parameter로 chdir=/opt/drush를 전달해주어야 한다. Composer가 끝나고 나면 /opt/drush/vendor/autoload.php는 생성될 것이고 creates 파라미터를 사용하여 Ansible에게 파일이 이미 존재할 경우 이 단계를 건너뛰게 할 것이다(idempotency를 위해).\n최종적으로 우리는 /usr/local/bin/drush에서 /opt/drush/drush로 symlink를 걸어주어 drush command가 시스템의 어느곳에서든지 실행할 수 있도록 하였다.\nInstall Drupal with Git and Drush 우리는 다시 git을 사용하여 Drupal을 이전에 virtualhost configuration에서 정의한 대로 apache document root로 clone할 것이다. 그러고나서 Drupal의 installation을 drush를 통해 하고 다른 파일 permission issue들을 수정하여 Drupal이 VM에서 제대로 load되도록 설정할 것이다.\n- name: Check out Drupal Core to the Apache docroot. git: repo: http://git.drupal.org/project/drupal.git version: \u0026#34;{{ drupal_core_version }}\u0026#34; dest: \u0026#34;{{ drupal_core_path }}\u0026#34; - name: Install Drupal. command: \u0026gt; drush si -y --site-name=\u0026#34;{{ drupal_site_name }}\u0026#34; --acount-name=admin --account-pass=admin --db-url=mysql://root@localhost/{{ domain }} chdir={{ drupal_core_path }} creates={{ drupal_core_path }}/sites/default/settings.php notify: restart apache # SEE: https://drupal.org/node/2121849#comment-8413637 - name: Set permissions properly on settings.php. file: path: \u0026#34;{{ drupal_core_path }}/sites/default/settings.php\u0026#34; mode: 0744 - name: Set permissions on files directory. file: path: \u0026#34;{{ drupal_core_path }}/sites/default/files\u0026#34; mode: 0777 state: directory recurse: yes 먼저 우리는 vars.yml파일 안에 정의된 drupal_core_version값으로 version을 지정하여 Drupal의 git repository를 clone할 것이다. git module의 version parameter는 branch(master, 8.0.x 등)와 tag(1.0.1, 7.24 등) 또는 개별 commit hash(50a1877 등)를 지정하여 clone할 수 있다.\n그 다음 Drush의 si 명령어(site-install의 줄임말)는 사용하여 Drupal의 installation(database를 configure하고, 몇가지 maintenance를 실행하고, site를 위한 몇몇 default configuration setting을 설정한다)을 실행할 것이다. drupal_core_version과 domain같은 몇가지 variable들을 전달한다. 또한 drupal_site_name을 추가하여 varaible을 vars.yml 파일에 추가한다.\n# Your Drupal site name. drupal_site_name: \u0026#34;D8 Test\u0026#34; 또한, Drupal의 installation process는 결과적으로 settings.php 파일을 생성한다. 따라서 우리는 해당 파일의 location을 creates parameter를 통해 Ansible이 site가 이미 설치되었는지를 판단하도록 할 수 있다(따라서 실수로 재설치하지 않는다). site가 install되고 나면, Apache도 재시작할 것이다. (Apache의 configuration을 업데이트할 때 사용했던 것처럼 notify를 다시 사용할 것이다.)\n마지막 두 task는 Drupal의 settings.php와 폴더들의 permission을 각각 744와 777로 설정하는 것이다.\nDrupal LAMP server summary 이제 http://drupaltest.dev/로 서버에 접속하게 되면(drupaltest.dev가 VM의 IP주소를 가르킨다고 가정한다) Drupal의 default home page를 볼 수 있고 admin/admin으로 접속이 가능하다. (production server에서는 명백히 안전한 password를 설정해야 한다)\nApache, MySQL, PHP를 실행하는 비슷한 server configuration으로 Drupal뿐 아니라 Symfony, Wordpress, Joomla, Laravel 등과 같은 다른 web frameworks와 CMS들을 실행할 수 있다.\n전체 Drupal LAMP server playbook의 예시는 책의 code repository인 https://github.com/geerlingguy/ansible-for-devops의 drupal directory에서 확인할 수 있다.\n Real-world playbook: Ubuntu Apache Tomcat server with Solr Apache Solr는 full-text search, word highlighting, faceted search, fast indexing 등에 optimize된 빠르고 scalable한 search server이다. 매우 유명한 search server로 Ansible을 통해 설치하고 설정하기가 꽤 쉽다. 다음의 example에서 우리는 Apache Solr를 Ubuntu 12.04와 Apache Tomcat을 통해 설정할 것이다.\n+-----------------------------------+ | | | Apache Solr Server/VM | | | +-----------------------------------+ | | | +-------------------------------+ | | | | | | | Apache Solr 4.x | | | | | | | +-------------------------------+ | | | | +-------------------------------+ | | | | | | | Apache Tomcat 7 | | | | | | | +-------------------------------+ | | | | +-------------------------------+ | | | | | | | Ubuntu 12.04 (Linux) | | | | | | | +-------------------------------+ | | | +-----------------------------------+ Apache Solr Server.\nInclude a variables file, and discover pre_tasks and handlers 이전의 LAMP server 예시처럼 우리는 분리된 vars.yml 파일에 있는 variable들을 Ansible에게 알려주는 것으로부터 playbook을 시작한다.\n- hosts: all vars_files: - vars.yml vars.yaml에 대해 생각해보는 동안 빠르게 vars.yaml 파일을 생성하자. Solr playbook과 동일한 폴더에 파일을 생성하고 다음의 내용을 추가하자.\ndownload_dir: /tmp solr_dir: /opt/solr 이 두 변수는 Apache Solr를 다운로드하고 설치하는 동안 사용할 path를 정의한 것이다.\nvars_files를 마치고 playbook으로 돌아와서 우리는 pre_tasks를 사용하여 apt cache가 업데이트되도록 할 것이다.\npre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 Drupal playbook처럼 우리는 다시 handlers를 사용하여 tasks section에서 notify를 받는 특정한 tasks를 정의할 것이다. 이번에는 handler를 사용하여 Apache Solr에 영향을 주는 tomcat7과 Java servlet container를 재시작할 것이다.\nhandlers: - name: restart tomcat service: name=tomcat7 state=restarted 우리는 playbook 안에서 handler를 notify: restart tomcat 옵션을 통해 호출할 것이다.\nInstall Apache Tomcat 7 Ubunntu 특정 서버에서 Tomcat7을 설치하는 것은 쉽다. apt repository에 package가 있기 때문에 우리는 이것이 설치 되었는지, tomcat7 service가 enabled 되었고 start 되었는지만 확인하면 된다.\ntasks: - name: Install Tomcat 7. apt: \u0026#34;name={{ item }} state=installed\u0026#34; with_items: - tomcat7 - tomcat7-admin - - name: Ensure Tomcat 7 is started and enabled on boot. service: name=tomcat7 state=started enabled=yes 엄청 쉽다. 우리는 apt module을 이용하여 tomcat7과 tomcat7-admin 두 package를 설치하였다(그래서 우리는 Tomcat의 administrative backend에 로그인할 수 있다). 그리고 tomcat7을 시작하고 system boot할 때 start 되도록 설정한다.\nInstall Apache Solr Ubuntu 12.04는 Apache Solr에 대한 package를 포함한다. 하지만 매우 오래된 버전을 설치하기 때문에 우리는 source로부터 Solr의 최신 버전을 설치할 것이다. 첫번째 단계는 source를 다운로드하는 것이다.\n- name: Download Solr. get_url: url: http://apache.osuosl.org/lucene/solr/4.9.1/solr-4.9.1.tgz dest: \u0026#34;{{ download_dir }}/solr-4.9.1.tgz\u0026#34; sha256sum: 4a546369a31d34b15bc4b99188984716bf4c0c158c0e337f3c1f98088aec70ee 우리는 가장 최근의 stable 버전인 Apache Solr 4.9.1을 설치한다. remote server에서 파일을 다운로드 했을 때 get_url module은 raw wget이나 curl 명령어보다 더 유연함과 편리함을 제공한다.\nget_url에 url(파일 소스를 다운로드할 주소)과 dest(다운로드된 파일이 위치할 곳)를 전달해 주어야 한다. dest parameter로 디렉토리를 넘기면 Ansible은 파일을 안에다가 위치시키지만 나중에 playbook이 실행될 때마다 다시 다운로드할 것이다(만약 변경사항이 있다면 기존의 것은 overwrite된다). 이런 overhead를 피하기 위해 우리는 다운로드할 파일의 full path를 입력한다.\n우리는 안정성을 위해 optional parameter인 또한 sha256sum을 사용한다. 파일이나 archive를 다운로드 하면 application의 functionality와 security의 취약점이 되기 때문에 file이 우리가 생각하는 것과 동일한지 확인하는 것은 좋은 생각이다. sha256sum은 다운로드된 파일에서 data의 hash와 지정한 256-bit hash(shasum -a 256 /path/to/file을 통해 파일의 sha256sum을 얻을 수 있다)를 비교한다. checksum이 제공한 hash와 일치하지 않으면 Ansible은 fail될 것이고 새로워진(그리고 유효하지 않는) 다운로드 파일을 버릴 것이다.\n- name: Expand Solr. command: \u0026gt; tar -C /tmp -xvzf {{ download_dir }}/solr-4.9.1.tgz creates={{ download_dir }}/solr-4.9.1/dist/solr-4.9.1.war - name: Copy Solr into place. command: \u0026gt; cp -r {{ download_dir }}/solr-4.9.1 {{ solr_dir }} creates={{ solr_dir }}/dist/solr-4.9.1.war 우리는 Apache Solr archive를 압축해제하고 위치로 복사해야 한다. 이 두 단계를 위해 내장된 tar과 cp utility를 (적절한 옵션을 가지고) 사용한다. creates를 설정하는 것은 Ansible이 나중에 다시 실행되었을 때 Solr war file이 이미 존재하기 때문에 이 단계를 건너 뛰게 한다.\n# Use shell so commands are passed in correctly. - name: Copy Solr components into place. shell: \u0026gt; cp -r {{ item.src }} {{ item.dest }} creates={{ item.creates }} with_items: # Solr example configuration and war file. - { src: \u0026#34;{{ solr_dir }}/example/webapps/solr.war\u0026#34;, dest: \u0026#34;{{ solr_dir }}/solr.war\u0026#34;, creates: \u0026#34;{{ solr_dir }}/solr.war\u0026#34; } - { src: \u0026#34;{{ solr_dir }}/example/solr/*\u0026#34;, dest: \u0026#34;{{ solr_dir }}/\u0026#34;, creates: \u0026#34;{{ solr_dir }}/solr.xml\u0026#34; } # Solr log4j logging configuration - { src: \u0026#34;{{ solr_dir }}/example/lib/ext/*\u0026#34;, dest: \u0026#34;/var/lib/tomcat7/shared/\u0026#34;, creates: \u0026#34;/var/lib/tomcat7/shared/log4j-1.2.16.jar\u0026#34; } - { src: \u0026#34;{{ solr_dir }}/example/resources/log4j.properties\u0026#34;, dest: \u0026#34;/var/lib/tomcat7/shared/classes\u0026#34;, creates: \u0026#34;/var/lib/tomcat7/shared/classes/log4j.properties\u0026#34; } notify: restart tomcat 다음 task는 Apache Solr를 실행하는데 필요한 특정 디렉토리와 파일을 복사하는 단계이다.\n여기서는 특별한게 없지만 이 예시는 with_items lists안에서 comment를 사용하여 list안의 item들을 명시해주는 것을 보여준다. 우리는 각 command들을 각각 따로 task로 만들 수 있지만 이렇게 한 이유는 총 Ansible task의 수를 줄여주고 with_items list를 필요시 external variable로 옮기기 위한 것이다.\n- name: Ensure solr example directory is absent. file: path: \u0026#34;{{ solr_dir }}/example\u0026#34; state: absent - name: Set up solr data directory file: path: \u0026#34;{{ solr_dir }}/data\u0026#34; state: directory owner: tomcat7 group: tomcat7 최신 버전의 Apache Solr는 {{ solr_dir }} 폴더 내부를 모두 resursive하게 검색하여 potential search configuration을 로딩한다. 우리는 서버의 default search core를 사용하기 위해 example 중 하나를 복사하였기 때문에 Solr는 그 examplㄷ과 duplicate라고 알게되고 crash가 날 것이다. 따라서 우리는 file module에 path를 사용하여 example directory가 없도록 할 것이다(state=absent).\nexample directory를 지우고 나서(다음에 실행할 때에도 없어져 있어야 한다) 우리는 Solr가 index data를 저장할 data directory가 존재하고 tomcat7 user와 group으로 owner를 설정되도록 해야한다.\n- name: Configure solrconfig.xml for new data directory. lineinfile: dest: \u0026#34;{{ solr_dir }}/collection1/conf/solrconfig.xml\u0026#34; regexp: \u0026#34;^.*\u0026lt;dataDir.+%\u0026#34; line: \u0026#34;\u0026lt;dataDir\u0026gt;${solr.data.dir:{{ solr_dir }}/data}\u0026lt;/dataDir\u0026gt;\u0026#34; state: present 앞에서 보았듯이 lineinfile은 idempotent하게 configuration file setting이 존재하는지 확인하는 데 유용한 module이다. 이 경우에 우리는 \u0026lt;dataDir\u0026gt; 줄이 우리 default search core의 configuration에 특정한 값으로 설정되어있는지 확인해야한다.\n- name: Set permissions for solr home. file: path: \u0026#34;{{ solr_dir }}\u0026#34; recurse: yes owner: tomcat7 group: tomcat7 {{ solr_dir }}의 전체 content에서 ownership option을 알맞게 설정하기 위해 우리는 file module을 recurse parameter를 yes로 설정하여 사용할 것이다. 이는 shell command에서의 chown -R tomcat7:tomcat7 {{ solr_dir }}과 동일하다.\n- name: Add Catalina configuration for solr. template: src: templates/solr.xml.j2 dest: /etc/tomcat7/Catalina/localhost/solr.xml owner: root group: tomcat7 mode: 0644 notify: restart tomcat 마지막 task는 template file (solr.xml.j2)를 remote host로 복사하고 Jinja2 문법으로 variable들을 대체한 뒤 file의 ownership과 permission을 Tomcat에 필요한 것으로 설정한다.\ntask가 실행되기 전에 local template file은 생성이 되어있어야 한다. templates 폴더를 Apache Solr playbook의 디렉토리와 같은 곳에서 생성하고 다음의 내용으로 solr.xml.j2를 그 안에 생성한다.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;Context docBase=\u0026#34;{{ solr_dir }}/solr.war\u0026#34; debug=\u0026#34;0\u0026#34; crossContext=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Environment name=\u0026#34;solr/home\u0026#34; type=\u0026#34;java.lang.String\u0026#34; value=\u0026#34;{{ solr_dir }}\u0026#34; override=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;/Context\u0026gt; playbook을 $ ansible-playbook [playbook-name.yml]로 실행하고 몇분 후에(서버의 인터넷 환경에 따라 다르다) Solr admin interface로 http://example.com:8080/solr를 통해 접속할 수 있다(example.com이 우리 서버의 hostname이나 IP 주소로 바뀌어야 한다).\nApache Solr server summary Apache Solr를 deploy할 때 사용했던 configuration은 multicore setup을 할 수 있고 따라서 우리는 admin interface를 통해 \u0026lsquo;search cores'를 추가할 수 있다(디렉토리와 core schema configuration이 filesystem에 위치시킨다). 그리고 multiple website와 application을 위한 multiple indexes를 가질 수 있다.\n위에서 보앗던 것과 유사한 playbook은 Drupal website에 대한 Apache Solr search core를 hosts하는 service인 infrastructure for Hosted Apache Solr의 일부로 사용되었다.\n전체 Apache Solr server playbook에 대한 예제는 이 책의 code repository인 https://github.com/geerlingguy/ansible-for-devops의 solr directory에서 확인 가능하다.\n Summary 이제 우리는 Ansible의 modus operandi에 친숙해지게 되었다. Playbook은 Ansible의 configuration management와 provisioning 기능의 심장이고 동일한 module과 비슷한 syntax로 ad-hoc command를 사용하여 일반 서버 management에 deployment를 할 수 있다.\n이제 playbook에 친숙해졌으니 task의 organization, condition, variable과 같은 playbook을 만드는 좀 더 깊숙한 개념들에 대해 알아볼 것이다. 나중에 우리는 role을 통한 playbook의 사용법을 배워 이를 무한정으로 유연하고 infrastructure를 configure하고 setting하는 데 드는 시간을 줄일 것이다.\n "
},
{
	"uri": "http://kimmj.github.io/jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "Jenkins  Jenkins Install     Workspace@2를 변경하기 - Workspace List 설정 변경     "
},
{
	"uri": "http://kimmj.github.io/iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Infrastructure as Code  [번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials     "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/",
	"title": "Ansible for DevOps",
	"tags": [],
	"description": "",
	"content": "Ansible Ansible For DevOps Ansible For DevOps라는 책을 번역한 것입니다.\n Chapter 1 - Getting Started With Ansible     Chapter 2 - Local Infrastructure Development: Ansible and Vagrant     Ad Hoc Commands     Ansible Playbooks     Chapter5    Ansible Playbooks Beyond the Basics     Handover     Environment Variables     Variables     if/then/when - Conditionals     Delegation Local Actions and Pauses     Prompts     Tags     Summary      Chapter6     "
},
{
	"uri": "http://kimmj.github.io/cicd/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "CICD  Deploy Strategy     "
},
{
	"uri": "http://kimmj.github.io/css/",
	"title": "CSS",
	"tags": [],
	"description": "",
	"content": "CSS  background image 어둡게 하기     Greater Than Sign     "
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/controllers-overview/",
	"title": "Controllers Overview",
	"tags": ["kubernetes", "concepts"],
	"description": "",
	"content": "Contents 이 포스트에서는 Kubernetes의 Controller들에 대해서 알아보도록 하겠습니다. 가장 작은 단위인 Container부터, 상위 개념인 Deployment, StatefulSet까지 다루어 보도록 하겠습니다.\n Containers Pods ReplicaSets Deployments StatefulSets  Monolithic vs. Microservice 우선 Monolithic과 Microservice에 대해서 짚고 넘어가도록 하겠습니다.\nMonolithic의 개념은 하나의 큰 어플리케이션을 말합니다. 여러 사람이 개발을 하고 나서 하나의 큰 패키지로 빌드하고 이를 배포하죠. 간단한 서비스라면 문제가 발생하지는 않겠지만, 점점 코드의 수가 늘어나고 거대해질 수록 문제점이 생깁니다. 예를 들면 빌드시간이 오래걸린다던지, scale-out을 하기 힘들다던지 하는 문제가 있겠네요.\n반면 Microservice는 하나의 큰 어플리케이션을 여러 조각으로 쪼갠 것을 의미합니다. 각 조각(microservice)는 자신의 역할이 있고, 이를 잘 수행하면 됩니다. 전체적인 관점에서 보면 어플리케이션은 microservice들간의 통신으로 행해진다고 보면 될 것 같습니다.\nMonolithic에 비해 Microservice는 몇가지 장점들이 있습니다.\n scale-out이 용이합니다.\nMonolithic에 비해 microservice의 단위는 작기 때문에 scale-out 하는데 시간도 오래 걸리지 않고, 간단합니다. 빠른 배포가 가능합니다.\nMonolithic에서는 사이즈가 커질수록 빌드하는 시간이 점점 길어진다고 설명했었습니다. 이는 곧 요즘처럼 트렌드라던지 상황이 급변하는 상황에서 약점이 될 수 밖에 없습니다. 반면 Microservice는 작은 조각이기 때문에 빌드하는 시간이 Monolithic에 비해 뛰어날 수밖에 없습니다. 그리고 이를 배포하는 시간도 매우 줄어들게 되죠. 문제가 발생하였을 때 영향이 적습니다.\n쉽게 우리가 가장 잘 알고있는 게임중 하나인 LoL을 가지고 설명해 보도록 하겠습니다. 만약 LoL을 플레이하고 싶은데, 상점에 에러가 있어서 이용하지 못한다면 어떻게 되나요? LoL이 Monolithic이었다면 게임 플레이가 막혀서 엄청난 원성을 샀을 것입니다. 하지만 LoL또한 Microservice이기 때문에 문제가 발생한 곳만 이용하지 못할 뿐, 나머지 서비스는 정상적으로 이용이 가능합니다. 따라서 복구하기도, 운영하기도 훨씬 쉽습니다.  이렇게 보면 무조건 Microservice만 해야하는 것처럼 보이기도 합니다. 하지만 세상일이 모두 그렇듯 여기에도 정답은 없습니다. 자신이 개발하고자 하는 어플리케이션의 특성을 잘 파악해서 한가지를 선택하고 개발하는 것이 좋은 방향이 될 것 같습니다.\nConatiners 이제 본격적인 설명으로 넘어가보도록 하겠습니다.\nDocker를 사용해 보았다면 container도 친숙한 개념이 될 것 같습니다. container는 VirtualBox처럼 가상화를 하지만, OS까지 가상화하는 것이 아니라 host OS 위에서 커널을 공유하는 방식으로 가상화를 합니다. 어려운 말일수도 있지만 간단하게 말하자면 VirtualBox같은 hypervisor에 의한 가상화보다 훨씬 가벼운 방법으로 가상화를 할 수 있다고 생각하시면 됩니다. 여기서 가볍다는 의미는 빠르게 생성/삭제할 수 있고 용량도 작다는 의미입니다.\nKubernetes는 이런 Docker와 같은 container runtime 기반으로 동작합니다. 그리고 각 container는 어플리케이션 내에서 자신의 역할을 수행하는 것들입니다. Docker를 통해 컨테이너를 동작시키는 것처럼, Kubernetes도 컨테이너를 Docker같은 container runtime의 힘을 빌려 동작시킵니다.\nPods Pod는 Kubernetes에서 어플리케이션을 관리하는 단위입니다.\n하나의 Pod는 여러개의 Container로 구성될 수 있습니다. 즉, 너무나 밀접하게 동작하고 있는 여러 Container를 하나의 Pod로 묶어 함께 관리하는 것입니다.\n이 때, 하나의 Pod 내에 존재하는 모든 Container들은 서로 localhost를 통해 통신할 수 있습니다.\nKubernetes에서 Pod는 언제 죽어도 이상하지 않은 것으로 취급됩니다. 동시에 언제 생성되어도 이상하지 않은 것, 어디에 떠있어도 이상하지 않은 것이죠. 그 만큼 어플리케이션 개발자는 Pod를 구성할 때 하나의 Pod가 장애가 나는 경우에도 어플리케이션이 제대로 동작할 수 있도록 만들어야 합니다. 언제 없어졌다가 생성될지 모르니까요.\nReplicaSet ReplicaSet은 Pod를 생성해주는 녀석입니다.\n단일 Pod를 그냥 생성해도 되지만 그렇게 할 경우 Kubernetes가 주는 여러 이점들, 예를 들어 auto healing, auto scaling, rolling update 등을 이용하지 못합니다. 때문에 Pod를 관리해주는 Controller가 필요하게 되는데 이것이 ReplicaSet입니다.\nReplicaSet은 자신이 관리해야하는 Pod의 template을 가지고 있습니다. 그리고 주기적으로 Kubernetes를 주시하며 내가 가지고 있는 template에 대한 Pod가 원하는 숫자만큼 잘 있는지 확인합니다. 부족하다면 Pod를 더 생성하고 너무 많으면 Pod를 삭제합니다.\nReplicaSet은 Pod를 label을 기준으로 관리합니다. 자신이 가지고 있는 matchLabels와 일치하는 Pod들이 자신과 관련된 Pod라고 인식하는 것이죠. 따라서 label을 운영중에 바꾸는 일은 웬만해선 피해야 합니다. 고아가 발생하여 어느 누구도 관리해주지 않는 Pod가 남게 될 수 있기 때문입니다.\n또한 ReplicaSet은 자신이 생성한 파드들을 \u0026lt;ReplicaSet의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;으로 생성합니다. 따라서 Pod의 이름만 보고도 어떤 ReplicaSet이 생성했는지 알 수 있게되죠.\nDeployments 위에서는 ReplicaSet에 대해서 이야기해 보았습니다. Deployments는 ReplicaSet보다 상위 개념의 Controller입니다.\n만약 Pod의 template을 수정하는 경우가 생긴다면 어떻게 해야 할까요? 예를 들어 cpu 할당량을 바꾼다던지, image를 다른 이미지로 변경한다던지 하는 경우가 발생한다면요. ReplicaSet만 존재한다면 이런 상황에서 새로운 template을 가지고 ReplicaSet을 생성하고, 기존의 ReplicaSet을 삭제하거나 replicas: 0으로 변경하여 ReplicaSet만 남아있고 실제 Pod는 없도록 해야합니다.\n그렇다면 새로 만든 template에 오류가 있어서 이전 버전으로 돌아가고 싶다면 어떻게 해야할까요? 이전에 작성했던 ReplicaSet의 replicas를 늘리고, 현재 올라가있던 ReplicaSet의 replicas를 줄이면 될 것 입니다. 하지만 이는 일일이 기억해야하는 크나큰 단점이 있겠죠.\n때문에 ReplicaSet을 관리해주는 Deployment가 필요합니다. Deployment는 ReplicaSet을 생성하고 이 ReplicaSet이 Pods를 생성하도록 만듭니다. 그리고 자신이 관리하는 ReplicaSet의 labels를 자신의 matchLabels로 일치시켜 구분합니다.\n그렇다면 Pod의 template이 변경되는 상황엔 이번에는 어떻게 적용이 될까요?\nDeployment는 새로운 ReplicaSet을 생성하고, 기존 ReplicaSet의 replicas를 0으로 줄입니다. 그러면서 자신이 생성했던 ReplicaSet들을 revision으로 관리하죠. 이렇게 되면 사용자는 kubectl 명령어만 가지고 이전 template을 가진 Pod로 변경할 수 있습니다.\n또한 Deployments가 ReplicaSet을 생성할 때는 \u0026lt;Deployment의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;으로 생성합니다. 위에서 ReplicaSet도 Pod를 생성할 때 \u0026lt;ReplicaSet의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;이라고 설명했었습니다. 따라서 Deployments에 의해 만들어진 Pod들은 \u0026lt;Deployment의 이름\u0026gt;-\u0026lt;ReplicaSet에 대한 hash 값\u0026gt;-\u0026lt;Pod에 대한 hash 값\u0026gt;과 같은 형태를 띄게 됩니다.\nStatefulSets StatefulSet은 좀 특이한 녀석입니다.\nPod는 어디에 떠있어도 이상하지 않은 것이라고 언급했었습니다. 그런데 StatefulSet은 그렇지 않습니다. 이름에 걸맞게 이전의 상태를 그대로 보존하고 있어야 합니다. 따라서 여러 제약사항들이 생기기도 합니다.\n여기에서는 Pod와의 관계만 알아보고 넘어가도록 하겠습니다.\nStatefulSets는 Deployments와는 다르게 ReplicaSet을 생성하지 않습니다. 대신 자신이 직접 Pod를 생성합니다. 이 때 Pod의 label 속성을 자신의 matchLabels과 일치시켜 자신이 관리하고 있는 Pod를 구분합니다.\n또한 이름도 hash값을 사용하지 않고 0부터 시작하는 숫자를 사용합니다. 즉, \u0026lt;StatefulSets의 이름\u0026gt;-\u0026lt;0부터 오름차순\u0026gt;의 이름을 가지는 Pod를 생성합니다.\n만약 생성해야하는 Pod의 template에 변화가 있다면 어떻게 해야할까요?\nStatefulSets는 Deployments와 다르게 변경할 수 있는 부분에 제약이 있습니다. 이 경우 절대 StatefulSet을 update할 수 없습니다. 따라서 기존 StatefulSet을 삭제하고 다시 kubectl apply와 같은 명령어로 새로운 template을 가지고 생성해야합니다.\nRollback에도 제약사항이 있습니다.\nRollback시 StatefulSet은 Pod가 완전히 Ready상태가 되길 기다립니다. 그런데 만약 StatefulSet이 생성한 Pod가 ImagePullbackOff같은 에러에 빠지게 된다면, 또는 영원히 rediness probe에 의해 Ready 상태가 되지 않는다면 StatefulSet의 rollback은 멈춰버립니다. 이는 Known Issue로 수동으로 해당 Pod를 삭제하는 방법밖에 없습니다.\n따라서 웬만하면 설계를 할 때 StatefulSet을 지양하는 것이 Kubernetes의 Design에 더욱 맞는 방향일 것입니다.\n마치며.. 이렇게 Container, Pods, ReplicaSets, Deployments, StatefulSets의 관계를 중심으로 알아보는 시간을 가졌습니다. 혹시나 잘못된 부분이 있다면 언제든지 댓글 또는 메일로 알려주시면 감사하겠습니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/",
	"title": "Concepts",
	"tags": [],
	"description": "",
	"content": "Kubernetes Concepts  Controllers Overview     Pods     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "Ubuntu Tools  Tmux     "
},
{
	"uri": "http://kimmj.github.io/prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "Prometheus  Install Prometheus     Federation     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/canaryanalysis/",
	"title": "CanaryAnalysis",
	"tags": [],
	"description": "",
	"content": "Spinnaker Canary Analysis Canary Analysis는 Spinnaker에서 자동으로 분석을 통해 새로운 버전에 문제가 없는지 확인해주는 pipeline입니다.\n Canary Analysis     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-a-storage-service/",
	"title": "Choose a Storage Service",
	"tags": ["install", "spinnaker", "minio"],
	"description": "",
	"content": "Spinnaker들의 데이터를 저장할 공간입니다.\n여러가지 옵션들이 있지만, 저는 local로 운용할 수 있는 minio를 통해 데이터를 저장해 볼 것입니다.\nminio를 docker-compose를 통해 쉽게 배포하도록 할 것입니다. 먼저, docker-compose를 설치합니다.\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 그 뒤 minio의 docker-compose.yaml을 만듭니다.\nversion: '3.7' services: minio: image: minio/minio:RELEASE.2020-01-16T22-40-29Z volumes: - ./data:/data ports: - \u0026quot;9000:9000\u0026quot; environment: MINIO_ACCESS_KEY: minio MINIO_SECRET_KEY: minio123 command: server /data healthcheck: test: [\u0026quot;CMD\u0026quot;, \u0026quot;curl\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;http://localhost:9000/minio/health/live\u0026quot;] interval: 30s timeout: 20s retries: 3 docker-compose를 통해서 deamon으로 실행합니다.\ndocker-compose up -d 이제 halyard와 연동을 하도록 합니다.\n먼저, ~/.hal/default/profiles/front50-local.yml 파일을 다음과 같이 생성합니다.\nspinnaker: s3: versioning: false 그 다음 다음의 명령어로 연동을 합니다.\nENDPOINT=http://10.0.2.4:9000 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 echo $MINIO_SECRET_KEY | hal config storage s3 edit --endpoint $ENDPOINT \\  --access-key-id $MINIO_ACCESS_KEY \\  --secret-access-key hal config storage edit --type s3 "
},
{
	"uri": "http://kimmj.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes Kubernetes는 deploy, scaling, 그리고 컨테이너화된 애플리케이션의\nmanagement를 자동화 해주는 open source container orchestration engine입니다.\n Install    Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm      CKA Study    10 Kubernetes the Hard Way     09 Networking     08 Storage     07 Security     06 Cluster Maintenance     05 Application Lifecycle Management     04 Logging and Monitoring     03 Scheduling     02 Core Concepts      Concepts    Controllers Overview     Pods      [번역] 쿠버네티스에서의 Port, TargetPort, NodePort     Stern을 이용하여 여러 pod의 log를 한번에 확인하기     "
},
{
	"uri": "http://kimmj.github.io/hugo/ibiza/",
	"title": "Ibiza",
	"tags": [],
	"description": "",
	"content": "Hugo Ibiza Ibiza는 이 블로그를 만드는 프로젝트입니다.\n Font Change     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Ubuntu Network  Netplan으로 static IP 할당받기     "
},
{
	"uri": "http://kimmj.github.io/hugo/",
	"title": "Hugo",
	"tags": [],
	"description": "",
	"content": "Hugo fast static website engine\n Ibiza    Font Change      Hugo에 Google Analytics 적용하기     Hugo에 Comment 추가하기 (Utterance)     HUGO로 HTML이 되지 않을 때 가능하게 하는 방법     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/",
	"title": "Spinnaker",
	"tags": [],
	"description": "",
	"content": "CI/CD Spinnaker Spinnaker는 Kubernetes 환경에서 배포 자동화를 위해 만들어진 툴입니다.\n배포하려는 클러스터가 GKE인지, EKS인지, On-Premise 환경인지 상관없이 하나의 툴로 배포하기 위해 만들어졌습니다.\n이 툴 자체가 MSA 구조로 만들어져있습니다.\n Installation    Overview     Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment      CanaryAnalysis    Canary Analysis      Tips    Pipeline Expressions      "
},
{
	"uri": "http://kimmj.github.io/ansible/",
	"title": "Ansible",
	"tags": [],
	"description": "",
	"content": "Ansible  Ansible for DevOps    Chapter 1 - Getting Started With Ansible     Chapter 2 - Local Infrastructure Development: Ansible and Vagrant     Ad Hoc Commands     Ansible Playbooks     Chapter5    Ansible Playbooks Beyond the Basics     Handover     Environment Variables     Variables     if/then/when - Conditionals     Delegation Local Actions and Pauses     Prompts     Tags     Summary      Chapter6      Create Vm With Ansible Libvirt     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/",
	"title": "Ubuntu",
	"tags": [],
	"description": "",
	"content": "Ubuntu Ubuntu에서 배운 것들을 기록하는 공간입니다.\n Tools    Tmux      Network    Netplan으로 static IP 할당받기      Ubuntu의 Login Message 수정하기     reboot 후에 tmux를 실행시켜 원하는 작업을 하기     oh-my-zsh에서 home key와 end key가 안될 때 해결방법     Ubuntu에서 Base64로 인코딩, 디코딩하기     Editor(vi)가 없을 때 파일 수정하기     열려있는 포트 확인하기     pipe를 사용한 명령어를 watch로 확인하기     watch를 사용할 때 alias 이용하기     password 없이 ssh 접속하기     SSH Tunneling 사용법     Gateway를 이용하여 SSH 접속하기     Hostname 변경하기     추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기     Ubuntu 설치 시 Boot Parameter를 수정하기     sudo를 password 없이 사용하기     "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/delegation-local-actions-and-pauses/",
	"title": "Delegation Local Actions and Pauses",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "notification을 보내는 것, load balancer와 통신하는것 또는 DNS, 네트워킹에 변화를 주는것, 서버를 모니터링하는 것과 같은 몇몇 task는 Ansible이 host machine(playbook을 실행하는)에서 동작하거나 playbook에 의해 관리되지 않는 host에서 실행할 필요가 있다. Ansible은 delegate_to를 사용한 특정 host에 task를 delegate할 수 있다.\n- name: Add server to Munin monitoring configuration. command: monitor-server webservers {{ inventory_hostname }} delegate_to: \u0026#34;{{ monitoring_master }}\u0026#34; delegation은 서버의 load balancer 또는 replication pool에 등록하는 것을 관리하기 위해 사용된다. 우리는 특정한 command를 local에서 실행하거나 Ansible에 내장된 load balancer module을 사용하고 delegate_to를 이용하여 직접 load balancer host에서 command를 실행할 수 있다.\n- name: Remove server from load balancer. command: remove-from-lb {{ inventory_hostname }} delegate_to: 127.0.0.1 task를 localhost로 delegate할 때 Ansible은 delegate_to 대신에 local_action이라는 간단한 명령을 사용할 수도 있다.\n- name: Remove server from load balancer. local_action: command remove-from-lb {{ inventory_hostname }} Pausing playbook execution with wait_for playbook의 중간에서 local_action을 사용하여 서버가 완전히 부팅되기까지를 기다리거나 어플리케이션이 특정 포트를 listen 할때까지 기다리도록 할 수 있다.\n- name: Wait for webserver to start. local_action: module: wait_for host: \u0026#34;{{ inventory_hostname }}\u0026#34; port: \u0026#34;{{ webserver_port }}\u0026#34; delay: 10 timeout: 300 state: started 위의 task는 webserver_port가 inventory_hostname에서 open될 때까지 기다리며 5분의 timeout을 가지고 Ansible playbook을 실행하는 host에서 체크를 한다(첫번째 체크를 하기 전과 체크 사이사이 10초의 지연시간이 있다).\nwait_for은 많은 것들을 기다리기 위해 playbook execution을 멈출 때 사용된다.\n host와 port를 사용하여 timeout까지 기다리며 port가 사용 가능할때까지(또는 불가능할때까지) 기다린다. path(필요한 경우 search_regex도 사용하여)를 통해 timeout까지 파일이 존재하기를(또는 존재하지 않기를) 기다린다. host, port, drained를 state의 parameter로 사용하여 주어진 port가 모든 active connection을 drain했는지 확인한다. delay를 이용하여 단순히 주어진 시간(초단위로)만큼 playbook의 실행을 멈춘다.  Running an entire playbook locally playbook을 task가 실행되어야 하는 server나 worksation에서 실행하거나(예를 들면 self-provisioning) ansible-playbook command가 실행되는 것과 같은 호스트에서 playbook이 실행되어야 할 때 --connection=local을 사용하여 SSH connection overhead를 없애 playbook의 실행을 빠르게 할 수 있다.\n간단한 예시로 ansible-playbook test.yml --connection=local을 사용하는 짧은 playbook이 있다.\n--- - hosts: 127.0.0.1 gather_facts: no tasks: - name: Check the current system date. command: date register: date - name: Print the current system date. debug: var=date.stdout 이 playbook은 localhost를 실행하고 현재의 날짜를 debug message에 출력한다. 이는 local connection으로 동작하기 때문에 굉장히 빠르게(저자의 Mac에서는 0.2초가 걸렸다) 동작할 것이다.\nplaybook을 --connection=local로 실행하는 것은 전체 playbook을 --check 모드로 configuration을 확인할 때(변화가 있을 때 email을 주는 cron job과 같은것) 또는 testing infrastructure에서 playbook을 testing할 때(Travis, Jenkins나 다른 CI tool을 통해) 유용하게 사용할 수 있다.\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/",
	"title": "Chapter5",
	"tags": [],
	"description": "",
	"content": "Ansible For DevOps Chapter 5  Ansible Playbooks Beyond the Basics     Handover     Environment Variables     Variables     if/then/when - Conditionals     Delegation Local Actions and Pauses     Prompts     Tags     Summary     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/deploy-and-connect/",
	"title": "Deploy and Connect",
	"tags": ["spinnaker", "install"],
	"description": "",
	"content": "드디어 마지막 절차입니다.\n먼저 어떤 버전을 설치할지 확인후 설정합니다.\nhal version list 작성 기준으로 최신 버전이 1.17.6이므로 이를 설정합니다.\nhal config version edit --version 1.17.6 halyard를 NodePort로 노출시키기 위해 api와 ui에 base url을 부여합니다.\nhal config security ui edit --override-base-url http://192.168.8.22:30100 hal config security api edit --override-base-url http://192.168.8.22:30200 이제 본격적으로 deploy를 하도록 합니다.\nhal deploy apply 그 후 Spinnaker를 NodePort로 서비스합니다.\nkubectl patch svc spin-deck -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;}]\u0026#39; kubectl patch svc spin-gate -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;}]\u0026#39; kubectl patch svc spin-deck -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;: 30100}]\u0026#39; kubectl patch svc spin-gate -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;: 30200}]\u0026#39; 이제 Spinnaker로 접속하여 확인합니다. url은 http://:30100 입니다.\n여기까지 했으면 Spinnaker를 Kubernetes에서 사용할 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter6/",
	"title": "Chapter6",
	"tags": [],
	"description": "",
	"content": "Ansible For DevOps Chapter 6  "
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/prompts/",
	"title": "Prompts",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "드문 경우이지만 사용자가 playbook에서 사용할 variable의 값을 입력하게 해야할 경우가 있을 수 있다. playbook이 사용자의 개인 로그인 정보를 필요로 하거나 playbook을 실행하는 사람에 의해 버전이나 다른 값들을 입력받아야 할 때, 어디에서 실행되어야 하는지 알아야 할 때면서 이런 정보들이 configured(environment variable나 inventory variable을 사용하는 것)되는 방법이 없는 경우에는 vars_prompt를 이용한다.\n간단한 예시로 사용자에게 username과 password를 입력하게 하여 network share에 로그인할 수 있도록 할 수 있다.\n--- - hosts: all vars_prompt: - name: share_user prompt: \u0026#34;What is your network username?\u0026#34; - name: share_pass prompt: \u0026#34;What is your network password?\u0026#34; private: yes Ansible이 play를 실행하기 전에 Ansible은 사용자에게 username과 password를 입력하게 할 것이고 나중에 입력하는 값들은 보안상의 문제때문에 command line에 숨겨져 보일 것이다.\nprompt에는 다음과 같은 특수 옵션들이 있다.\n private: yes로 설정될 경우 사용자의 입력이 command line에서 숨겨진다. default: prompt에 대해 default value를 설정하여 end user의 시간을 절약할 수 있다. encrypt/confirm/slat_size: 이 값들은 password를 설정할 때 사용하여 entry를 확인(confirm이 yes로 설정되었을 경우 사용자는 password를 두번 입력해야 한다)하고 이를 salt(지정된 size와 crypt scheme을 이용하여)를 이용하여 암호화한다. Ansible의 Prmopts documentation을 통해 prompted variable encryption에 대한 자세한 정보를 얻을 수 있다.  prompts는 user-specific한 정보를 수집하는 간단한 방법이지만 대부분의 경우에 절대적으로 필요하지 않는 이상 피하는 것이 좋다. playbook을 실행하는 데 완전한 automation을 유지하고자 한다면 role이나 playbook variable, inventorty variable 또는 심지어 local environment variable을 사용하는 것이 더 선호되는 방법이다.\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/install-in-air-gaped-environment/",
	"title": "Install in Air Gaped Environment",
	"tags": ["install", "spinnaker", "air-gaped"],
	"description": "",
	"content": "이번에는 인터넷이 되지 않는 환경에서 어떻게 Spinnaker를 설치하는지에 대해 알아보도록 하겠습니다.\n먼저 halyard에서 언제 인터넷과 통신하는지를 대강 추려보도록 하겠습니다.\n Spinnaker의 version.yaml을 불러와서 최신의 halyard 버전과 최신 Spinnaker의 버전들을 보여줍니다.  gs://halconfig/version.yml   설치하고자 하는 Spinnaker의 버전을 선택하면, 그에 따른 배포에 필요한 yaml들을 불러옵니다.  gs://halconfig/bom/VERSION.yml gs://halconfig/MICRO_SERVICE/TAG.yml   deploy를 하기 위해 Google Cloud Repository에서 이미지를 가지고 옵니다.  gcr.io/spinnaker-marketplace/SERVICE   마지막으로 dependency가 있는 몇가지 서비스를 Google Cloud Repository에서 가지고옵니다. (consul, redis, vault)  gcr.io/kubernetes-spinnaker/SERVICE    여기서 local 설정으로 변경이 가능한 것은 2020.01.20 현재 1,2,3번 항목들입니다. 이것들을 어떻게 인터넷이 되지 않는 환경에서 설치가 가능하도록 설정하는지에 대해 알아보겠습니다.\ngsutil로 gs://halconfig 파일들을 로컬에 복사하기 우선 인터넷이 잘 되는 서버가 하나 필요합니다. 이 서버에서 우리는 필요한 BOM(Bill of Materials)를 미리 다운로드 할 것입니다.\ngsutil이 설치되어 있어야 합니다.\ngsutil -m cp -r gs://halconfig . 이렇게하면 로컬에 halconfig라는 폴더가 생겼을 것입니다. 이를 인터넷이 안되는 halyard가 설치된 서버로 복사합니다. 이때, halconfig 폴더 내의 내용들은 ~/.hal/.boms/ 폴더 내에 복사합니다.\n$ ls ~/.hal/.boms/ bom clouddriver deck echo fiat front50 gate igor kayenta monitoring-daemon orca rosco versions.yml 여기서 rosco/master 폴더로 들어가면 packer.tar.gz라는 폴더가 있습니다. 이를 rosco 폴더로 옮기고 압축을 해제합니다.\nmv ~/.hal/.boms/rosco/master/packer.tar.gz ~/.hal/.boms/rosco cd ~/.hal/.boms/rosco tar xvf packer.tar.gz halyard에서 gcs의 version.yml이 아닌 로컬의 version.yml을 참조하도록 설정 기본적으로 halyard는 gs://halconfig/version.yml을 참조하려 할 것입니다. 이를 local:이라는 접두사를 붙여 로컬을 바라보게 할 수 있습니다.\nhal config version edit --version local:1.17.4 그리고 halyard가 gcs를 바라보지 않도록 설정합니다.\n# /opt/spinnaker/config/halyard-local.yml spinnaker: config: input: gcs: enabled: false 그러고 난 뒤, 각 서비스들의 BOM도 로컬을 바라보게 설정해야 합니다. 아까 위에서 1.17.4 버전을 사용한다고 했으니, 해당 yaml파일을 열고 local: 접두사를 추가합니다.\nartifactSources: debianRepository: https://dl.bintray.com/spinnaker-releases/debians dockerRegistry: gcr.io/spinnaker-marketplace gitPrefix: https://github.com/spinnaker googleImageProject: marketplace-spinnaker-release dependencies: consul: version: 0.7.5 redis: version: 2:2.8.4-2 vault: version: 0.7.0 services: clouddriver: commit: 024b9220a1322f80ed732de9f58aec2768e93d1b version: local:6.4.3-20191210131345 deck: commit: 12edf0a7c05f3fab921535723c8a384c1336218b version: local:2.13.3-20191210131345 defaultArtifact: {} echo: commit: acca50adef83a67e275bcb6aabba1ccdce2ca705 version: local:2.9.0-20191029172246 fiat: commit: c62d038c2a9531042ff33c5992384184b1370b27 version: local:1.8.3-20191202102650 front50: commit: 9415a443b0d6bf800ccca8c2764d303eb4d29366 version: local:0.20.1-20191107034416 gate: commit: a453541b47c745a283712bb240ab392ad7319e8d version: local:1.13.0-20191029172246 igor: commit: 37fe1ed0c463bdaa87996a4d4dd81fee2325ec8e version: local:1.7.0-20191029183208 kayenta: commit: 5dcec805b7533d0406f1e657a62122f4278d665d version: local:0.12.0-20191023142816 monitoring-daemon: commit: 59cbbec589f982864cee45d20c99c32d39c75f7f version: local:0.16.0-20191007112816 monitoring-third-party: commit: 59cbbec589f982864cee45d20c99c32d39c75f7f version: local:0.16.0-20191007112816 orca: commit: b88f62a1b2b1bdee0f45d7f9491932f9c51371d9 version: local:2.11.2-20191212093351 rosco: commit: 269dc830cf7ea2ee6c160163e30d6cbd099269c2 version: local:0.15.1-20191202163249 timestamp: \u0026#39;2019-12-12 14:34:16\u0026#39; version: 1.17.4 이렇게 설정하면 echo를 예로 들 때 ~/.hal/.boms/echo/2.9.0-20191029172246/echo.yml을 참조하게 될 것입니다.\n배포에 필요한 이미지들을 private registry에 불러오기 이제 실제 배포에 필요한 이미지를 로컬로 복사해두어야 합니다. 저는 내부에서 사용하는 docker registry에다가 저장해 둘 것입니다. 인터넷이 되는 서버에서 다음과 같이 작업하면 됩니다.\ndocker pull gcr.io/spinnaker-marketplace/SERVICE:TAG docker tag gcr.io/spinnaker-marketplace/SERVICE:TAG private-docker-registry/repository-name/SERVICE:TAG docker push private-docker-registry/repository-name/SERVICE:TAG 이렇게 private registry로 저장을 해 두었을 경우 VERSION.yml 파일에서 dockerRegistry 항목을 수정합니다.\nartifactSources: debianRepository: https://dl.bintray.com/spinnaker-releases/debians #dockerRegistry: gcr.io/spinnaker-marketplace dockerRegistry: private-docker-registry/repository-name gitPrefix: https://github.com/spinnaker googleImageProject: marketplace-spinnaker-release 또는 docker pull을 이용해서 이미지를 다운받고, 이를 docker save 명령어를 통해 tar.gz 파일로 변환한 뒤, Kubernetes의 모든 워커노드에서 이를 이리 docker load 하는 방법도 있습니다. 이렇게 하면 이미 로컬에 있는 이미지이기 때문에 외부로 접속하지 않습니다.\ndocker pull gcr.io/spinnaker-marketplace/SERVICE:TAG docker save -o SERVICE.tar.gz gcr.io/spinnaker-marketplace/SERVICE:TAG scp SERVICE.tar.gz TARGET_IP:~/path/to/target ssh TARGET_IP docker load -i ~/path/to/target/SERVICE.tar.gz 이번에는 dependency와 관련된 이미지를 불러와야 합니다. 먼저 Image Registry를 변경하기 위해서는 다음과 같이 조치합니다.\n ~/.hal/default/service-settings/redis.yml파일을 생성합니다. 다음과 같이 작성합니다. artifactId: private-docker-registry/repository-name/redis-cluster:v2   이 다음에는 마찬가지로 image를 pull하고 이를 private docker registry로 push합니다.\ndocker pull gcr.io/kubernetes-spinnaker/SERVICE:TAG # redis-cluster:v2 docker tag gcr.io/kubernetes-spinnaker/SERVICE:TAG private-docker-registry/repository-name/SERVICE:TAG docker push private-docker-registry/repository-name/SERVICE:TAG 또는 이미지를 tar로 묶어서 복사하는 방법도 있습니다.\ndocker pull gcr.io/kubernetes-spinnaker/SERVICE:TAG docker save -o SERVICE.tar.gz gcr.io/kubernetes-spinnaker/SERVICE:TAG scp SERVICE.tar.gz TARGET_IP:~/path/to/target ssh TARGET_IP docker load -i ~/path/to/target/SERVICE.tar.gz Image Registry가 kubernetes-spinnaker로 변경된 것을 주의하시면 됩니다.\nDeploy 여기까지 왔으면 모든 준비작업은 끝났습니다. 이제 배포만 하면 됩니다.\nhal deploy apply Reference https://www.spinnaker.io/guides/operator/custom-boms/\nhttps://github.com/spinnaker/spinnaker/issues/3967#issuecomment-522306893\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/tags/",
	"title": "Tags",
	"tags": [""],
	"description": "",
	"content": "Tags는 playbook의 task들의 subset을 실행할 수 있도록(또는 제외할 수 있도록) 한다.\nrole, included file, individual task, 심지어 전체 play에 대해서 tag를 달 수 있다. 문법은 매우 간단하며 아래의 예시는 tag를 추가하는 다양한 방법을 보여준다.\n--- # You can apply tags to an entire play. - hosts: webservers tags: deploy roles: # Tags applied to a role will be applied to the tasks in the role. - { role: tomcat, tags: [\u0026#39;tomcat\u0026#39;, \u0026#39;app\u0026#39;] } tasks: - name: Notify on completion. local_action: module: osx_say msg: \u0026#34;{{ inventory_hostname }} is finished!\u0026#34; voice: Zarvox tags: - notifications - say - include: foo.yml tags: foo 위의 playbook을 tags.yml 파일로 저장했다고 가정하면 우리는 아래의 command를 통해 tomcat role과 Notify on completion task만 실행하도록 할 수 있다.\n$ ansible-playbook tags.yml --tags \u0026#34;tomcat, say notifications tag를 가진것을 제외하고 싶으면 --skip-tags를 사용하면 된다.\n$ ansible-playbook tags.yml --skip-tags \u0026#34;notifications\u0026#34; 이는 웬만한 tagging structure를 가지고 있다면 매우 손쉽다. playbook의 특정한 부분만 실행하고 싶을 때, 또는 series에서 하나의 play만 실행하고 싶은 경우(또는 play나 included task를 제외하고 싶은 경우) --tags나 --skip-tags를 사용하면 쉽다.\nplaybook에서 tags 옵션을 사용하여 하나 이상의 tag를 생성할 때에는 한가지 주의해야할 점이 있다. tags: tagname은 하나의 tag를 추가할때만 사용하고 하나보다 더 많은 경우 YAML이 list syntax를 사용해야 한다.\n# Shorthand list syntax. tags: [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] # Explicit list syntax. tags: - one - two - three # Non-working example. tags: one, two, three 일반적으로 특히 individual role과 play가 있는 더 큰 playbook에서는 tag를 많이 사용하지만 task의 묶음을 디버깅하는 경우가 아니라면 저자는 일반적으로 tag를 individual task나 includes에 추가하는 것을 피한다(visual clutter를 줄이기 위해 tag를 어디에도 추가하지 않는다). 우리의 니즈에 맞는 tagging style을 찾아야 하고 그러면 playbook에서 원하는 특정 부분을 실행할 수(또는 실행하지 않을 수) 있을 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/ansible/ansible-for-devops/chapter5/summary/",
	"title": "Summary",
	"tags": ["ansible", "ansible-for-devops"],
	"description": "",
	"content": "playbook은 Ansible의 automating infrastructure management의 중요한 수단이다. 이 챕터를 읽은 뒤로 우리는 어떻게 variable, inventory, handler, conditional, tag 등을 사용하는지 알게 되었다.\nplaybook에 대해서 기본적인 component들을 더 알고싶다면 infrastructure를 Ansible로 만들고 확장하는 것을 해보는 것이 더욱 효과적일 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/10-kubernetes-the-hard-way/",
	"title": "10 Kubernetes the Hard Way",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Design a Kubernetes Cluster  Purpose  Education  Minikube Single node cluster with kubeadm/GCP/AWS   Development \u0026amp; Testing  Multi-node cluster with a Single Master and Multiple workers Setup using kubeadm tool or quick provision on GCP or AWS or AKS   Hosting Production Applications  HA Multi node cluster with multiple master nodes 최대 5000노드 클러스터 내에 최대 약 150000 파드 최대 300000개의 컨테이너 노드당 최대 약 100개의 파드     Cloud or OnPrem  on-prem에는 kubeadm을 사용하자. GKE for GCP Kops for AWS AKS for Azure   Storage  High Performance - SSD 사용 Multiple Concurrent connections - Network based storage Persistent shared volumes for shared access across multiple PODs Node를 disk type에 따라 label을 주입하기 Node selector를 사용하여 특정 disk type을 사용하는 노드로 application 할당하기   Nodes  Virtual or Physical Machines 최소 4개의 노드 Master vs Worker Nodes  Master node도 workload를 호스트할 수 있다.   Linux X86_64 Architecture   Master Nodes  대규모의 Cluster에서는 Master Nodes와 ETCD cluster를 분리해야한다.    Choosing Kubernetes Infrastructure  Windows로 호스팅을 하려면 바이너리 파일이 따로 없기 때문에 VM을 이용해서 리눅스를 띄워야 한다. 간단하게 로컬로 클러스터를 만들고자 한다면 Minikube를 사용할 수 있다.  Single Node   kubeadm으로도 가능하다.  Single/Multi Node 가능   Turnkey Solutions  VM을 Provision 해야함. VM을 Configure 해야함. Script를 통해 Cluster를 배포함. 스스로 VM을 관리해야함. AWS에서의 KOPS   Hosted Solutions  Kubernetes-As-A-Service Provider가 VM을 Provision함. Provider가 Kubernetes를 설치함. Provider가 VM을 관리함. GKE   OpenShift는 on-prem kubernetes platform중 하나이다.  쿠버네티스 위에 설치되어있는 서비스. 쿠버네티스 관리를 위한 추가적인 툴과 GUI를 제공함. CI/CD 파이프라인 제공   Cloud Foundry Container Runtime은 BOSH라는 오픈소스를 통해 HA cluster를 관리함.  Configure High Availability  Master Node가 삭제되면 어떡하나?  일단 계속 동작은 함. 그러다가 파드가 크래시나기 시작하면 문제가 발생. 파드를 재시작해줄 수 없음.   따라서 마스터노드를 여러대 두어야 한다. Signle Point of Failure를 없애기 위함. 마스터노드를 두개 두려면 컴포넌트들 역시 둘 다 배포되어야 한다.  ETCD, API Server, Controller Manager, Scheduler   API Server는 Active, Active 모드로 떠있을 수 있다.  기존에는 API server의 6443 포트로 kubectl 이 접근했었다. 이를 LoadBalancer를 통해서 트래픽을 분기해주는 것이 필요하다. NGINX, HA proxy등이 이용가능하다.   Scheduler, Controller Manager의 경우  클러스터의 상태를 보고 행동을 취하는 것들이다. 계속해서 파드의 상태를 체크한다. 동시에 동작한다면 실제로 생성해야 하는 파드보다 더 많은 파드를 생성해버릴 수 있다. 따라서 Active/Standby Mode로 동작해야한다. leader election process가 필요하다. kube-controller-manager 에서 --leader-elect 를 true 로 설정해야한다. 이들은 kube-controller-manager endpoint 에 lock을 걸로 lease를 한다. 둘다 2초마다 leader가 되고자 한다. 하나가 down되면 두번째 것이 up이 된다.   ETCD는 쿠버네티스 마스터 노드 안에 위치할 수 있고 이는 Stacked Topology라고 한다.  이렇게 구성했을 때에는 하나가 죽었을 때 Redundancy가 제대로 되지 않는다. 다른 방법은 ETCD를 Control plan node와 분리시키는 것이다.   ETCD를 다른 노드에 두는 것을 External ETCD Topology라고 한다.  이는 control plane node가 죽더라도 ETCD cluster와 데이터에 영향을 주지 않아 덜 위험하다. 외부 노드가 필요하기 때문에 서버가 더 필요하고 셋업이 더 어렵다. API Server가 유일하게 ETCD를 바라보고 있고, API Server에 ETCD의 주소를 적을 수 있기 때문에 API Server가 제대로 바라보도록 설정해야 한다.    ETCD in HA  ETCD는 distributed reliable key-value store이다. distributed라는 의미는 동일한 copy를 가진 database를 가지고 있다는 것을 의미한다.  ETCD는 어떤 copy도 동시에 접근이 가능함을 보장한다.   READ는 문제가 없지만 WRITE는 상황이 다르다. ETCD는 하나의 instance만 write를 처리할 수 있다.  내부적으로 Leader/Follower를 정의한다. Follower로가 write 요청을 받으면 이를 Leader로 포워딩하여 WRITE를 처리한다.   Leader Election은 RAFT 알고리즘을 통해서 한다. Quorum 이상의 숫자만큼 write가 되면 정상적으로 쓰기가 되었다고 판단한다.  N/2 + 1 따라서 2개의 인스턴스만 있으면 fault tolerance가 제대로 보장되지 않는다. 짝수로 클러스터를 구성하면 네트워크가 나뉘었을 때 클러스터가 망가질 수 있다. 따라서 홀수가 추천되고 5개가 가장 적절하다.    TLS Bootstrap worker node  kubelet의 client는 kube-apiserver이다. Bootstrap Token을 생성하고 이를 group system:bootstrappers에 넣는다. system:node-bootstraper를 system:bootstrappers 그룹에 할당한다. system:certificates.k8s.io:certificatesigningrequests:nodeclient를 system:bootstrappers 그룹에 넣는다. systemLcertificates.k8s.io:certificatessiningrequests:selfnodeclient를 system:nodes에 넣는다. kube documentation에 있다. 이렇게 하면 워커는 자동으로 TLS certificates를 생성하고 expire 시 renew가 가능하다. kubelet에 --rotate-certificates=true를 넣으면 된다. server certificates는 보안상의 이유로 수동으로 approve해야 한다.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/09-networking/",
	"title": "09 Networking",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Switching Routing  Switching  컴퓨터 두대를 스위치를 통해 연결하는 방법. 연결된 스위치의 인터페이스에 ip를 할당한다. 동일한 네트워크로만 트래픽을 보내준다.   Routing  라우터는 스위치로 연결된 두 네트워크 대역을 연결해준다. 각 네트워크 대역에 대해서 아이피를 동시에 가지고 있다.   Gateway  어떤 아이피 대역에 대해서 어떤 라우터를 통해서 가게 할지 결정하는 것 외부 인터넷과 연결하고 싶다면 먼저 인터넷을 라우터에 연결하고, 이 곳으로 route 설정을 하면 된다. default gateway는 Destination=0.0.0.0인 것. gateway가 0.0.0.0이라면 라우터가 필요없다는 의미.   기본적으로 /proc/sys/net/ipv4/ip_forward가 0으로 설정되어 자신에게 오는 트래픽이 아닌 경우 포워딩하지 않는다.  DNS  /etc/hosts에 domain name을 넣을 수 있다. DNS server에 domain name을 정리하고, 각 컴퓨터에서 이를 조회하도록 한다. /etc/resolv.conf에 nameserver를 넣어서 서버에 있는 dns를 사용할 수 있다. domain name lookup할 때 local 파일부터 찾고 그 다음에 nameserver에서 검색한다. DNS server에 Forward All to 8.8.8.8로 public internet의 DNS를 default로 참조하도록 할 수 있다. apps.google.com으로 조회를 하면 root DNS, .com DNS, Google DNS의 순서로 조회하여 IP를 가져오고, 이를 캐싱하게 된다. /etc/resolv.conf에 search를 mycompany.com으로 등록하게 되면 ping web을 입력했을 때 web.mycompany.com으로 검색한다. Record Types  A: IPv4 AAAA: IPv6 CNAME: name alias (name to name mapping)    Network Namespaces  쿠버네티스는 네임스페이스를 기준으로 나누어져 있다. 네임스페이스 안에서는 서로의 프로세스가 무엇인지 볼 수 있다. 컨테이너에서 실행한 것은 호스트에서 프로세스 확인이 가능하다.  process ID는 컨테이너 내부와 바깥이 서로 다르다.   컨테이너가 생성되면 네트워크 네임스페이스를 호스트에서 생성한다.  컨테이너는 그 안에서 Routing Table과 ARP Table을 가진다.   리눅스 호스트에서 network namespace를 생성하려면 ip netns add red와 같이 입력하면 된다.  ip netns로 리스트 확인 가능.   ip link를 입력하면 interface를 볼 수 있는데 이를 네임스페이스 아래에서 실행하고자 한다면 다음과 같이 하면 된다.  ip netns exec red ip link 또는 ip -n red link   나머지도 비슷하다.  ip netns exec red arp ip netns exec red route   네트워크 네임스페이스 두개를 연결하려면 파이프를 연결하면 된다(가상의 랜선이라고 생각하면 된다.)  ip link add veth-red type veth peer name veth-blue ip link set veth-red netns red ip link set veth-blue netns blue ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue ip -n red link set veth-red up ip -n blue link set veth-blue up ip netns exec red ping 182.168.15.2   virtual switch를 통해 네트워크 네임스페이스를 연결할 수 있다.  linux bridge, open vswitch가 있다. ip link add v-net-0 type bridge ip link set dev v-net-0 up ip -n red link del veth-red \u0026lt;- 위에서 생성한 연결을 끊는 것. ip link add veth-red type veth peer name veth-red-br ip link add veth-blue type veth peer name veth-blue -br ip link set veth-red netns red ip link set veth-red-br master v-net-0 ip link set veth-blue netns blue ip link set veth-blue-br master v-net-0 ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue ip -n red link set veth-red up ip -n blued link set veth-blue up 이를 호스트의 네트워크와 연결할 수 있다.  virtual switch(bridge)는 호스트의 자원. ip addr add 192.168.15.5/24 dev v-net-0을 추가하면 연결이 가능   외부 인터넷과 연결하려면 호스트의 eth0와 연결을 해야한다. gateway를 추가. ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5  이 상태에서는 ping이 되지 않음. private ip를 달고 나가기 때문에 외부에서는 인식하지 못하는 IP. 따라서 NAT를 해야한다. iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE     외부에서 network namespace로 들어오려면?  2가지 해법이 있음.  직접 ip 할당하고 라우팅하기 iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT      Docker Networking  docker run에서 --network none을 하면 컨테이너는 어떤 곳과도 통신할 수 없다. --network host를 사용하여 port를 열면 host의 port가 열린다. --network bridge는 default로 도커 컨테이너끼리 통신이 가능하다. docker는 실행시 default로 brdige를 생성한다. docker network ls를 통해 확인 가능.  ip link를 통해 docker0를 볼 수 있다.  ip link add docker0 type bridge를 통해 생성한 것과 동일.   ip addr을 통해 보면 ip를 확인할 수 있다. (brdige의 ip)   docker run을 하면 도커는 네트워크 네임스페이스를 생성한다.  ip netns를 보면 확인이 가능하다. docker inspect \u0026lt;container id\u0026gt;를 통해서 어떤 네트워크 네임스페이스인지 확인이 가능하다.   host에서 ip link를 하면 네임스페이스와 bridge(docker0)를 연결한 것이 보인다.  ip -n \u0026lt;network namespace\u0026gt; link를 하면 컨테이너에서 brdige에 연결한 것이 보인다. ip -n \u0026lt;network namespace\u0026gt; addr을 하면 컨테이너의 아이피를 확인할 수 있다.   컨테이너의 port를 호스트의 port와 연결하려면 -p 8080:80과 같은 옵션을 사용하면 된다.  이는 nat를 사용하여 할 수 있다. ip tables -t nat -A DOCKER -j DNAT --dport 8080 --to-destination 172.17.0.3:80 iptables -nvL -t nat을 통해 확인 가능.    CNI  bridge program은 특정한 task를 쭉 실행해준다. bridge add \u0026lt;cid\u0026gt; \u0026lt;namespace\u0026gt;를 통해 할 수 있다.  쿠버네티스에서도 이 방식을 사용한다.   container runtime간에 제대로 동작을 하는지 확인하기 위한 표준이 생겼다.  bridge도 CNI의 plugin 중 하나이다.   CNI 표준  Container Runtime은 반드시 network namespace를 생성해야한다. container가 연결되어야 하는 network를 확인해야한다. container runtime은 컨테이너가 생성될 때 network plugin(brdige)를 호출해야한다. container runtime은 컨테이너가 삭제될 때 network plugin(brdige)를 호출해야한다. JSON 형식으로 network configuration을 할 수 있어야 한다.   Plugin 표준  add/del/check에 대한 command line arguments를 제공해야한다. contianer id, network 등에 대한 파라미터를 제공해야한다. 파드에 할당되는 IP를 관리해야한다. 특정한 형식으로 결과를 리턴해야한다.   Plugin에는 다음과 같은 것들이 있다.  bridge vlan ipvlan macvlan windows dhcp host-local   그러나 docker는 CNI 표준을 지키지 않는다.  자신만의 독자적인 Container Network Model을 사용한다. 따라서 docker run --network=cni-brdige nginx와 같이 사용할 수는 없다. 하지만 그렇다고 해서 docker에서 아주 못사용하는 것은 아니다.  docker run --network=none nginx처럼 우선 생성한 뒤 수동으로 plugin을 실행하면 된다. 이 방식을 쿠버네티스에서 사용한다.      Cluster Netwrking  쿠버네티스는 마스터, 워커 노드가 있고, 이들이 네트워크로 연결이 되어야 한다. 이들은 서로 hostname과 MAC address가 달라야 한다. 마스터 노드는 특정한 포트를 열어야 한다.  kube-apiserver에서 사용하기 위한 6443 포트 kube-scheduler에서 사용하기 위한 10251 포트 kube-controller-manager에서 사용하기 위한 10252 포트 ETCD에서 사용하기 위한 2379, multi-master일 경우 2380도 열어야 한다.   또한 워커 노드는 kubelet을 위한 10250 포트를 열어야 한다.  그리고 service expose를 위한 30000-32767까지의 포트를 열 수 있어야 한다.    Pod Networking  pod layer에서의 네트워킹이 중요하다. 파드는 수백개가 생성이 될 수 있는데 이들끼리 서로 통신이 되어야 하고, 아이피가 할당되어야 한다. 쿠버네티스는 built-in solution으로 이를 해결하지 않는다. 다음이 구현되어야 한다.  모든 파드는 IP address를 가지고 있다. 모든 파드는 같은 노드 안에서의 파드와 통신할 수 있어야 한다. 모든 파드는 다른 노드에 있는 파드와 NAT가 아닌 방법으로 통신할 수 있어야 한다.   이를 구현한 네트워킹 모델들이 있다. (flannel, calico 등) 강의에서는 이전에 배운 네트워크 네임스페이스를 기반으로 어떻게 동작하는지를 알아볼 것이다.  각 노드들은 192.168.1.0 대역을 가지고 있다고 가정한다. 각각의 노드에 대해서 bridge를 생성한다. 그 다음 bridge를 up 한다. 각 노드에 대해서 10.244.1.0/24, 10.244.2.0/24, 10.244.3.0/24의 네트워크 대역을 가지도록 한다. 그 뒤 컨테이너가 생성될때마다 특정 스크립트를 생성해서 아이피를 할당하고 노드 내부 bridge와 연결한다. 그 다음 각 노드의 파드끼리 route 설정을 넣어준다.   이런 것들을 CNI를 통해서 해결한다. kubelet은 각 노드에 있는 컨테이너의 생성/삭제를 관리한다.  따라서 컨테이너가 생성될 때 kubelet을 실행할 때 사용된 arguments에 있는 CNI를 확인하고, script를 실행한다.    CNI in kubernetes  CNI plugin은 각 kubelet을 실행할 대 arguments로 등록할 수 있다. --cni-bin-dir에서 설정한 디렉토리에는 CNI의 binary 파일들이 있다. --cni-conf에는 어떤 CNI를 사용할지 알파벳 순서로 골라서 사용한다.  CNI weave  파드에서 파드로 가는 트래픽은 노드 바깥의 router로 나간 뒤, 알맞은 노드로 들어가서 파드에 전달된다.  그러나 이는 수백개의 노드가 각각 수백개의 파드를 가지고 있는 상황에서는 제대로 작동할 수 없다. 라우팅 테이블이 모든것을 담을 수 없다.   weave는 각각의 노드에 agent를 두고, 서로 통신하게 하여 서로의 위치를 확인한다.  agent는 다른 노드의 파드로 가는 트래픽을 가로채서 어디로 보낼지 확인한 뒤 트래픽을 감싼다. 해당 트래픽은 다른 노드로 전달된 뒤에 agent가 다시 가로채서 트래픽을 확인하고 원하는 파드로 전달한다.    IP Address Management - Weave  어떻게 노드의 Virtual Brdige가 IP를 할당하는지에 대해 알아볼 것이다. network plugin에 의존하여 CNI는 IP를 할당한다.  쿠버네티스는 할당한 IP의 관리에는 영향을 주지 않는다.   /etc/cni/net.d/net-script.conf에는 사용하는 CNI에 대한 정보가 적혀있다(IPAM). weaveworks는 10.32.0.0/12로 IP 대역을 파드에 할당하려 하고 각 노드에 10.32.0.1, 10.38.0.1, 10.44.0.0으로 IP 대역을 나누어 노드에 할당한다. customization이 가능하다.  Service Networking  서비스가 생성되었을 때에도 노드의 위치에 상관없이 모든 파드에서 접근이 가능해야한다. 서비스는 특정 노드에 bound되는 것이 아니라 노드 전역에 걸쳐 생성된다. ClusterIP는 클러스터 내부에서만 연결이 된다.  DB처럼 클러스터 내부에서만 통신하려는 목적이면 ClusterIP를 사용하면 된다.   외부로 노출하기 위해서는 NodePort를 사용한다.  IP를 가지는것 뿐만 아니라 노드에서 포트를 할당받기도 한다.   service는 단순히 virtual object로 어딘가에 생성되는 것이 아니다.  쿠버네티스에 serivce를 생성하면 미리 설정된 범위에서 IP 주소를 할당받는다. 할당받은 IP 주소로 forwarding rule이 생성된다. 단순히 IP로 forwarding하지 않고 IP:port로 forwarding한다   userspace, iptables, ipvs 모드로 kube-proxy가 설정될 수 있으며 기본은 iptables이다. service의 ip range는 kube-api-server에서 default로 10.0.0.0/24로 설정되어있다. pod ip range와 service ip range는 겹치면 안된다. kube-proxy의 로그를 보면 어떻게 iptables를 조작했는지 볼 수 있다.  Cluster DNS  쿠버네티스는 설치시 자신만의 DNS 서버를 가진다. 서비스가 생성이 될때, hostname과 IP 주스를 해당 서비스에 맞게 등록한다. 파드 내에서는 해당 domain을 통해 접근할 수 있다. 동일한 네임스페이스에서는 service name만 가지고 접근할 수 있다. 다른 네임스페이스에 있는 경우 \u0026lt;service name\u0026gt;.\u0026lt;namespace\u0026gt;로 접근해야한다. apps namespace 안의 web-service가 있는 경우  hostname: web-service namespace: apps type: svc root: cluster.local   pod에 대해서도 dns에 등록이 된다. 그러나 IP 주소를 10-244-2-5와 같이 dash를 통해 구분할 뿐이다.  type: pod    CoreDNS in Kubernetes  여러 서비스 및 파드가 생성/삭제될 때 domain을 정리하기 위해 중앙의 DNS 서버를 사용한다. 파드 내에서는 /etc/resolv.conf에서 nameserver로 등록한다. 새로운 파드가 생기면 DNS 서버에 엔트리를 추가한다. 1.12 이전에는 kube-dns를 사용했지만 이후로는 CoreDNS를 권장한다. CoreDNS는 kube-system namespace에 파드로 배포되어있다.  Deployment로 두개가 동작중이다.   CoreDNS는 Corefile이라는 파일을 이용한다.  여기에 kubernetes라는 plugin 구역을 보면 된다. root domain이 적혀있다. pods insecure 부분은 파드에 대한 record를 관리할지를 결정한다.  default로 disable이다.   proxy 부분은 dns에 등록되지 않은 것들을 /etc/resolv.conf에서 검색하도록 설정되어있다. 이 파일은 configmap으로 전달된다.   파드에서 DNS 서버에 접근할 떄는 service사용한다.  기본적으로 CoreDNS도 kube-dns 이름으로 사용한다. 이는 파드가 생성될 때 자동으로 IP를 기입한다. (/etc/resolv.conf) kubelet이 설정한다.  option으로 설정되어있음.   /etc/resolv.conf에 search 항목으로 default.svc.cluster.local, svc.cluster.local, cluster.local이 설정되어 있어서 간단하게 이름만 입력해도 근이 가능한 것이다.  그러나 pod는 정의되어있지 않아서 FQDN으로 접근해야한다.      Ingress  service와 ingress의 차이점은 무엇인가? 일반적으로 NodePort로 expose하면 30000 이상의 port(ex: 38080)를 사용하게 된다.  유저가 80포트를 통해 접속하도록(http) 하려면 proxy server를 둬서 트래픽을 80에서 38080으로 전달해야 한다. LoadBalancer를 사용할 때도 이와 비슷하다.   Ingress는 유저가 하나의 external accessble URL을 통해 클러스터내의 서비스에 접속할 수 있게 해준다.  https설정도 할 수 있다. ingress 또한 expose해야한다. NodePort 또는 LoadBalancer. 그러나 서비스가 늘어나도 한번만 수행하면 된다.   ingress controller, ingress resources로 ingress 설정을 한다.  default로 설치되어있지 않음. GCE와 Nginx는 쿠버네티스 프로젝트에서 관리한다.   nginx는 configmap으로 nginx의 설정을 한다.  ServiceAccount를 통해 적절한 권한을 줘야 한다.   ingress resource는 url에 따라 다른 파드로 트래픽을 전달할 수 있다. ingress에 backend로 serviceName과 servicePort를 지정하여 트래픽을 전달할 수 있다.  해당 ingress로 온 트래픽은 모두 serviceName으로 전달된다.   각각의 rule을 지정하여 원하는 서비스에 트래픽을 전달할 수 있다.  host를 따로 지정하지 않으면 모든 트래픽에 대해 매칭이 된다.    "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/08-storage/",
	"title": "08 Storage",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Introduction to Docker Storage  docker에는 두가지 컨셉의 stroage가 있다.  storage drivers volume drivers    Storage in Docker  file system  /var/lib/docker에 docker가 사용하는 파일들이 있다.   docker는 layered architecture라서 caching을 사용할 수 있다. 이렇게 해서 이미지를 실행시키면 해당 이미지에 사용된 layer들은 Read Only이다. 컨테이너 상에서 실행하는 것들은 container layer로, Read Write이다. Read Only상의 파일을 수정하면 도커는 자동으로 이를 복사해서 수정한다. 여기서 영구적으로 보관하고 싶은 것들이 있다면, volume을 사용해야한다. /var/lib/docker/volumes를 만드록 docker run -v \u0026lt;volume\u0026gt;:\u0026lt;path\u0026gt;의 형태로 사용한다.  absolue path를 통해서도 mount가 가능하다.   strorage driver는 layer를 관리한다. (mount)  Volume Driver Plugins in Docker  volume은 volume drivers가 할당한다. default = local (/var/lib/docker/volumes)  Container Storage Interface (CSI)  쿠버네티스에서 스토리지를 확장하는 방법. 쿠버네티스에서만 사용하는 규격이 아니다.  Volumes  파드에 volume을 할당해서 container의 파일을 저장할 수 있다. 원하는 stroage type을 사용해서 특정 스토리지 서비스를 이용할 수 있다.  Persistent Volumes  파드마다 volume을 할당하는 것은 매우 힘든 일이다. 좀 더 나은 방법: Persistent Volumes 사용하기  admin이 pv를 생성하고, pod는 필요시에 이를 가져다가 사용하기 persistent volume claim을 통해 요청한다.    Persistent Volume Claims  admin은 Persistent Volume을 생성하고, user는 Persistent Volume Claim을 생성하여 PV를 사용한다. PVC가 생성되면 쿠버네티스는 Bind 과정을 통해 알맞은 PV를 선택한다. 모든 PVC는 하나의 PV에 할당된다. label기반으로 bind도 할 수 있다. PVC를 했는데 bind될 것이 없으면 Pending 상태가 된다. default로 persistentVolumeReclaimPolicy=Retain이라서 PVC가 삭제되어도 PV의 데이터는 남아있다.  Reuse되지 않는다. Delete로 설정하면 자동으로 삭제된다. Recycle로 설정하면 다시 사용할 수 있지만 권장하지 않는다.    Tips  Storage Class나 StatefulSets에 관한 것은 CKAD의 범위.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/07-security/",
	"title": "07 Security",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Kubernetes Security Primitives  host와 cluster 자체의 security  당연히 host는 그 자체로 안전해야하고, root access는 disable 되어야 하며 password based authentication은 disabled되고 SSH key based authentication만 가능해야 한다. Kubernetes를 실행시키고 있는 호스트에 대한 physical, virtual infrastructure에 대한 보안도 필요하다.   쿠버네티스와 관련된 security  kube-apiserver는 모든것을 하는 주체가 되기 때문에 이곳에 대한 보안부터 챙겨야 한다. Who can access? / What can they do? API 서버에 누가 접근을 할 수 있게 할 것인가?  Files - Username and Passwords Files - Username and Tokens Certificates External Authentication providers - LDAP Service Accounts   What can they do  RBAC Authorization ABAC Authorization Node Authorization Webhook Mode   TLS Certificates  모든 controller component들은 TLS Encryption이 되어있다.   Cluster 내의 application에서의 communication  모든 파드는 클러스터 내의 모든 파드로 access 할 수 있다. Network Policies를 통해 access를 제한할 수 있다.      Authentication  Admin, Developers, End Users, Third Party가 클러스터에 접근한다. 내부 컴포넌트에서의 통신에 보안성을 추가하는 방법과 authentication, authorization 메카니즘으로 클러스터에 접근하는 것에 security management를 할 것이다. 그 중 authentication 메커니즘에 대해 집중할 것이다. End user에 대한 보안은 어플리케이션 내에서 해야한다. 두가지 방식으로 구분해보면 관리자, 개발자같은 사람인 User와 자동화 프로세스 등을 하는 로봇인 Service Accounts로 구분지을 수 있다. 쿠버네티스는 유저관리같은 것을 원래 지원하지는 않는다. 하지만 Service Account는 쿠버네티스가 지원한다. kube-apiserver는 authenticate user -\u0026gt; proccessing 의 순서로 동작한다. authenticate machanisms  Static Password File Static Token File Certificates Identity Services(LDAP)   static password, token file  kube-apiserver에 csv형태로 \u0026lt;password\u0026gt;,\u0026lt;user name\u0026gt;,\u0026lt;user ID\u0026gt;를 저장한뒤 전달하면 이를 사용할 수 있다.  --basic-auth-file=user-details.csv 옵션을 kube-apiserver에 추가하여 적용. restart 필요 kubeadm을 사용했다면 pod definition file을 수정해야 한다. 이는 curl 사용 시 -u \u0026quot;user1:password123\u0026quot;의 형태로 사용할 수 있다. csv 파일에 4번째 column을 추가할 수 있는데 이는 \u0026lt;group ID\u0026gt;를 의미한다.   이와 비슷하게 csv 형태로 \u0026lt;token\u0026gt;,\u0026lt;user name\u0026gt;,\u0026lt;user ID\u0026gt;,\u0026lt;group ID\u0026gt;를 저장할 수 있다.  --token-auth-file=user-details.csv 형태로 적용 가능 curl 사용 시 --header \u0026quot;Authorization: Bearer \u0026lt;token\u0026gt;\u0026quot; 형태로 적용이 가능.     static file로 설정하는 것은 추천하는 방법은 아니다. consider volume mount while providing the auth file in a kubeadm setup.  TLS Basics  transaction에서 두 party를 guarantee하기 위해 사용된다. user가 web server에 접근한다고 생각해보면 둘 간의 통신이 encrypt 되었음을 보장하기 위해 TLS를 사용한다. 데이터를 서버에 전송할 때 key를 통해 encrypt 되기 때문에 이를 받는 서버 또한 key를 가지고 있어야 복호화가 능하다.  여기서 암호화/복호화에 동일한 key를 사용하는 것을 Symmetric Encription이라고 한다. 해커가 key를 전송하는 과정 중에 이를 탈취해서 복호화할 가능성이 있다.   asymmetric encryption  private key와 public key 두개로 구성 간단하게 생각한다면 private key - public lock의 구성이라고 보면 된다. private key는 나만 가지고 있고 public key는 공개하여 누구나 암호화할 수 있도록 한다.   SSH를 asynmmetric으로 접근하기  ssh-keygen을 통해 private key와 public key를 생성한다. 서버를 public key를 통해 암호화한다.  public key를 서버에 추가하면 된다. ~/.ssh/authorized_keys에 추가. 그 뒤 ssh -i \u0026lt;private key file\u0026gt; user@server로 접속하면 된다.   더 많은 서버에 접속한다면 pulic key만 서버들에 대해 복사하면 된다. 다른 사람도 접근하고자 한다면 private key를 추가하면 된다.   symmetric encryption에서 key를 함께 보낼 때 key를 해커가 탐지할 수 있다.  이를 asymmetric encryption으로 key를 암호화한 뒤 보내면 해결 가능. private key 생성: openssl genrsa -out my-bank.key 1024 public key 생성: openssl rsa -in my-bank.key -pubout \u0026gt; mybank.pem   유저가 https로 서버에 최초 접근하면 public key를 서버로부터 받게 된다.  이 때 자신의 symmetric key를 서버의 Public key로 encryption한 뒤 전송한다. 이를 받으면 서버는 자신의 private key로 복호화를 진행한다. 해커가 이를 가져가도 어떤것을 할 수 없다. 이를 통해 서버와 유저는 symmetric key를 통해 서로 통신한다.   해커는 replica 서버를 만들어서 유저의 symmetric key를 탈취하려 할 것이다.  해커의 서버를 보면 key와 함께 certificate를 전송한다. 이 certificate 안에는 Issuer가 있다.  이는 그 서버의 identity의 유효성을 검증하는데 매우 중요하다.   Subject은 그 certificate에서 인증하는 서버를 나타낸다.   certificate는 누구나 만들 수 있기 때문에 누가 certificate를 sign하는지가 중요하다.  스스로 만들었다면 자신이 sign할 것이고 이를 self-signed certificates라고 한다. 이 self-signed certificates는 안전하지 않은 인증서이다. 따라서 해커가 사용하는 certificates도 self-signed certificates이다.   웹 브라우저는 자동으로 이런 self-signed certificates에 대해 경고를 해준다. 다른 3자에게 인증을 받고자 한다면 Certificates Authority(CA)가 필요하다.  이를 위해서는 Certificate Signing Request(CSR)을 전송해야한다. openssl req -new -key my-bank.key -out my-bank.csr -subj \u0026quot;/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com CA는 이를 적절히 확인하고 sign 후 응답을 준다. 해커가 CA로부터 인증을 받으려 하면 validation에서 fail이 날 것이다. 그러면 web browser에서 올바른 certificate임을 간주한다.   Web browser는 어떻게 CA가 올바른 CA라고 판정을 하는가  CA가 만약 fake CA라면? 어떻게 certificate에 서명된 CA가 진짜 CA라고 판정할 수 있는가. CA는 private key를 브라우저 내에 심어놓는다. 이를 통해 web browser는 자신에게 온 sign이 진짜 CA가 전송한 것인지 확인이 가능해진다.   PKI(Public Key Infrastructure) nameing  Public Key는 보통 *.crt, *.pem 형식이다. Private Key는 보통 *.key, *-key.pm  보통 key라는 단어를 섞는다.      TLS in Kubernetes - Certificate Creation  certificate 생성 방법: easyrsa, openssl, cfssl certificate 생성 절차:  public key 생성: openssl genrsa -out ca.key 2048 certificate signing request: openssl req -new -key ca.key -subj \u0026quot;/CN=KUBERNETES-CA\u0026quot; -out ca.csr sign certificates: openssl x509 -req -in ca.csr -singkey ca.key -out ca.crt   client certificate 생성 절차(admin user)  private key 생성: openssl genrsa -out admin.key 2048 certificate signing request: openssl req -new -key admin.key -subj \u0026quot;/CN=kube-admin/O=system:masters\u0026quot; -out admin.csr  반드시 kube-admin일 필요는 없고 kubectl에 사용되는 이름이면 된다.   sign certificates: openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt  ca.key로 certificate를 validate한다.   admin user라는 것을 certificate에 담으려면 group을 추가하면 된다.  system-master를 사용해야 한다. certificate sigining request에 /O=system:masters를 추가하여 나타냄.     system component들은 certificate에서 앞에 SYSTEM:이라는 prefix가 필요하다.  kube-scheduler kube controller manager kube-proxy   여기서 생성한 certificate를 사용하는 방법  curl \u0026lt;url\u0026gt; --key \u0026lt;private key\u0026gt; --cert \u0026lt;certificate\u0026gt; --cacert ca.crt 또는 kubeconfig에 이 내용을 추가한다.   CA certificate는 우리가 만든 것이기 때문에 파라미터로 넣을때 같이 넣어줘야 인증이 원할하게 된다. kube-apiserver는 IP, domain name이 다양하다.  kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local 이런 것들이 모두 certificate에 기록되어야 한다. 방법  certificate signing request시 config 파일을 두어 여기에다가 기록해둔다.  [req] req_extensions = v3_req [v3_req] basicConstraints = CA:FALSE keyUsage = nonRepudiation, subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 10.96.0.1 IP.2 = 172.17.0.87  그 다음 openssl req -new -key apiserver.key -subj \u0026quot;/CN=kube-apiserver\u0026quot; -out apiserver.csr -config openssl.cnf처럼 config 파일을 지정한다.     kubelet의 경우 노드마다 떠있게 되는데 여기에 대한 certificate의 name은 노드의 이름을 따온다.  kubelet-config.yaml에 certificate에 대한 정보를 지정한다. 또한 system:node: prefix를 넣는다. 그리고 SYSTEM:NODES 그룹에 넣어야 한다.    View Certificate Details  이미 설치된 클러스터 내에서 certificate을 확인하는 방법 쿠버네티스가 어떤 방식으로 설치되어 있는지 알아야 한다. scartch로 구성을 할 경우 모든 certificate을 스스로 생성한다. kubeadm같은 automation tool을 이용할 경우 모든 certificate을 알아서 구성해준다. kubeadm  /etc/kubernetes/manifests/kube-apiserver.yaml 여기서 --client-ca-file, --etcd-cafile, --etcd-certfile, --etcd-keyfile, --kubelet-client-certificate, --kubelet-client-key, --tls-cert-file, --tls-private-key 확인. --tls-cert-file=/etc/kubernetes/pki/apiserver.crt로 설정되어 있을 경우 다음의 명령어로 조회 가능.  openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout     journalctl -u etcd.service -l로 로그 확인  Certificates API  새로운 admin이 들어오면, Certificates Request를 작성하게 한 뒤 기존 admin에게 전송하고, 이를 CA 서버에 전달하여 certificate을 생성한 뒤 되돌려준다.  이 경우 certificate이 expire되면 다시 이 프로세스대로 반복해야한다.   CA 서버가 있으면 여기에 접속할 수 있는 사람들이 certificate를 마음껏 생성할 수 있다.  매우 안전한 서버에다가 구성해야한다. certificate에 접속하고자 할 경우 반드시 그 서버를 통해야 한다.   쿠버네티스는 Certificate API를 보유하고 있어서 다음과 같은 작업을 할 수 있다.  admin이 Certificate Singing Request를 받으면 마스터 노드에 접속해서 직접 sign을 받는 것이 아니라, API를 통해서 한다. CertificateSigningRequest Object를 생성한다. 이를 통해 Request를 받고 Approve한 뒤 User에게 Certificate를 전송한다.   Flow  openssl genrsa -out jane.key 2048 openssl req -new -key jane.key -subj \u0026quot;/CN=jane\u0026quot; -out jane.csr 이를 base64로 인코딩해서 request에 담는다. kubectl get csr을 통해 조회해보면 status.certificates에 결과가 나온다. 이 결과를 base64로 decode한다.   csr에 관련된 것은 controller manager에서 관리한다.  Kubeconfig  일반적으로 curl 명령어를 통해 kube-apiserver와 통신을 할 수 있다. * curl https://my-kube-playground:6443/api/v1/pods \\  --key admin.key \\  --cert admin.crt \\  --cacert ca.crt  kubectl을 이용할 때는 다음과 같이 사용할 수 있다. * kubectl get pods --server my-kube-playground:6443 \\  --client-key admin.key \\  --client-certificate admin.crt \\  --certificate-authority ca.crt  이를 매번 치는 것은 귀찮으니 이를 kubeconfig 파일에 넣을 수 있다.   kubeconfig의 default directory는 $HOME/.kube/config 3가지 섹션으로 나뉨.  cluster  접속할 클러스터   users  클러스터에 접속하는 유저   contexts  클러스터와 유저를 연결하는 것     이미 존재하는 user를 골라야 한다. kubeconfig에서 contexts에 특정한 네임스페이스를 사용하도록 지정할 수 있다.  API Groups  쿠버네티스에서 api는 각각의 목적에 따라 그룹으로 구성되어 있다. 크게 core 그룹과 named 그룹으로 나뉜다. core group  /api -\u0026gt; /v1 -\u0026gt; namespaces, pods \u0026hellip;   새로운 feature같은 것들은 named 그룹  /apis -\u0026gt; /apps, extensions, /networking.k8s.io, storage.k8s.io, /authentication.k8s.io, certificates.k8s.io 이 /apps, /extensions 등을 API group이라고 한다. 그 아래에 해당하는 api들을 resources라고 부르고, 각 resource는 특정한 verbs를 가지고 있다.   api 문서를 보면 그룹에 관한 자세한 정보가 있다 kube-apiserver로 요청을 보내면 사용할 수 있는 api 버전들을 보여준다. 그러나 certificates들을 기입해야 한다. 이를 안하려면 kubectl proxy를 사용하면 된다. 그러면 certificates를 사용하지 않을 수 있다. kubeproxy와 kubectl proxy는 다른 것이다.  Role Based Access Controls  kind=Role을 통해 어떤 api group, resources, verbs를 사용할 수 있는지 정의할 수 있다. core 그룹을 사용하려면 api group을 공백으로 두면 된다. 이렇게 정의한 role을 유저에게 적용시키려면 kind=RoleBinding이라는 것을 정의하면 된다. kubectl get roles 및 kubectl get rolebindings를 통해 확인할 수 있다. kubectl auth can-i를 통해 어떤 것을 할 수 있는지 확인할 수 있다.  --as를 통해 user를 특정할 수 있다. --namespace를 통해 네임스페이스에 관해서도 볼 수 있다.   resourceNames 필드를 통해 role이 적용되는 resource의 이름을 지정할 수 있다.  Cluster Roles  Role과 RoleBindings는 네임스페이스 아래에 있다. 네임스페이스를 지정하지 않으면 default 네임스페이스에 대해서 적용된다. 노드는 clusterwide resource이기 때문에 특정 네임스페이스에 대해서 지정할 수 없다. 리소스는 네임스페이스 또는 클러스터 스코프로 정의된다. cluster scoped  nodes, PV, clusterroles, clusterrolebindings, certificatesigningrequests namespaces   kubectl api-resources --namespaced=true, kubectl api-resources --namespaced=false cluster wide인 리소스에 대해서 Role과 RoleBindings를 설정하는 방법은? ClusterRole을 설정한다.  이걸로 전체 네임스페이스에 관한 resource에 대한 권한을 가지기 위해 설정할 수도 있다.    Image Security  private repository를 구축해서 외부로 노출되지 않은 이미지 저장소를 만들 수 있다. image path를 full path를 써서 사용하면 된다. credential이 필요하다면 secret을 만들어 docker-registry type을 사용하면 된다. 이는 built-in type이다.  이를 podSpec의 imagePullSecrets에 추가하면 된다.    Security Contexts  securityContexts를 지정하면 기존 docker image에 정의된 security context를 overwrite할 수 있다.  capability를 설정하여 특정한 capability를 부여할 수 있다. 이는 파드가 아닌 컨테이너 단위로만 설정 가능하다.    Network Policy  web - api - db가 있을 때 웹을 기준으로 user로부터 들어오는 트래픽을 ingress traffic, app server로 나가는 트래픽을 egress traffic이라고 한다.  요청에 대한 응답은 크게 관련 없음.   파드는 라우팅같은 세팅을 통해 어떤 파드에도 접근할 수 있어야 한다. network policy는 리소스로, 파드에 대해 특정한 룰을 통과하는 트래픽만 흐르게 해주고 나머지는 블럭처리한다. labels로 network policy와 pod를 연결시킨다. Flannel은 network policies를 지원하지 않는다.  Tips  Service Account는 CKAD의 영역.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/06-cluster-maintenance/",
	"title": "06 Cluster Maintenance",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "OS Upgrades  Node가 Down된 상태에서 5분이 지나면 pod는 terminate가 된다.  Dead로 간주   ReplicaSet으로 관리되면 다른곳에 파드를 띄워준다. kube-controller-manager에 pod-eviction-timeout이 기본적으로 5분이 설정되어 있다. 다시 online으로 오게되면 blank node로 뜨게 된다. ReplicaSet으로 관리되지 않았던 파드는 삭제되고 재생성되지 않는다. 따라서 kubectl drain node를 통해 파드를 이주시키고 cordon을 통해 스케줄되지 않도록 한다. 그 다음에 노드를 down시키고 살리면 된다. 산 뒤에도 이전에 처리한 cordon이 남아있게 되는데 이를 삭제하기 위해선 kubectl uncordon을 해야한다. 자동으로 이주되었던 파드가 돌아오지 않는다. cordon은 unschedulable로 처리하는 작업이다.  Kubernetes Software Versions  Release Number는 3가지로 구분  MAJOR, MINOR, PATCH MINOR 버전은 Feature, Functionalities 위주 PATCH는 Bug Fixes 위주   stable한 버전(MINOR의 업그레이드)는 매달 일어남. alpha 버전은 default로 disable되어 있는 feature. 버그 등이 있을 수 있음 beta 버전은 잘 테스트 되었고 default로 enable되어 있는 feature. stable로 통합됨. release package에는 모든 controller component가 패키징되어있음.(동일한 버전으로) ETCD cluster나 CoreDNS같은 controller component가 아닌 것은 포함되어있지 않음.  supported version을 명시해둠.    Cluster Upgrade Process  웬만하면 Controller Component는 동일한 버전을 가지고 있어야 한다. kube-apiserver가 1.x 버전이면 controller-manager와 kube-scheduler는 1.x ~ 1.x-1 버전이 될 수 있다.  kubelet과 kube-proxy는 1.x-2 ~ 1.x kubectl은 1.x-1 ~ 1.x+1   kubernetes는 최근 3버전만 support한다. 업그레이드 시 MINOR 버전 하나씩 업그레이드 하는 것을 권장한다. Cloud provider로 관리되고 있으면 클릭만 해서 업그레이드가 가능하다. kubeadm과 같은 툴을 사용할 경우 tool에 내장된 업그레이드 플랜을 사용하면 된다. scratch로 구성했을 경우 수작업으로 업그레이드 해야한다. 강의에서는 kubeadm 사용. 클러스터 업그레이드는 두가지 major step으로 나뉨.  마스터 노드 업그레이드  업그레이드 시 Control plane component들 (apiserver, scheduler, controller manager)는 잠시 끊긴다. 그렇다고 해서 워커노드에 영향이 있다는 것을 의미하지는 않는다. 마스터 노드가 다운이 되기 떄문에 클러스터에 접속이 불가능하다. 마스터 노드가 업그레이드 되더라도 워커노드들의 controller component와의 통신이 지원된다.(버전 하나차이)   워커 노드 업그레이드  한번에 업그레이드 하기  유저가 접속할 수 없음. 다운타임 발생   하나씩 업그레이드 하기  다운타임 없음.   새 버전의 노드 새로 추가  public cloud 사용할 떄 좋음. 하나씩 추가.       kubeadm은 upgrade 명령어가 있음. kubeadm upgrade plan  kubeadm은 kubelet의 업그레이드를 하지 않음. 따라서 스스로 업그레이드 해야함. 또한 kubeadm의 업그레이드를 먼저 해야함. apt-get upgrade 명령어시 다른 패키지도 업그레이드가 되기 때문에 apt-get install 명령어로 단일 패키지만 업그레이드 시킨다. apt-get install -y kubeadm=\u0026lt;version\u0026gt; kubeadm upgrade apply \u0026lt;version\u0026gt; 이후에도 여전히 kubectl get nodes로 조회한 것은 이전 버전이 나온다. 이는 kubelet의 버전을 표시하기 때문이다. 설치 방법에 따라 kubelet이 설치 되었을수도, 아닐수도 있다. scratch 방식으로 설치했을 떄에는 kubelet을 설치하지 않는다. apt-get install -y kubelet=\u0026lt;version\u0026gt; systemctl restart kubelet kubectl get nodes시 업그레이드된 버전이 보임. 워커노드의 업그레이드  워커노드를 업그레이드 하려면 drain을 통해 노드를 비워야 한다. cordon을 통해 unschedulable로 변경 apt-get install -y kubeadm=\u0026lt;version\u0026gt; apt-get install -y kubelet=\u0026lt;version\u0026gt; kubeadm upgrade node config --kubelet-version \u0026lt;version\u0026gt; systemctl restart kubelet 이를 각 노드를 순회하며 실행한다.     apt-mark kubelet kubectl kubeadm을 통해 자동업그레이드가 안되도록 설정한다.  Backup and Restore Methods  Backup Candidates  Resource Configuration ETCD Cluster Persistent Volumes   Resource Configuration  Declarative방식으로 사용하여 저장할 것을 권장. yaml파일로 구성하는 것. 이런 파일을 github등으로 저장할 것. kube-apiserver로부터 모든 리소스를 가져와서 백업할 수도 있음. kubectl get all --all-namespace -o yaml \u0026gt; all-deploy-services.yaml 그러나 몇가지 리소스는 빠져있음. VELERO라는 툴로 모든 리소스를 저장할 수 있음.   ETCD Cluster  cluster의 state를 저장한다. master node에 배포된다. --data-dir=/var/lib/etcd의 형태로 모든 데이터를 저장하는 path를 지정한다. Backup  ETCD는 etcdctl에서 snapshot command를 제공한다. backup하기 위해서는 service kube-apiserver stop을 해야한다.  ETCD cluster를 restart를 하게 되는데 kube-apiserver가 이에 대해 의존성을 가지고 있기 때문이다.     Restore  etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup ... --initial-cluster-token etcd-cluster-1 ... 이 정보들은 original file에서 가져와야 한다. 여기서 설정한 --data-dir과 --initial-cluster-token으로 기존의 etcd.service를 채워야 한다. systemctl daemon-reload를 통해 설정을 다시 읽는다 systemctl etcd restart service kube-apiserver start     ETCD cluster에 접근할 수 없으면 kube-apiserver를 통한 백업이 가장 나은 방법이다. 두 방법 모두 장/단점이 있다.  Working with ETCDCTL  실습에서 ETCD는 master 노드에서 static pod로 v3가 배포되어 있다. backup과 restore를 위해서는 ETCDCTL_API=3을 설정해야 한다. 환경변수로 추가하면 된다. TLS 기반으로 만들어진 ETCD Cluster이기 떄문에 --cacert, --cert, --endpoints=[127.0.0.1:2379], --key는 필수이다.  Tips  실제 시험 환경에서는 어떤것이 문제인지 알 수 없으니 kubectl describe pod를 통해 디버깅을 해야한다. https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster/ https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md/ https://www.youtube.com/watch?v=qRPNuT080Hk/  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/05-application-lifecycle-management/",
	"title": "05 Application Lifecycle Management",
	"tags": ["kubernetes", "cks"],
	"description": "",
	"content": "Rolling Updates and Rollbacks  kubectl rollout status deployment/myapp-deployment로 롤아웃 상태 확인 가능 kubectl rollout history deployment/myapp-deployment로 히스토리 확인 가능 Recreate: old를 모두 죽인 뒤 new를 생성 Rolling Update: old를 하나씩 죽이고 하나씩 new를 생성 kubectl set image deployment myapp-deployment nginx=nginx:1.9.1로 이미지 수정 가능 StrategyType이 Recreate면 old의 replica가 모두 0으로 줄고난 뒤 new의 replica를 늘린다. 반면 RollingUpdate일 경우 old를 하나씩 죽이고 new를 하나씩 늘린다. Rollback을 할때는 반대로 동작한다. kubectl run을 통해 파드를 생성하면 deployment가 생성된다.  Commands  CKA에는 굳이 필요 없을 수 있다.. container는 OS를 구동하려고 만들어진 것이 아니라 특정한 태스크를 실행하기 위해 만들어졌다.  앱이 crash가 나면 컨테이너는 종료된다.   Dockerfile을 보면 CMD로 어떤 프로세스를 시작할지 결정한다. 도커는 기본적으로 컨테이너를 시작할 때 터미널 연결을 하지 않는다. 따라서 CMD가 bash일 경우 종료된다. CMD에 입력할 때에는 executable command와 parameter를 하나의 리스트안에 담으면 안된다.  CMD[\u0026quot;sleep\u0026quot;, \u0026quot;5\u0026quot;] vs CMD[\u0026quot;sleep 5\u0026quot;] CMD는 overwrite되는 값이다.   ENTRYPOINT는 CMD와 미슷하지만, 도커 실행시 뒤에 적는 추가적인 글자들은 entrypoint의 parameter가 되어 들어간다.  appending을 하지 않으면? sleep만 전달된다. 따라서 default를 넣고 싶다면 ENTRYPOINT와 CMD를 함께 쓴다. 아예 덮어쓰고 싶다면 docker run --entrypoint ... 형식으로 entrypoint를 지정한다.    Commands and Arguments  파드에 arguments를 넣고 싶으면 pod.spec.containers[].args에 넣으면 된다. Dockerfile에서 CMD는 default parameter이다. args는 Dockerfile에서 CMD를 변경한다. ENTRYPOINT를 수정하고 싶다면 command를 수정하면 된다.  Configure Environment Variables in Applications  환경변수를 넣으려면 env에 넣으면 되고 이는 array이다. key-value pair로 넣을수도 있지만 ConfigMap이나 Secrets에서 value를 가져올 수도 있다.  valueFrom으로 가져온다.    Configuring ConfigMaps in Applications  ConfigMap은 key-value pair data를 저장하는 리소스이다. 파드가 생성되면 ConfigMap 안에 있는 정보들이 environment로 들어가서 파드 내에서 사용할 수 있게 된다. 사용법은 먼저 생성하고 이를 파드 내에 inject하는 것이다. 간단 생성: kubectl create configmap \u0026lt;config-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; 파일에서: kubectl create configmap \u0026lt;config-name\u0026gt; --from-file=\u0026lt;path-to-file\u0026gt; 환경변수 전체를 가져올때 사용가능, 또는 단일 value만을 가져올 수도 있음 아니면 volumes에서 configMap에 있는 데이터를 통해 volume mount도 가능.  Configure Secrets in Applications  app 내에서 사용하는 값들을 configMap으로 관리할 수 있다. 여기서 공개하기 어려운 값들이 있을 수 있는데 이를 Secret으로 관리한다. 이게 Environment Value로 들어간다. 사용법: 생성하고 파드에 inject kubectl create secret generic \u0026lt;secret-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; kubectl create -f 로도 생성 가능.  key-value pair 그러나 그냥 저장하면 보안 취약. encode할 것. kubectl describe secrets로 조회되지 않음. kubectl get secrets -o yaml으로 조회해야 함. hash value로 나옴. (base64)   환경변수, 단일 ENV, Volume으로 마운트도 가능. 각 Key값에 해당하는 파일이 생성되며 그 내용이 value값이다. 그러나 secret은 그 자체로 보안이 뛰어난 것이 아니다. 보호하는 방법  secret은 vcs로 관리하지 않기 ETCD에 암호화해서 저장하기   참조:  https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ https://blog.outsider.ne.kr/1266/    Multi Container PODs  micro service로 변경하면서 작은 단위로 코드를 관리하고 배포할 수 있게 되었다. scale up, scale down을 각 마이크로 서비스 단위로 할 수 있다. 그 중에서 두 pair가 반드시 함께해야하는 경우가 있을 수 있다. 개발은 따로 하지만 항상 짝을 이뤄야 하는 경우 multi container를 사용하면 된다. 같은 lifecycle을 가짐. localhost로 통신 가능. 같은 storage volume에 접근. 따라서 volume sharing이나 파드간 서비스를 설정하지 않아도 된다.  Multi-container PODs Design Patterns  logging같은 것을 하기 위해 sidecar pattern으로 multicontainer 사용하기 adpater 패턴 ambassador 패턴 이 내용은 CKAD에서.  initContainers  파드 내의 컨테이너들은 함께 죽고 산다. 이 중 하나라도 fail이 되면 파드는 재시작된다. 파드가 생성되고나서 한번만 실행하고 싶은 동작들은 initContainers로 관리할 수 있다. 다른 컨테이너처럼 initContainers 안에서 설정이 가능하다. 여러개의 initContainer가 있으면 순서대로 실행하고, 순서대로 성공해야한다. 하나의 initContainer라도 실패하게 되면 파드를 계속해서 재시작한다.  Tips  readiness, liveness는 CKAD에서  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/04-logging-and-monitoring/",
	"title": "04 Logging and Monitoring",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Monitor Cluster Componets  쿠버네티스에서 자체 제공하는 것은 없으나 다음과 같은 것들이 있다.  Metrics Server Prometheus Elastic Stack DataDog Dynatrace   metric server는 각 쿠버네티스의 노드와 파드의 메트릭을 모아서 메모리에 저장한다. metric server는 유일한 in-memory monitoring solution이다.  데이터를 저장하지 않아서 이전 자료를 보지 못한다.   kubelet은 cAdvisor를 포함한다.  파드로부터 퍼포먼스 메트릭을 수집하여 메트릭 서버로 전송한다.   metric server가 설치되면 kubectl top node, kubectl top pods를 사용하여 메트릭을 볼 수 있다.  Managing Application Logs  도커에서 stdout으로 로깅을 하는 상황  docker logs 명령으로 로그를 볼 수 있음.   파드에 여러개의 컨테이너가 있으면 하나를 지정해야 로그를 볼 수 있다.  "
},
{
	"uri": "http://kimmj.github.io/jenkins/workspace-list/",
	"title": "Workspace@2를 변경하기 - Workspace List 설정 변경",
	"tags": ["jenkins", "workspace"],
	"description": "",
	"content": "운용하는 노드에 executor가 2개 이상이라면, concurrent build 옵션을 disable했다고 하더라도 zombie process가 있을 경우 workspace@2처럼 @ 캐릭터가 들어간 workspace를 사용할 수 있다.\nworkspace 안에 특정한 파일을 넣고 사용하는 경우라면 @2가 생기면 안될 것이다. 그러나 이는 그렇게 좋은 방법은 아닌것 같으며 이런 경우에는 github 등에 스크립트같은 파일을 옮겨놓고 git pull이나 git 관련 플러그인을 통해 다운로드 한 뒤 사용하는 것이 더 좋은 방법인 것 같다.\n하지만 나의 경우 Perforce를 사용하고 있었느데 p4 sync에서 문제가 발생했다. @라는 문자가 없어야 한다는 것이었다. 사실 저 문자만 없다면, workspace의 폴더 이름이 무엇이든 상관이 없었기 때문에 @대신 다른 캐릭터를 사용하기로 결정했다.\nJAVA_OPTS= '-Dhudson.slaves.WorkspaceList=A'를 추가하면 된다. 내 경우 docker-compose를 통해 jenkins를 설치했으므로 다음과 같이 작성했다.\nenvironments: JAVA_OPTS= \u0026#39;-Dhudson.slaves.WorkspaceList=_\u0026#39; 파일에서 envieonments의 정확한 위치는 때에 따라 다를 수 있으므로 검색을 통해 environments의 정확한 위치를 확인해보고 설정하면 될 것 이다. 위와 같이 설정하면 두번째 workspace는 workspace_2와 같이 생길 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/03-scheduling/",
	"title": "03 Scheduling",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Manual Scheduling  how scheduling works  podSpec에 nodeName 필드를 채워넣으면 해당 노드로 파드가 뜬다. default로는 비워져있음. 스케줄링은 알고리즘에 의해 파드를 띄울 노드를 선택하고 나면 nodeName 필드를 채운다. 스케줄러가 없으면 파드가 계속해서 pending 상태에 있게 된다. 따라서 스케줄러가 없으면 단순히 nodeName을 채우면 될 것이다. runtime에 nodeName을 변경할 수 있는데, 이는 kind=Binding object를 binding API로 POST 요청을 보내는 방식으로 가능하다. binding definition에 target.name=\u0026lt;NodeName\u0026gt;으로 설정하면 된다. 이를 JSON 형식으로 보내면 된다.  따라서 yaml을 json으로 변경해야한다.      Labels and Selectors  각 리소스에 대해 label로 속성을 부여하고 selector로 선택할 수 있다. metadata.labels에 key-value 형태로 제공 ReplicaSet을 예로 들 때, spec.template.metadata.labels에 있는 정보는 파드에 대한 label이다.  metadata.labels에 있는 레이블은 ReplicaSet에 대한 label이다. spec.selector.matchLabels은 생성할 파드와 연결해주는 것이다.   label와 selector는 object를 그룹화하고 선택할 때 사용된다.  annotations는 inflammatory purpose를 위해 기록되는 것이다. integration purpose로도 사용됨.    Taints and Tolerations   사람이 벌레를 퇴치하는 것에 비유\n 벌레가 싫어하는 스프레이를 사람에게 뿌리면 사람은 taint 처리가 됨. 벌레는 이를 견디지 못해서 사람에게 갈 수 없다. (intolerant) 만약 이 스프레이에 내성이 있는 벌레가 있다면 사람에게 다가갈 수 있을 것이다. (tolerant)    즉, 사람에게 taint 처리를 하고 벌레에게 해당 taint에 대해 tolerant 속성을 주면 벌레가 사람에게 다가갈 수 있다.\n  Taints: kubectl taint nodes node-name key=value:taint-effect\n taint-effect에는 NoSchedule,PreferNoSchedule,NoExecute가 있다. kubectl taint nodes node1 app=blue:NoSchedule    podSpec에서 다음과 같이 설정\nspec: tolerations: - key: \u0026#34;app\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;blue\u0026#34; effect: \u0026#34;NoSchedule\u0026#34;   taint, toleration 설정을 했다고 해서 파드가 항상 taint쪽으로 가는 것은 아니다.\n  toleration이 없는 파드들이 taint 처리된 노드로 뜨지 못할 뿐이다.\n  파드를 특정 노드로 할당하고 싶다면 이는 node affinity를 사용해야 한다.\n  kubectl describe node node-name | grep Taint\n  Node Selectors  특정한 노드로 파드가 뜨게 하는 방법에는 여러개가 있다.  nodeSelector 사용하기  nodeSelector는 key=value를 사용하고, 이는 노드에 할당된 label이다. 제약사항: equal 상황에서만 사용할 수 있음      Node Affinity  spec.affinity.nodeAffinity에 지정  requiredDuringSchedulingIgnoreDuringExecution  nodeSelectorTerms  matchExpressions  In을 사용하면 key값에 대해 복수개의 value중 만족하는 것이 있는 경우에 적용할 수 있다. NotIn을 사용하여 key=value를 만족하지 않는 것에 대해 적용할 수 있다. Exists를 사용하여 key값이 있는지 확인할 수 있다. value가 필요 없음.         requiredDuringSchedulingIgnoreDuringExecution과 preferredDuringSchedulingIgnoreDuringExecution이 있음. required는 스케쥴링 시 없으면 기다림. preferred는 없으면 딴데 띄움. DuringExecution의 경우 Ignore만 있으며 노드의 label이 삭제되더라도 evict 하지 않음.  Taints and Tolerations vs Node Affinity  Taints \u0026amp; Tolerations만 설정할 경우 뜨기 희망하지 않는 다른 노드에 뜰 수도 있다. Node Affinity만 설정하면 다른 파드가 나의 노드에 뜰 수도 있다. 따라서 둘 다 사용해야 우리가 원하는 어떤 파드만 어떤 노드에 뜰 수 있도록 하는 상황을 이용할 수 있을 것이다.  Resource Requirements and Limits  노드에 자원이 충요하지 않으면 pending이 된다. Resource Request: Minimum Requirement 1 CPU라는 의미는 1vCPU와 같다.  AWS에서는 1vCPU, GCP와 Azure에서는 1Core, 또는 1 Hyperthread. Mi 처럼 i가 들어가는 것들은 2^10 단위, M만 들어가는 것은 10^3 단위   도커는 원래 노드의 cpu를 모두 사용할 수 있을 정도로 cpu 사용량에 제한이 없다. default로 쿠버네티스는 컨테이너에 대해 1vCPU 제약을 둔다. 메모리 또한 같다. 512Mi가 default vCPU의 경우 쿠버네티스가 CPU 사용량에 제한을 둘 수 있다. 하지만 Memory의 경우 그럴 수 없기 떄문에 초과하면 죽인다. 기본으로 Request와 Limit을 설정하는 것들은 LimitRange를 설정하면 된다.  Edit POD and Deployments  이미 있는 파드에 대해 다음의 필드 이외에 수정이 불가능.  spec.containers[*].image spec.initContainers[*].image spec.activeDeadlineSeconds spec.tolerations   Deployment 안에 있는 파드스펙은 수정이 모두 수정이 가능하며 자동으로 파드를 삭제하고 새로 띄운다.  DaemonSets  DaemonSets은 ReplicaSet과 유사. 각 노드당 하나의 파드를 띄울 수 있도록 한다. 클러스터의 모든 노드에 최소 하나의 파드가 떠있는 것을 보장한다. 대표적으로 kube-proxy가 있다. ReplicaSet과 작성방법이 유사. 작동 원리  파드를 생성하면서 각 노드에 해당하는 nodeName을 설정한다. 1.12까지 이런식으로 사용되었다. 1.12부터 default schedular와 node affinity를 사용하도록 변경되었다.    Static Pods  kube-apiserver, kube-scheduler, controller, ETCD cluster가 없다면? master 조차 없다면? 하나의 worker node만 있을 때 kubelet이 할 수 있는 것이 있을까? kubelet은 노드를 독립적으로 관리할 수 있다. kubelet이 아는것은 파드를 생성하는 것 뿐이다. kube-apiserver 없이 kubelet에게 어떻게 podSpec을 전달할 수 있을까? 노드의 /etc/kubernetes/manifests에 파드 yaml을 넣으면 된다.  kubelet은 이 디렉토리를 주기적으로 감지하여 파드를 생성한다. 또한 파드가 살아있는지 확인한다. 파드가 죽으면 재시동한다. 파일에 변경사항이 있으면 쿠버네티스는 파드를 다시 생성한다. 디렉토리에서 파일을 삭제하면 파드도 삭제된다. 즉, API Server에 의해 관리되지 않는 파드이다. 이를 Static POD라고 한다. 이 방법으로 파드만 생성해야한다. replicaset이나 deplpoyment, service는 생성할 수 없음. kubelet은 파드만 관리할 수 있어서 파드만 생성가능하다. --pod-manifest-path로 kubelet을 시작할 때 인자로 넣어주어야 한다. --config 옵션으로 yaml 파일을 넣을 수 있다.  그 안에서 staticPodPath를 통해 디렉토리를 넣을 수 있다.     다른 워커/마스터가 있더라도 static pod 사용 가능.  kubectl로도 볼 수 있음. kubelet은 클러스터일 때 static pod에 대한 것을 kube-apiserver에도 미러링한다. 단, kube-apiserver는 read only이다. 수정 또는 삭제는 불가능하며 노드의 manifest folder를 수정해야만 한다. 파드의 이름은 뒤에 자동으로 노드이름이 추가된다.   static pod는 kuberentes control plane에 의존하지 않음. 따라서 control plane 그 자체를 배포할 때 사용한다. control plane들을 pod manifest path에 넣으면 kubelet이 알아서 생성해준다. 이것이 kubeadm 툴이 클러스터를 생성하는 방법이다. DaemonSet은 클러스터의 모든 노드에 파드를 띄우는 방법  control plane의 개입이 있음.   Static pod는 kubelet이 자체생성하는 것  Multiple Schedulers  쿠버네티스는 여러개의 스케줄러를 동시에 가지고 있을 수 있다. 파드나 Deployment를 생성할 때 쿠버네티스에게 특정 스케줄러를 사용하도록 지정할 수 있다. kube-scheduler binary를 다운로드하고 이를 옵션을 통해 서비스로서 동작하도록 실행한다. kube-scheduler는 스케줄러 이름을 결정하는 것이고 default-scheduler가 default이다. kubeadm은 kube-scheduler를 파드 형태로 배포한다.  manifest 폴더에서 확인 가능. 여기서 파일을 복사하고 --scheduler-name을 내 스케줄러의 이름으로 변경하면 됨.   multimaster 상황에서 HA 구성이 되어있을 경우 --lock-object-name을 통해 leader-election을 조정한다? podSpec에서 shedulerName에 scheduler의 이름을 명시하면 해당 스케줄러를 사용한다. kubectl get events를 통해 어떤 scheduler가 사용되었는지 알 수 있다. scheduler의 로그는 스케줄러 파드의 로그 확인한다.  Tips   cd /etc/systemd/system/kubelet.service.d/ cat 10-kubeadm.conf 여기에서 KUBELET_CONFIG_ARGS를 확인할 수 있다.\n  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/02-core-concepts/",
	"title": "02 Core Concepts",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": " Controller는 Kubernetes의 brain과 같다.  ReplicaSets  Pod가 죽게 되면 사용자가 접근을 할 수 없게 된다. 따라서 여러개의 파드를 띄워 하나가 죽어도 나머지가 동작하도록 해야한다. Replication Controller는 여러개의 파드를 띄울 수 있도록 도와준다. 이를 High Availability라고 한다. 하나의 파드만 관리한다고 해서 쓸모없는게 아니라 이는 하나가 죽으면 다시 하나를 실행시키는 방식으로 동작한다. 로드가 늘어나면 파드를 늘릴 수 있다. Replica Controller와 Replica Set은 비슷하지만 다르다.  Replica Controller는 Replica Set으로 대체되었다.   Replica Controller  apiVersion=v1 spec.template에 파드의 스펙을 정의한다.  파드를 정의할때의 yaml에서 apiVersion과 kind같은것만 빼고 나머지 정의들을 spec.template에 적으면 사용가능하다.     Replica Set  apiVersion=apps/v1 selector를 작성해야한다. -\u0026gt; Replica Controller과의 차이점. 어떤 파드를 컨트롤하는지 적어야한다. label을 적어서 apply시 생성했던 파드가 아니더라도 관리할 수 있도록 한다.    Deployments  roll back, rolling upgrade 각 파드는 Replica Set으로 관리된다.  이를 Deployment가 감싼다. rolling upgrade, roll back, scale out, pause, resume 등을 사용할 수 있음   Definition  ReplicaSet에서 Deployment로만 변경하면 된다. ReplicaSet과 Deployment는 크게 다르지 않다.   commands  Certification Tips  kubectl run을 이용하여 yaml template을 생성하라. Create an NGINX pod  kubectl run --generator=run-pod/v1 nginx --image=nginx   Generate POD Manifest YAML file (-o yaml). Don't create it(\u0026ndash;dry-run)  kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml   Create a deployment  kubectl create deployment --image=nginx nginx   Generate Deployment YAML file (-o yaml). Don't create it(\u0026ndash;dry-run) with 4 Replicas (\u0026ndash;replicas=4)  kubectl create deployment --image=nginx nginx --dry-run -o yaml \u0026gt; nginx-deployment.yaml Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment 즉, deployment를 create할 때는 먼저 생성해놓고 replicas를 조정해야한다. (--replicas 옵션이 없음)    Namespaces  각 네임스페이스에는 그 안에서 소비할 수 있는 리소스들이 있다. default 네임스페이스는 쿠버네티스가 처음 만들어질 때 생성되는 것. 쿠버네티스는 네트워킹이나 DNS와 같은 내부적인 목적으로 파드와 서비스를 만든다. 이러한 것들을 유저가 사용하는 공간과 다르게 두어 실수로 삭제하지 않도록 만들어준다.  kube-system이다.   kube-public은 모든 유저가 사용할 수 있는 리소스들이 있다. 작은 공간에서 사용할 경우 default만 써도 되지만 큰 기업으로 가면 네임스페이스를 고려해야 한다. 클러스터 내에서 dev와 prod를 다른 네임스페이스를 두고 만들수도 있다. 각 네임스페이스는 누가 무엇을 할 수 있는지에 관한 정책을 가지고 있다. 또한 네임스페이스별로 리소스 쿼터를 설정하여 최소한의 서비스를 할 수 있도록 한다. 네임스페이스에 내에 있는 리소스끼리는 이름만 가지고 접근이 가능하다. 다른 네임스페이스에 있는 것에 접근하려면 네임스페이스를 알려주어야 한다.  servicename.namespace.svc.cluster.local   이러한 형태로 dns가 설정되기 때문. cluster.local: domain svc: subdomain, service yaml이 특정 네임스페이스에만 뜰 수 있도록 하고싶다면 이를 manifest에 옮기면 된다. 기본 네임스페이스를 변경하고 싶으면 kubectl config set-context $(kubectl config current-context) --namespace=dev 와 같은 방식으로 조정 리소스 쿼터는 kind=ResourceQuota로 설정하면 된다.  Service  내 외부의 다양한 컴포넌트와 통신을 가능하게 해준다. 어플리케이션을 loose-coupling하게 해준다. 클러스터 내부의 파드와 통신하는 방법  ssh로 클러스터 노드에 접속 후 curl을 통해 파드의 아이피로 직접 통신 가능 ssh 없이 통신을 하기 위해서는 노드 안에서 포워딩을 도와주는 무언가가 필요  서비스는 노드의 포트를 listen하고 이를 파드의 포트로 포워딩한다. 이 방식은 NodePort라고 알려진 것.   ClusterIP  클러스터 내부에 virtual IP를 만들어 서로 통신하게 만듬.   LoadBalancer  cloud provider 내부에서 서비스에 대한 로드밸런싱을 사용하게 해줌.     [30008]node - service [80] - [80]pod 형태일 때  targetPort로 파드가 떠있는 80포트를 지정.  서비스가 요청을 포워딩할 곳.   port는 서비스 자체의 포트.  서비스 관점에서의 포트 노드 내에서 virtual server처럼 작동. 자신의 IP를 가지고 있고, 이를 cluster ip라고 부름.   nodePort는 노드에서 외부로 expose하는 포트 기본적으로 노드포트 범위는 30000~32767   spec에는 type과 port를 작성. 포트에서 필수 필드는 port 하나.  targetPort는 안적으면 port와 동일 nodePort는 안적으면 범위내에서 랜덤 생성   label과 selector로 파드를 연결한다. 서비스와 멀티 파드가 연결되어 있으면 랜덤 알고리즘으로 로드밸런싱을 한다. 모든 클러스터의 노드로 노드포트를 expose한다.  Service Cluster IP  service들간에 통신을 하는 올바른 방법은 서비스의 아이피를 이용하는 것이다.  파드의 아이피는 변경될 수 있기 때문에.   랜덤으로 로드를 분배한다. 이를 클러스터 아이피라고 한다. targetPort는 파드가 expose하는 포트 port는 서비스가 사용하는 포트  Certification Tips  파일을 직접 만드는 것은 어렵기 때문에 imperative command를 사용해서 파일템플릿을 생성하는 것이 좋다. --dry-run과 -o yaml을 자주 사용하면 좋다. 파드  kubectl run --generator=run-pod/v1 nginx --image=nginx kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml   Deployment  kubectl create deployment --image=nginx nginx kubectl run --generator=deployment/v1beta1 nginx-image=nginx --dry-run --replicas=4 -o yaml   Important  kubectl create deployment는 --replicas 옵션이 없다.  kubectl scale 명령어로 크기를 조정해야한다.   kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml \u0026gt; nginx-deployment.yaml kubectl create deployment --image=nginx nginx --dry-run -o yaml \u0026gt; nginx-deployment.yaml   Service  kubectl expose pod redis --port 6379 --name redis-service --dry-run -o yaml  자동으로 파드와 매칭되는 라벨을 사용하게 됨.   kubectl create service clusterip redis --tcp=6379:6379 --dry-run -o yaml  app=redis라는 셀렉터가 있다고 가정함. 따라서 배포 전 selectordp rhdp   kubectl expose pod nginx --port=80 --name nginx-service --dry-run -o yaml  수동으로 nodeport 설정 넣어줘야 함.   kubectl create service nodePort nginx --tcp=80:80 --node-port=30080 --dry-run all  셀렉터 지정 필요     kubectl expose를 더 추천.  "
},
{
	"uri": "http://kimmj.github.io/ibiza/diary/20200331/",
	"title": "20200331",
	"tags": [""],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/port-targetport-nodeport-in-kubernetes/",
	"title": "[번역] 쿠버네티스에서의 Port, TargetPort, NodePort",
	"tags": ["kubernetes", "service", "targetport"],
	"description": "",
	"content": "원문: https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ports-targetport-nodeport-service.html\n쿠버네티스의 port declaration 필드에는 여러가지가 있다. 각 type에 대해 빠르게 살펴보고 YAML에서 각각 어떤 의미를 가지고 있는지 알아보도록 하자.\nPod ports list pod.spec.containers[].ports로 정의된 이 배열은 container가 노출하고 있는 포트의 리스트를 나타낸다. 이 리스트를 꼭 작성해야할 필요는 없다. 리스트가 비어있다고 하더라도 container가 포트를 listening하고 있는 한 여전히 네트워크 접속이 가능하다. 이는 단순히 쿠버네티스에게 추가적인 정보를 줄 뿐이다.\n List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \u0026ldquo;0.0.0.0\u0026rdquo; address inside a container will be accessible from the network. Cannot be updated. - Kubernets API Docs\n Service ports list 서비스의 service.spec.ports 리스트는 서비스 포트로 요청받은 것을 파드의 어느 포트로 포워딩할 지 설정하는 것이다. 클러스터 외부에서 노드의 IP 주소와 서비스의 nodePort로 요청이 되면 서비스의 port로 포워딩되고, 파드에서 targetPor로 들어온다.\n nodePort 이는 서비스가 쿠버네티스 클러스터 외부에서 노드의 IP 주소와 이 속성에 정의된 포트로 보일수 있도록 한다. 이 때, 서비스는 type: NodePort로 지정해야 한다 이 필드는 정의되어 있지 않을 경우 쿠버네티스가 자동으로 할당한다.\nport 서비스를 클러스터 안에서 지정된 포트를 통해 내부적으로 노출한다. 즉, 서비스는 이 포트에 대해서 보일 수 있게 되며 이 포트로 보내진 요청은 서비스에 의해 선택된 파드로 전달된다.\ntargetPort 이 포트는 파드로 전달되는 요청이 도달하는 포트이다. 서비스가 동작하기 위해서는 어플리케이션이 이 포트에 대해 네트워크 요청을 listening을 하고 있어야 한다.\n"
},
{
	"uri": "http://kimmj.github.io/docker/insecure-registry/",
	"title": "http를 사용하는 docker registry를 위한 insecure registry 설정",
	"tags": ["docker", "insecure-registry", "docker-registry"],
	"description": "",
	"content": "회사같은 곳에서는 보안상의 문제 때문에 Dockerhub에다가 이미지를 올리지 못하는 경우가 많습니다. 이를 위해서 docker에서도 docker registry라는 툴을 제공하는데요, 이는 자신의 local server를 구축하고, dockerhub처럼 이미지를 올릴 수 있는 툴입니다.\n이러한 docker registry는 사용자의 환경에 따라 http를 사용하는 경우가 있습니다. 이 때, docker는 default로 https 통신을 하려 하기 때문에 문제가 발생합니다. 이 경우 다음과 같이 조치를 하면 http 통신을 할 수 있습니다.\n절차 insecure-registry 설정 /etc/docker/daemon.json 파일을 열어 예시처럼 작성합니다. 없을 경우 생성하면 됩니다.\n{ \u0026#34;insecure-registries\u0026#34; : [\u0026#34;docker-registry:5000\u0026#34;] } docker 재시작 # flush changes sudo systemctl daemon-reload # restart docker sudo systemctl restart docker  이제 다시한번 docker pull 명령어를 통해 이미지가 제대로 다운로드 되는지 확인합니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/customize-login-message/",
	"title": "Ubuntu의 Login Message 수정하기",
	"tags": ["ubuntu", "login-message", "motd"],
	"description": "",
	"content": "TLDR   Expand me...    This package seeks to make the /etc/motd (Message of the Day) more dynamic and valuable, by providing a simple, clean framework for defining scripts whose output will regularly be written to /etc/motd.\n Ubuntu에서는 /etc/update-motd.d 안에 있는 파일들을 확인하여 console, ssh 등 어떤 방법으로든 로그인했을 때 메시지를 띄워줍니다. 여기서 파일들을 사전순으로 로딩하게 됩니다.\n따라서 해당 폴더에 적절한 파일들을 생성하게 된다면 로그인 시 출력되는 메시지를 조작할 수 있습니다.\n  적용법 /etc/update-motd.d/로 이동 cd `/etc/update-motd.d` 99-message 파일 생성 99-message 이름으로 파일을 생성합니다.\n#!/bin/sh  printf \u0026#34;hello. this is customized message.\\n\u0026#34; printf \u0026#34;\\n\u0026#34; 그다음 실행파일로 변경해줍니다.\nchmod +x 99-message 다시 세션 로그인하기 hello. this is customized message. Last login: Wed Mar 11 14:19:56 2020 from x.x.x.x wanderlust@wonderland $ "
},
{
	"uri": "http://kimmj.github.io/css/background-img-darken/",
	"title": "background image 어둡게 하기",
	"tags": ["css", "bgimg-darken"],
	"description": "",
	"content": "배경 이미지를 삽입했는데 사진이 너무 밝아 어둡게 필터처리를 넣고 싶은 경우가 있을 수 있습니다. 저의 경우 logo에 제 깃허브 프로필사진을 빼고 노을진 풍경을 넣었는데 사진이 너무 밝아 부자연스러운 느낌이 들었습니다.\n이 때 검색 후 다음과 같이 조치를 하여 어두워지는 효과를 줄 수 있었습니다.\n#sidebar #header-wrapper { background-image: linear-gradient( rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3) ),url(../images/sunset_on_ibiza.jpg); } linear-gradient 속성은 선형으로 gradient를 적용하는 속성인데 이를 활용하여 검정색 필터를 넣었습니다. 필수 파라미터로 두개의 색깔이 필요하고, 이를 각각 검정색(rgb(0, 0, 0))에 투명도를 0.3으로 설정한 rgba(0, 0, 0, 0.3)으로 설정하여 이미지가 전체적으로 어둡게 보이도록 설정한 것입니다.\n여담으로, 파워포인트에서 사진을 어둡게 하여 텍스트를 강조하고자 할 때 많이 이용했던 검정색 반투명 박스를 이용하는 것과 비슷한 방법이라고 느껴지네요.\n"
},
{
	"uri": "http://kimmj.github.io/hugo/google-analytics/",
	"title": "Hugo에 Google Analytics 적용하기",
	"tags": ["hugo", "google-analytics"],
	"description": "",
	"content": "google analytics는 내 블로그를 사용하는 사람들이 얼마나 많은지, 어떤 정보를 보는지 확인할 수 있는 서비스입니다. 무료로 사용할 수 있고, 사용 방법도 어렵지 않기 때문에 search console과 더불어 사용하면 좋은 서비스로 보입니다.\n그래서 저도 제 블로그에 google analytics를 설정할 수 있는지 찾아보던 중 다음과 같은 글을 확인하고, 이를 통해 설정할 수 있었습니다.\nhttps://discourse.gohugo.io/t/implementing-google-analytics-in-hugo/2671/2\n적용 방법 config.toml의 수정 hugo에서는 config 파일을 사용하여 사용자의 설정정보를 보관합니다. 저는 config.toml을 사용하고 있었는데 여기에다가 다음과 같이 추가하면 됩니다.\ngoogleAnalytics = \u0026#34;UA-123-45\u0026#34; 만약 yaml이라면 googleAnalytics: \u0026quot;UA-123-45\u0026quot; 정도로 설정하면 될 것 같습니다.\n여기서 해당 코드를 github에 올릴 때 안전하게 올리고 싶으시다면, git-secret을 참조하시기 바랍니다.\nheader.html에 추가 자신이 사용하는 theme에서 layouts/partials로 이동하면 header.html이라는 파일이 있을 것입니다. 제가 사용중인 Learn 테마에서는 custom-header.html이라는 파일로 사용자가 추가하기를 원하는 내용을 적을 수 있는 파일이 있었습니다. 따라서 저는 그 파일에 다음과 같이 추가해주었습니다.\n{{ template \u0026quot;_internal/google_analytics.html\u0026quot; . }} 또는 async로 이용하길 원한다면 다음과 같이 적으면 됩니다.\n{{ template \u0026quot;_internal/google_analytics.html\u0026quot; . }} 확인 google analytics는 하루에 한번 데이터를 전송합니다. 따라서 하루정도가 지난 후에 google analytics 페이지를 들어가면 통계자료를 열람할 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/git/git-secret/",
	"title": "git-secret을 통한 github 파일 암호화",
	"tags": ["git-secret", "github", "git"],
	"description": "",
	"content": "git을 사용하다 보면 password나 credential같은 정보가 git에 올라가는 경우가 종종 있습니다. aws같은 cloud provider의 crediential을 git에 생각없이 올리고, 이를 다른 해커가 크롤링을 통해 얻어 비트코인을 채굴하는 사례도 있었습니다.\n이처럼 보안이 필요한 파일을 git에 올릴 때, 암호화를 하여 업로드하는 방법이 있습니다.\nhttps://github.com/sobolevn/git-secret\ngit-secret git-secret은 파일에 대한 암화를 지원하기 위해 사용되는 프로그램입니다.\ngit-secret add 명령어를 통해 파일을 암호화하고, git-secret reveal을 통해 복호화합니다. 이 때 gpg를 이용하게 됩니다.\nInstall 사용자 환경에 따라 brew, apt, yum을 통해 설치할 수 있습니다. 또한 make를 통해서 빌드할 수도 있습니다.\n저의 경우 Ubuntu를 사용하고 있고 apt가 있기 때문에 이를 이용하여 설치하였습니다.\napt install git-secret 파일 암호화하기 gpg를 이용하여 키 생성하기 먼저, gpg가 설치되어있는지 확인하고 설치합니다.\n$ gpg -h | grep version License GPLv3+: GNU GPL version 3 or later \u0026lt;https://gnu.org/licenses/gpl.html\u0026gt; # 위와같은 결과가 나오지 않는다면, 설치해야합니다. $ apt install gpg 설치가 되었으면 gpg key를 생성합니다.\n$ gpg --full-generate-key gpg (GnuPG) 2.2.4; Copyright (C) 2017 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) Your selection? 1  RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072) 4096 Requested keysize is 4096 bits Please specify how long the key should be valid. 0 = key does not expire \u0026lt;n\u0026gt; = key expires in n days \u0026lt;n\u0026gt;w = key expires in n weeks \u0026lt;n\u0026gt;m = key expires in n months \u0026lt;n\u0026gt;y = key expires in n years Key is valid for? (0) 0  Key does not expire at all Is this correct? (y/N) y  GnuPG needs to construct a user ID to identify your key. Real name: Ibiza  Email address: my@email.com  Comment: Wanderlust  You selected this USER-ID: \"Ibiza (Wanderlust) \u0026lt;my@email.com\u0026gt;\" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O  We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy.  전부 완료되기까지 꽤 시간이 많이 걸립니다.\ngit-secret을 통한 암호화 본격적으로 시작하기 위해 git-secret을 init합니다.\ngit secret init 그 다음 git-secret에 사용자를 추가합니다.\ngit secret tell my@email.com 이제 secret을 통해 암호화할 파일은 원본이 github에 올라가지 않게 하기 위해 .gitignore에 추가하도록 합니다. 저의 경우 ./config.toml이라는 파일을 암호화하기 위해 다음과 같이 추가하였습니다.\nconfig.toml 그 다음 git-secret이 암호화 하도록 파일을 추가합니다.\ngit secret add config.toml 이 때 다음과 같은 에러가 발생했는데, github issue 중 비슷한 문제가 있어 이를 통해 해결할 수 있었습니다.\n$ git secret add config.toml config.toml is not a file. abort. $ git rm --cached config.toml $ git secret add config.toml 1 items added. 이제 hide를 통해 암호화할 수 있습니다.\ngit secret hide 이후 ls를 통해 확인해보면 .secret이라는 postfix가 들어간 파일을 보실 수 있습니다.\n$ ls config.toml* config.toml config.toml.secret git-secret을 이용한 복호화 reveal을 통해 파일을 복호화할 수 있습니다.\n$ git secret reveal File \u0026#39;/home/wanderlust/Ibiza/config.toml\u0026#39; exists. Overwrite? (y/N) y done. all 1 files are revealed. 이제 .secret 파일을 github에 올리면 안전하게 이용할 수 있습니다.\n다른 컴퓨터에서 접속할 때는 어떻게 해야하나요? gpg 알고리즘은 public key로 암호화하여 private key로 복호화합니다. 즉, 우리는 gpg key를 생성한 host에서 다른 host로 private key를 복사해주어야 합니다.\n따라서 다음과 같이 private key를 복사하고, 이를 import합니다.\n# gpg key를 생성했던 host wanderlust@wonderland $ gpg --export-secret-keys my@email.com \u0026gt; private-key.asc # key 복사 wanderlust@wonderland $ scp private-key.asc wanderlust@wonderland-laptop:~/ # gpg key를 import할 host wnaderlust@wonderland-laptop $ gpg --import ~/private-key.asc 그 다음 tell 옵션을 사용합니다.\ngit secret tell my@email.com 마지막으로 reveal 옵션을 통해 복호화합니다.\n$ git secret reveal done. all 1 files are revealed. "
},
{
	"uri": "http://kimmj.github.io/harbor/install/",
	"title": "Harbor 설치",
	"tags": ["harbor", "docker-registry"],
	"description": "",
	"content": "docker-compose 설치 $ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose # 설치 후 docker-compose 명령어가 실패한다면, symbolic link를 직접 걸어주도록 합니다. $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose $ docker-compose --version docker-compose version 1.24.1, build 1110ad01 harbor installer 다운로드 공식 Github에서 원하는 인스톨러를 다운로드 받습니다. 저는 online installer를 사용할 예정입니다.\n다운로드가 완료되었으면 압축을 해제합니다.\n$ tar xvf harbor-*.tgz harbor/prepare harbor/LICENSE harbor/install.sh harbor/common.sh harbor/harbor.yml harbor.yml 설정 vi로 harbor/harbor.yml을 열고 적당하게 편집합니다.\n여기서는 http로 간단하게 배포하는 설정을 해볼 것입니다.\nhostname: \u0026lt;domain or IP\u0026gt; # 192.168.x.x 그리고 https와 관련된 value를 모두 주석처리해줍니다.\n# https: # https port for harbor, default is 443 # port: 443 # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path proxy 환경이 아니라면 더 손댈 곳은 없습니다. proxy 환경일 경우 하단에 있는 proxy 설정을 /etc/environment 등을 참조하여 미리 적혀있는 부분을 추가하여 작성하시면 됩니다.\ninstall install시 clair, notary, chart-museum을 함께 설치할 수 있습니다.\n여기서 notary는 https 설정이 필요하기 때문에 생략하고 나머지 두개를 설치합니다.\n~/harbor$ ./install.sh --with-clair --with-chartmuseum 확인 이제 hostname에서 설정한 곳으로 접속하여 확인합니다. 설정을 바꾸지 않았다면 80포트로 접속할 수 있기 때문에 address만 입력해주면 접속할 수 있습니다.\n기본 ID/PW는 admin/Harbor12345 입니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/start-tmux-after-reboot/",
	"title": "reboot 후에 tmux를 실행시켜 원하는 작업을 하기",
	"tags": ["tmux", "reboot", "ubuntu"],
	"description": "",
	"content": "tmux는 terminal을 한 창에 여러개 띄울 때 사용하는 프로그램입니다.\n이 프로그램의 특징은 detach 모드로 들어가면, 어디서든 terminal에 접속하여 해당 session에 접속했을 때, 그 화면 그대로를 가져올 수 있다는 것입니다.\n즉, 원격 접속을 통해 서버에 접속했을 때 작업을 돌려놓고 detach모드로 들어가면 나의 session을 꺼도 실제 서버에서는 해당 작업이 계속해서 돌아가고 있다는 것입니다. 퇴근하기 전 시간이 오래걸리는 작업을 돌려놓고 가야할 때 유용하게 사용할 수 있습니다.\n저의 경우는 제 로컬 컴퓨터에서 hugo를 통해 사이트를 생성하여 블로그를 편집할 때마다 즉시 그 결과를 보고 있습니다. 이 떄 사용되는 명령어는 간단하지만, 매번 컴퓨터를 켤 때마다 이를 수행해주어야 한다는 것은 여간 귀찮은 일이 아닙니다. 따라서 다음과 같은 프로세스로 tmux를 생성하여 제가 하는 작업을 수행하도록 설정할 것입니다.\noverview  tmux session을 이름을 지정하여 생성하고, daemon으로 돌린다. 해당 session에 직접 접속하지 않고 명령어를 전달한다. 1, 2의 작업을 스크립트로 만들고 재부팅 시 실행하도록 한다.  절차 sh script 작성 다음과 같은 스크립트를 작성합니다. (tmux-on-reboot.sh)\n#!/bin/zsh  SESSIONNAME=\u0026#34;script\u0026#34; tmux has-session -t $SESSIONNAME 2\u0026gt; /dev/null if [ $? != 0 ] then tmux new-session -s $SESSIONNAME -n script -d \u0026#34;bin/zsh\u0026#34; tmux send-keys -t $SESSIONNAME \u0026#34;cd /home/wanderlust/Ibiza\u0026#34; C-m tmux send-keys -t $SESSIONNAME \u0026#34;hugo server --bind 0.0.0.0 --port 8000 --disableFastRender\u0026#34; C-m fi 여기서 C-m은 enter 명령을 주기 위함입니다.\n그 다음 해당 파일에 실행권한을 줍니다.\nchmod +x tmux-on-reboot.sh 재부팅 시 실행하도록 설정 이를 위해서는 crontab을 사용할 것입니다.\ncrontab을 사용하는 방법은 crontab -e를 통해 root 권한으로 명령어를 실행하는 방법과 /etc/crontab을 수정하여 원하는 유저를 부여하는 방법이 있습니다. 여기서는 제가 사용할 계정을 가지고 생성하는 것을 해보도록 하겠습니다.\nvi /etc/crontab vim이 열리면 가장 아랫줄에 다음과 같이 추가합니다.\n@reboot wanderlust /home/wanderlust/scripts/tmux-on-reboot.sh 저장 후 재부팅하여 실행되는지 확인합니다.\nReference  https://superuser.com/a/440082?  "
},
{
	"uri": "http://kimmj.github.io/docker/use-docker-without-sudo/",
	"title": "Docker를 sudo없이 실행하기",
	"tags": ["docker", "sudo"],
	"description": "",
	"content": "docker 명령어는 docker group으로 실행됩니다. 그러나 저희가 기존에 사용하던 일반 user는 해당 group에 속하지 않기 때문에 docker 명령어를 쳤을 때 permission에 관한 에러가 발생하게 됩니다.\n이 때 다음과 같이 조치를 하면 sudo 없이 user가 docker 명령어를 사용할 수 있게 됩니다.\nsudo usermod -aG docker $USER session을 다시 열고 docker ps 명령어를 입력하여 에러가 발생하는지 확인합니다.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES "
},
{
	"uri": "http://kimmj.github.io/ubuntu/oh-my-zsh-home-end-key/",
	"title": "oh-my-zsh에서 home key와 end key가 안될 때 해결방법",
	"tags": ["oh-my-zsh", "zsh"],
	"description": "",
	"content": "oh-my-zsh을 설치하고 원격접속이나 로컬환경에서 터미널에 접속했을 때 home key와 end key가 먹히지 않는 경우가 있습니다.\n이런 경우에 사용하는 terminal에서 home key와 end key를 눌러 실제 어떤 값이 전달되는지 확인한 후, 이를 beginning-of-line, end-of-line으로 설정하면 해결할 수 있습니다.\n해결법  home key가 되지 않는 terminal에 접속합니다. Control+V를 누릅니다. 문제가 되는 home key를 누릅니다. terminal에 뜬 문자를 기록합니다. ~/.zshrc에 다음과 같이 추가합니다. 여기서 case에 관한 부분은 상황에 따라 넣지 않거나 변경해야 합니다. case $TERM in (xterm*) bindkey \u0026#39;^[[H\u0026#39; beginning-of-line bindkey \u0026#39;^[[F\u0026#39; end-of-line esac  source ~/.zshrc를 하거나 새로운 session을 열어서 확인합니다.  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/base64-encode-decode/",
	"title": "Ubuntu에서 Base64로 인코딩, 디코딩하기",
	"tags": ["ubuntu", "base64"],
	"description": "",
	"content": "Encode echo로 입력하기 $ echo \u0026#34;password\u0026#34; | base64 cGFzc3dvcmQK Control+D를 누를때까지 입력하기 $ base64 admin password ^D # Control+D # result YWRtaW4KcGFzc3dvcmQK Decode echo로 입력하기 $ echo \u0026#34;cGFzc3dvcmQK\u0026#34; | base64 --decode password Control+D를 누를때까지 입력하기 $ base64 --decode YWRtaW4KcGFzc3dvcmQK ^D # Control+D # result admin password "
},
{
	"uri": "http://kimmj.github.io/ubuntu/file-edit-without-editor/",
	"title": "Editor(vi)가 없을 때 파일 수정하기",
	"tags": ["ubuntu", "file", "editor"],
	"description": "",
	"content": "echo로 파일 내용을 입력하는 방법 \u0026gt;로 파일 덮어쓰기 $ cat file asdfasdfasdf $ echo \u0026#34;asdf\u0026#34; \u0026gt; file $ cat file asdf \u0026gt;\u0026gt;로 파일에 이어쓰기 $ cat file asdf $ echo \u0026#34;asdf\u0026#34; \u0026gt;\u0026gt; file $ cat file asdf asdf cat으로 파일 입력하는 방법 \u0026gt;로 파일 덮어쓰기 $ cat file asdf $ cat \u0026gt; file aaaa bbbb ^D # Command+D $ cat file aaaa bbbb \u0026gt;\u0026gt;로 파일에 이어쓰기 $ cat file asdf $ cat \u0026gt;\u0026gt; file aaaa bbbb ^D # Command+D $ cat file asdf aaaa bbbb \u0026laquo;EOF로 EOF을 입력하면 입력 완료하기 $ cat file asdf $ cat \u0026lt;\u0026lt;EOF \u0026gt; file aaaa bbbb EOF $ cat file asdf aaaa bbbb "
},
{
	"uri": "http://kimmj.github.io/kubernetes/stern/",
	"title": "Stern을 이용하여 여러 pod의 log를 한번에 확인하기",
	"tags": ["kubernetes", "stern", "log"],
	"description": "",
	"content": "Kubernetes에서의 trouble shooting kubernetes 환경에서 어떤 문제가 발생하면 다음과 같은 flow로 확인을 해보면 됩니다.\n kubectl get pods -o yaml로 yaml을 확인하기 kubectl describe pods로 pod에 대한 설명 확인하기 kubectl describe deployments(statefulset, daemonset)으로 확인하기 kubectl logs로 로그 확인하기  보통 kubernetes 리소스의 부족과 같은 kubernetes단의 문제는 1~3을 확인하면 전부 문제점을 찾을 수 있습니다. 그러나 어플리케이션의 직접적인 원인을 알아보기 위해서는 log를 확인해야 합니다.\n하지만 kubectl의 logs에는 한가지 한계점이 있는데, 바로 단일 container에 대해서만 log 확인이 가능하다는 점입니다.\nstern은 이러한 문제를 해결하는 tool입니다.\n설치 방법 공식 Github: https://github.com/wercker/stern\nRequirements  golang govendor 기타 build에 필요한 dependency는 govendor로 관리  절차 golang 설치하기 go 언어는 직접 github에서 가져와 build할 수도 있지만, 간단하게 Ubuntu라면 apt를, macOS라면 brew를 이용하여 설치할 수 있습니다.\n# Ubuntu sudo apt install golang # macOS brew install go govendor 설치하기 govendor는 external package를 import하는데 사용하는 툴입니다. 이를 통해 dependency를 쉽게 import할 수 있습니다.\n간단하게 go 명령어만으로 설치할 수 있습니다.\n# install $ go get -u github.com/kardianos/govendor # test $ govendor --version v1.0.9 만약 govendor 명령어가 되지 않는다면, 보통은 go의 binary 폴더가 PATH에 추가되지 않은 것입니다.\ngo env를 통해 GOPATH, GOBIN을 확인합니다. GOBIN이 비어있을 경우 $GOPATH/bin이 GOBIN입니다.\n정상적으로 binary 파일이 있다면 PATH에 추가합니다. 저는 GOPATH가 /root/go였으므로 다음과 같이 입력하였습니다. terminal에 입력하는 것은 임시조치이므로 영구적으로 기록하려면 /etc/profile 또는 ~/.bashrc같은 파일에 저장합니다.\nexport PATH=$PATH:/root/go/bin stern 설치하기 env | grep GOPATH 명령어를 통해 GOPATH가 제대로 적용이 되는지 먼저 확인합니다. 제대로 안되어있을 경우 export GOPATH=/root/go처럼 적용해줍니다. 그 다음 다음의 명령어를 입력합니다.\nmkdir -p $GOPATH/src/github.com/wercker cd $GOPATH/src/github.com/wercker git clone https://github.com/wercker/stern.git \u0026amp;\u0026amp; cd stern govendor sync go install 확인 $ stern -v stern version 1.11.0 사용법 정확한 사용법은 이곳을 확인하시면 좋습니다.\n간단하게 설명하면, stern은 regEx로 매칭되는 pod나 container의 log를 tail -f처럼 보여주는 것입니다. 또한 그 결과는 색깔로 구분지어 보기 쉽습니다.\nstern -n kube-system kube-proxy-* "
},
{
	"uri": "http://kimmj.github.io/git/gitignore/",
	"title": "Gitignore 설정",
	"tags": ["git", "gitignore"],
	"description": "",
	"content": "참조: https://git-scm.com/docs/gitignore\ngitignore은 git에서 어떤 파일을 무시할지 설정하는 파일입니다. 이미 tracked된 것들에는 영향을 주지 않습니다.\ngitignore 참조 순서 syntax blank line 파일과 매칭되지 않습니다. 따라서 가독성을 위해 사용할 수 있습니다.\n# #은 comment로 처리됩니다.\nTraling space \\로 감싸졌다고 하더라도 무시됩니다.\n! !는 not과 같습니다. 파일 이름 맨 앞에 !가 있고, 이를 ignore할 때 사용하려면 \\를 통해 escape 해줘야 합니다.\n/ /는 디렉토리를 구분합니다. 처음, 중간, 끝 어느 위치에도 올 수 있습니다.\n끝에 /를 사용하면 매칭되는 폴더에만 적용이 됩니다. 사용하지 않으면 매칭되는 폴더, 파일 모두 적용됩니다.\n* /를 제외한 모든 길이의 문자와 매칭됩니다.\n? /를 제외한 한 문자와 매칭됩니다.\nrange [a-zA-Z]처럼 range를 설정할 수 있습니다.\n** **는 /를 포함한 모든 길이의 문자와 매칭됩니다.\n**로 시작하고 /를 적으면 모든 디렉토리에서 검색합니다. 예를 들어 **/foo는 모든 디렉토리에 있는 foo와 매칭됩니다. (현재 디렉토리인 .도 포함됩니다. ..는 포함되지 않습니다.)\n**로 끝나는 경우 모든 내부 파일, 폴더와 매칭됩니다. 예를 들어 abc/**는 abc 폴더 내부의 모든 파일, 폴더와 매칭됩니다.\n**를 중간에 적은경우 모든 sub directory(0개 이상)와 매칭됩니다. 예를 들어 a/**/b는 a/b, a/x/b, a/x/y/b 모두 매칭됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/check-listen-port/",
	"title": "열려있는 포트 확인하기",
	"tags": ["linux", "port", "network"],
	"description": "",
	"content": "열려있는 포트 확인하기 # 방법 1 lsof -i -nP | grep LISTEN | awk \u0026#39;{print $(NF-1)\u0026#34; \u0026#34; $1}\u0026#39; | sort -u # 방법 2 netstat -tnlp 열려있는 포트 확인하기 + 관련된 프로세스 이름 확인하기 netstat -tnlp | grep -v 127.0.0.1 | sed \u0026#39;s/:::/0 /g\u0026#39; | sed \u0026#39;s/[:\\/]/ /g\u0026#39; | awk \u0026#39;{print $5\u0026#34;\\t\u0026#34;$10}\u0026#39; | sort -ug "
},
{
	"uri": "http://kimmj.github.io/ubuntu/using-watch-with-pipes/",
	"title": "pipe를 사용한 명령어를 watch로 확인하기",
	"tags": ["watch", "pipe"],
	"description": "",
	"content": "pipe(|)는 grep과 다른 기타 명령어들과 함께 사용하면 좀 더 다양한 작업을 할 수 있습니다.\nwatch는 특정 명령어를 주기적으로 입력하여 결과 메시지를 확인합니다. 즉, 무엇인가를 모니터링할 때 주로 사용하곤 합니다.\n바로 본론으로 들어가서 pipe를 사용한 명령어를 watch로 확인하는 방법은 다음과 같습니다.\nwatch \u0026#39;\u0026lt;command\u0026gt;\u0026#39; 위와같이 quote로 감싸주세요.\nls -al을 가지고 확인해 보도록 하겠습니다.\n$ ls -al | grep config -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml $ watch ls -al | grep config # quote를 사용하지 않은 것 ^C # 결과 출력되지 않음 $ watch \u0026#34;ls -al | grep config\u0026#34; Every 2.0s: ls -al | grep config -rw-rw-r-- 1 wanderlust wanderlust 2960 1월 21 23:40 config.toml $ watch \u0026#34;ll | grep config\u0026#34; Every 2.0s: ll | grep config sh: 1: ll: not found 특히 마지막을 보시면 alias된 명령어는 인식하지 못하는 것을 알 수 있습니다.\n다른 페이지에서 했던 alias watch=\u0026quot;watch \u0026quot;를 적용했다고 하더라도, alias된 것을 quote로 감쌌을 때에는 인식하지 못하는 것을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/use-alias-in-watch/",
	"title": "watch를 사용할 때 alias 이용하기",
	"tags": ["watch", "ubuntu", "alias"],
	"description": "",
	"content": "watch는 정해진 시간동안 뒤에 적은 명령어를 실행해주는 프로그램입니다. 가령 kubernetes를 다룰 때 watch kubectl get pods -n kube-system을 통해 kube-system 네임스페이스에 있는 파드들을 지속적으로 모니터링 할 수 있습니다.\n그러나 watch는 alias된 명령어를 인식하지 못합니다.\n$ ll total 44K drwxrwxr-x 2 wanderlust wanderlust 4.0K 1월 7 20:38 archetypes -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml drwxrwxr-x 16 wanderlust wanderlust 4.0K 2월 22 23:08 content $ watch ll Every 2.0s: ll sh: 1: ll: not found 이 때 해결할 수 있는 가장 편한 방법은 watch 자체를 alias 시켜버리는 것입니다.\n저는 zsh을 사용하고 있으므로 ~/.zshrc에 다음과 같이 추가하였습니다.\nalias watch=\u0026#39;watch \u0026#39; 그 다음 설정파일을 다시 불러오거나 새로운 session을 생성합니다.\n# 설정파일 다시 불러오기 source ~/.zshrc 다시 watch를 하여 확인해 봅니다.\n$ watch ll Every 2.0s: ls --color=tty -lh total 44K drwxrwxr-x 2 wanderlust wanderlust 4.0K 1월 7 20:38 archetypes -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml drwxrwxr-x 16 wanderlust wanderlust 4.0K 2월 22 23:08 content 정상적으로 alias된 명령어를 인식하게 되었습니다.\n"
},
{
	"uri": "http://kimmj.github.io/python/python-beautiful-cli/",
	"title": "[번역]Python을 통해 이쁜 CLI 만들기",
	"tags": ["python"],
	"description": "",
	"content": "링크 : https://codeburst.io/building-beautiful-command-line-interfaces-with-python-26c7e1bb54df\ncommand line application을 만드는 것을 다루기 전에 빠르게 Command Line에 대해서 알아보자.\ncommand line 프로그램은 컴퓨터 프로그램이 생성되었을 때부터 우리와 함께 해왔고, 명령어들로 구성되어있다. commnad line 프로그램은 command line에서 또는 shell에서 동작하는 프로그램이다.\ncommand line interface는 user interface이지만 마우스를 사용하는 것이 아닌 terminal, shell, console에서 명령어를 입력하여 사용하는 것이다. console은 이미지나 GUI가 하나도 없이 전체 모니터 스크린이 텍스트로만 이루어진 것을 의미한다.\n위키피디아에 의하면\n CLI는 주로 1960년대 중만에 컴퓨터 terminal에서의 대부분의 컴퓨터 시스템과의 상호작용을 의미하고 1970년대와 1980년대를 거쳐 OpenVMS, MS-DOS를 포함한 개인용 컴퓨터와 Unix system, CP/M과 Apple DOS에서 사용되어 왔다. 인터페이스는 보통 명령어를 텍스트로 받고 이 명령어를 통해 적절한 system function을 동작시키게 하는 command line shell에서 동작한다.\n 왜 Python인가? Python은 유연성과 현존하는 프로그램들과도 잘 작동하기 때문에 보통 glue code language라고 여겨진다. 대부분의 Python 코드는 script와 command-line interface(CLI)로 작성된다.\n이런 command-line interface와 tool은 거의 대부분 원하는 것들을 자동화할 수 있기 때문에 특히 강력하다.\n우리는 예쁘고 상호작용을 하는 인터페이스, UI, UX가 매우 중요한 시대를 살고 있다. 우리는 이런 것들을 Command Line에 추가하고 사람들이 이를 받아들이고 Heroku같은 유명한 회사는 이를 공식적으로 사용할 것이다.\narguments와 option을 파싱하는 것에서부터 output에 색깔을 주고, progress bar를 추가하고 email을 전송하는 것과 같은 엄청난 CLI \u0026ldquo;frameworks\u0026quot;를 flagging하기까지 command line app을 build하는데 도움을 주는 많은 Python library와 module이 있다.\n이런 module과 함께 우리는 Heroku나 Vue-init이나 NPM-init같은 Node programe처럼 예쁘고 상호작용을 하는 command line interface를 만들 수 있다.\n예쁜 vue init CLI를 쉽게 만드려면 Inquirer.js를 Python에 이식하는 Python-inquirer를 사용한는 것을 권장한다.\n불행히도 Python-inquirer는 blessings(Unix같은 시스템에서만 사용가능한 _curses와 fcntl module을 import하는 python package)를 사용하기 때문에 Windows에서 작동하지 않는다. 어떤 대단한 개발자들은 _curses를 Windows에 이식할 수도 있긴 할것이다. Windows에서 fcntl의 대체제는 win32api이다.\n하지만 구글링을 통해 나는 이를 고치게 되었고 이를 PyInquirer라고 불르게 되었다. 이는 python-inquirer의 대체제이고 더 좋은 점은 이것은 Windows를 포함한 모든 플랫폼에서도 사용이 가능하다는 것이다.\nBasics in Commnad Line Interface with Python 이제 간단하게 command line interface를 보고 Python으로 하나를 만들어보자.\ncommand-line interface(CLI)는 실행파일의 이름으로 보통 시작한다. 그냥 console에서 이름을 입력하면 pip처럼 스크립트의 main entry point에 접근하게 된다.\nscript가 어떻게 개발되었는지에 따라 우리가 전달해 주어야 할 parameter들이 있고 이들은 이런 종류가 있다.\n Arguments: 스크립트에 전달되어야 하는 required parameter이다. 이를 작성하지 않으면 CLI는 error를 발생시킬 것이다. 예를 들어 django는 여기서 arguments이다. pip install django  Options: 이름에서 알 수 있듯이 optional parameter이다. 보통은 pip install django --cache-dir ./my-cache-dir처럼 name과 value의 쌍으로 사용한다. --cache-dir은 option parameter이고 value ./my-cache-dir은 cache directory로 사용되는 것이다. Flags: script에게 특정 행동을 disable할지 enable할지 알려주는 특수한 option parameter이다. 대부분은 --help같은 것들이다.  Heroku Toolbelt같은 복잡한 CLI를 통해 우리는 main entry point 아래의 몇몇 command들에 접근할 수 있게 된다. 이를 보통 commands와 sub-commands라고 한다.\n다른 python package들로 똑똑하고 예쁜 CLI를 어떻게 빌드하는지 보자.\nArgparse argparse는 command line program을 생성하는 default python module이다. 간단한 CLI를 만드는 데에 필요한 모든 기능을 제공한다.\nimport argparse parser = argparse.ArgumentParser(description=\u0026#39;Add som integers.\u0026#39;) parser.add_argument(\u0026#39;integers\u0026#39;, metavar=\u0026#39;N\u0026#39;, type=int, nargs=\u0026#39;+\u0026#39;, helm \u0026#39;integer list\u0026#39;) parser.add_argument(\u0026#39;--sum\u0026#39;, action=\u0026#39;store_const\u0026#39;, const=sum, default=max, help=\u0026#39;sum the integers (default: find the max)\u0026#39;) args = parser.parse_args() print(args.sum(args.integers)) argparse는 간단한 추가적인 동작을 한다. argparse.ArgumentParser는 프로그램에 description을 추가할 수 있도록 하는 반면 parser.add_argument는 command를 추가할 수 있도록 한다. parser.parse_args()는 주어진 arguments를 리턴하고 보통 이것들은 name-value 쌍으로 제공된다.\n예를 들어 args.integers를 통해 integers arguments에 접근할 수 있다. 위의 script에서 --sum은 optional argument이지만 N은 positional argument이다.\nClick click으로 우리는 CLI를 argparse보다는 쉽게 만들 수 있다. click은 argparse가 해결하는 문제와 동일한 것을 해결해준다. 하지만 약간은 다른 접근방식을 사용한다. 이는 decorators concept을 사용한다. 이는 command가 function이 되도록 하며 이를 decorators로 감쌀 수 있게 한다.\n# cli.py import click @click.command() def main(): click.echo(\u0026#34;This is a CLI built with Click\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 아래와 같이 argument와 option을 추가할 수 있다.\n# cli.py import click @click.command() @click.argument(\u0026#39;name) @click.option(\u0026#39;--greeting\u0026#39;, \u0026#39;-g\u0026#39;) def main(name, greeting): click.echo(\u0026#34;(), ()\u0026#34;.format(greeting, name)) if __main__ == \u0026#34;__main__\u0026#34;: main() 위의 script를 실행해보면 다음과 같을 것이다.\n$ python cli.py --greeting \u0026lt;greeting\u0026gt; Oyetoke Hey. OyeTOKI 모든걸 모아 Google Books의 query books에 간단한 CLI를 만들었다.\n click에 대한 자세한 정보는 official documentation에서 확인할 수 있다.\nDocopt docopt는 POSIC-style이나 Markdown 사용법을 파싱하여 쉽게 command line interface를 생성하는 lightweight python package이다. docopt는 수년간 사용해온 command line interface를 설명하는 정형화된 helm message와 man page에 대한 convention을 사용한다. docopt에서의 interface description은 helm message와 비슷하지만 정형화되어있다.\ndocopt는 파일의 젤 위에 required docstring이 어떻게 형식을 갖추는지가 중요하다. tool의 이름 다음에 올 docstring에서의 맨 위의 element는 Usage이여야 하고 이는 command가 어떻게 호출되고자 하는지에 대한 리스트들을 나열해야 한다.\ndocstring에서 두번째 element는 Options여야 하고 이는 Usage에서 나타난 option과 arguments에 대한 정보를 제공해야 한다. docstring의 내용들은 help text의 내용이 될 것이다.\n PyInquirer PyInquirer는 interactive command line user interface이다. 우리가 위에서 보았던 package들은 우리가 원하는 \u0026ldquo;예쁜 interface\u0026quot;를 제공하지 않는다. 따라서 어떻게 PyInquirer를 사용하는지 알아보도록 하자.\nInquirer.js처럼 PyInquirer는 두가지 간단한 단계로 구성되어 있다.\n 질문 리스트를 정의하고 이를 prompt에 전달한다. prompt는 답변 리스트를 리턴한다.  from __future__ import print_function, unicode_literals from PyInquirer import prompt from pprint import pprint questions = [ { \u0026#39;type\u0026#39;: \u0026#39;input\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;first_name\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;What\\\u0026#39;s your first name\u0026#39;, } ] answers = prompt(questions) pprint (answers) 상호작용하는 예시이다.\n 결과는 다음과 같다.\n이 스크립트의 몇몇 부분을 보도록 하자.\nstyle = style_from_dict({ Token.Separator: \u0026#39;#cc5454\u0026#39;, Token.QuestionMark: \u0026#39;#673ab7 bold\u0026#39;, Token.Selected: \u0026#39;#cc5454\u0026#39;, # default Token.Pointer: \u0026#39;#673ab7 bold\u0026#39;, Token.Instruction: \u0026#39;\u0026#39;, # default Token.Answer: \u0026#39;#f44336 bold\u0026#39;, Token.Question: \u0026#39;\u0026#39;, }) style_from_dict는 우리의 interface에 적용하고자 하는 custom style을 정의할 때 사용한다. Token은 component와 같은 것이고 이 아래에는 다른 component들도 있다.\n이전 예시에서 questions list를 prompt로 전달하여 처리하는 것을 보았다.\n이 방식으로 우리가 생성할 수 있는 interactive CLI에는 이런 예시가 있다.\n 결과\nPyFiglet pyfiglet은 ASCII text를 arts fonts로 변환해주는 python module이다. pyfiglet은 모든 FIGlet(http://www.figlet.org/)들을 pure python에 이식한 것이다.\nfrom pyfiglet import Figlet f = Figlet(font=\u0026#39;slant\u0026#39;) print f.rederText(\u0026#39;text to render\u0026#39;) 결과\nClint clint는 CLI를 만드는데 필요한 모든 것들을 통합한 것이다. color도 지원하고 뛰어난 nest-able indentation context manager도 지원하며 custom email-style quotes, auto-expanding column이 되는 column printer도 지원한다.\n EmailCLI 모든걸 통합하여 나는 SendGrid를 통해 메일을 전송하는 간단한 cli를 작성하였다. 아래의 스크립트를 사용하려면 SendGrid에서 API Key를 받아야 한다.\nInstallation pip install sendgrid click PyInquirer pyfiglet pyconfigstore colorama termcolor six   읽으면 좋은 것: https://www.davidfischer.name/2017/01/python-command-line-apps/  "
},
{
	"uri": "http://kimmj.github.io/docker/connect-container-to-container/",
	"title": "[docker-compose] container에서 다른 container로 접속하기",
	"tags": ["docker", "docker-compose", "network", "bridge", "container"],
	"description": "",
	"content": "배경 docker-compose에서는 network bridge를 설정합니다. 이 bridge로 내부 통신을 하게 되죠. 여기서 port-forward를 통해 외부로 서비스를 expose하게 되면 host의 IP와 port의 조합으로 접속할 수 있습니다.\n그런데 저는 네트워크 설정의 문제인지, 하나의 container에서 host IP로 접속이 불가능했습니다. 그러면서도 저는 어떻게든 다른 docker-compose의 서비스로 네트워킹이 됐어야 했습니다. 정확히 말하자면 harbor라는 서비스(docker registry)에서 jenkins로 webhook을 날려야 하는 상황이었죠.\n먼저 시도했던 것은 jenkins의 ip를 docker inspect jenkins_jenkins_1을 통해 알아내고, 이를 통해 webhook을 전송하는 것이었습니다. 그러나 실패했죠.\n다음으로 생각해본 것은, 그렇다면 jenkins를 harbor의 bridge로 연결해보자는 것이었습니다.\n따라서 다음과 같은 조치를 취해주었습니다.\ncontainer에서 다른 container로 접속하기 이미 동작중인 container에 bridge 연결 docker network connect harbor_harbor jenkins_jenkins_1 # docker network connect \u0026lt;bridge\u0026gt; \u0026lt;container\u0026gt; 연결된 bridge에서의 ip 확인 $ docker inspect jenkins_jenkins_1 \u0026#34;harbor_harbor\u0026#34;: { \u0026#34;IPAMConfig\u0026#34;: {}, \u0026#34;Links\u0026#34;: null, \u0026#34;Aliases\u0026#34;: [ \u0026#34;---\u0026#34; ], \u0026#34;NetworkID\u0026#34;: \u0026#34;---\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;---\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.24.0.1\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.24.0.11\u0026#34;, # \u0026lt;--------IP 연결 확인 harbor-core $ curl 172.24.0.11:8080 이 때, 연결은 host가 port-forward한 port가 아닌, container가 expose하고 있는 port를 입력해주어야 합니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-without-password/",
	"title": "password 없이 ssh 접속하기",
	"tags": ["ssh", "passwordless"],
	"description": "",
	"content": "자주 접속하는 서버에 패스워드를 항상 입력하는 것은 귀찮은 일이 될 것입니다.\n여기에서는 ssh key를 생성하고, 이를 이용하여 인증을 해 password를 입력하지 않는 방법을 알아볼 것입니다.\nssh-keygen을 통한 ssh key 생성 ssh 접속을 할 때 password를 입력했던 것처럼, 항상 ssh 접속을 위해서는 인증을 위한 key가 필요합니다.\n인증에 사용할 키를 ssh-keygen으로 생성하는 방법은 다음과 같습니다.\nssh-keygen -t rsa -b 4096 -t는 rsa 알고리즘을 통해 key를 생성하겠다는 의미이며, -b는 key의 사이즈를 정해주는 것입니다.\n다른 알고리즘들과 다른 옵션들은 https://www.ssh.com/ssh/keygen에서 더 확인할 수 있습니다.\n이제 ssh 인증을 위한 public key를 생성하였습니다.\nssh-copy-id를 통한 public key 복사 위에서 생성한 public key를 접속하고자 하는 서버에 복사를 하면, 서버에 접속할 때 서버는 해당 파일을 참조하여 인증을 시도할 것입니다.\nssh-copy-id user@server 이 때 최초 1회만 패스워드를 입력하면 됩니다. 그 뒤 다시한번 로그인을 시도해보면 패스워드 없이 로그인에 성공할 것입니다.\n  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-tunneling/",
	"title": "SSH Tunneling 사용법",
	"tags": ["ubuntu", "tunneling"],
	"description": "",
	"content": "-D 옵션으로 socks proxy 사용하기 A라는 서버에서 B라는 서버에 있는 서비스를 보려고 합니다. 이 때, 해당 웹 어플리케이션은 B에서만 연결된 특정 IP로 통신을 하고 있고, 이 때문에 A에서 어플케이션이 제대로 동작하지 않는 상황입니다.\n이 때 사용할 수 있는 것이 -D 옵션입니다.\n예시\nssh -D 12345 user@server.com 해당 세션이 꺼져있지 않은 상태에서 A 서버에서 웹 브라우저가 localhost:12345를 프록시로 사용하도록 하면 해당 웹 어플리케이션이 제대로 동작합니다.\n만약 windows라면 다음과 같이 진행하면 socks proxy를 사용하도록 할 수 있습니다. CMD를 열고 다음과 같이 입력하면 새로운 창으로 chrome이 뜰 것입니다.\n\u0026#34;C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\u0026#34; --user-data-dir=\u0026#34;%USERPROFILE%\\proxy-profile\u0026#34; --proxy-server=\u0026#34;socks5://localhost:12345\u0026#34; 해당 창에서 어플리케이션을 실행하면 실제 B 서버의 desktop에서 동작하는 것과 같은 효과를 볼 수 있습니다.\n-L 옵션으로 특정 포트로 접속하기 A에서 B라는 서버의 어플리케이션을 사용하려 합니다. 이 어플리케이션은 특정 포트를 사용하고 있습니다. 쉬운 예시를 위해 jenkins를 B에서 구동한다고 하고 포트를 8080이라고 하겠습니다.\n이 상황에서 A에서는 server-b.com:8080에 접속할 수 없는 상황입니다.\n이 경우 -L 옵션을 사용하면 됩니다.\nssh -L 1234:localhost:8080 user@server-b.com 해당 설정을 읽어보자면, A서버의 localhost:1234로 들어오는 요청들을 user@server-b.com으로 접속한 뒤 localhost:8080으로 보내라는 의미입니다.\n다음과 같은 설정도 있을 수 있습니다.\nssh -L 12345:server-c.com:8080 user@server-b.com 이 설정은 A에서 C로 직접 통신이 안되지만, B에서 C로 통신이 되는 상황입니다.\n이 때, 위의 설정은 localhost:12345로 들어오는 요청들을 user@server-b.com으로 접속한 뒤 server-c.com:8080으로 보내라는 의미입니다.\n-R 옵션을 이용하여 Reverse Proxy 사용하기 A에서 B로 ssh 접속을 하고 있습니다. 이 때, B에서 A로는 접속이 방화벽으로 막혀있는 상황입니다.\n이런 상황에서 -R 옵션을 주면 B에서도 A로 접속할 수 있습니다.\nssh 12345:localhost:22 user@server-b.com 해당 설정은 A서버에서 접속하는 localhost의 22 포트를 B서버의 12345 포트와 연결하라는 의미입니다.\n그 다음 해당 세션이 유지된 상태에서 B에서 다음과 같이 접속합니다.\nssh localhost -p 12345 그러면 B서버에서 A서버로 접속이 가능한 것을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-with-jump/",
	"title": "Gateway를 이용하여 SSH 접속하기",
	"tags": ["ssh", "linux"],
	"description": "",
	"content": "ssh cli 이용하는 방법 -J 옵션을 이용한다.\nssh user@server -J user2@server2 두개 이상의 경우 ,로 구분한다.\n예: user2@server2로 접속 후 user3@server3로 접속한 뒤 user@server로 접속해야 할 경우\nssh user@server -J user2@server2,user3@server3 이 상황에서 ssh-copy-id를 이용해 패스워드를 입력하지 않고 이동하려면\nlocaluser@localhost $ ssh-copy-id user2@server2 localuser@localhost $ ssh user2@server2 user2@server2 $ ssh-copy-id user3@server3 user2@server2 $ ssh user3@server3 user3@server3 $ ssh-copy-id user@server 이후 ssh를 통해 진입하면 패스워드 없이 접속 가능.\n만약 port가 필요한 경우 server:port 형태로 입력\nssh user@server:port -J user2@server2:port2,user3@server3:port3 ssh config 파일 이용하는 방법 Host server HostName remote-server User user ProxyJump gateway2 Host server2 HostName gateway1 User user2 Host server3 HostName gateway2 User user3 ProxyJump gateway1 "
},
{
	"uri": "http://kimmj.github.io/iac/translate-what-is-infrastructure-as-a-code/",
	"title": "[번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials",
	"tags": ["IaC", "infrastructure-as-code"],
	"description": "",
	"content": "link: https://stackify.com/what-is-infrastructure-as-code-how-it-works-best-practices-tutorials/\n과거에 IT infrastructure를 관리하는 것은 힘든 일이었다. 시스템 관리자는 수동으로 관리하고 어플리케이션을 구동시키기 위해 모든 하드웨어와 소프트웨어를 설정해야 했다.\n하지만 최근 몇년간 급격하게 상황들이 바뀌었다. cloud computing같은 트렌드가 디자인, 개발, IT infrastructure의 유지를 하는 방법을 혁명화하고 발전시켰다.\n이러한 트렌드의 핵심 요소는 infrastructure as code이다. 여기에 대해 이야기 해보도록 하겠다.\nDefining Infrastructure as Code infrastructure를 코드로 정의하는 것부터 시작해보도록 하자. 이것이 무엇을 의미하는지, 어떤 문제들을 해결하는지 배우게 될 것이다.\nWikipedia에서 IaC는 다음과 같이 정의되어 있다.\n Infrastructure as code is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.\n  Infrastructure as code는 물리적인 하드웨어 설정이나 상호작용을 하는 설정 툴을 이용하는 것이 아닌, 기계가 읽을 수 있는 파일로 정의하여 computer data center들을 관리하고 프로비저닝하는 프로세스이다.\n 이 정의는 나쁘지 않지만 약간 너무 장황하게 설명했다. 더 간단하게 해보자.\n Infrastructure as code (IaC)는 IT infrastructure를 설정 파일로 관리한다는 의미이다.\n 다음 질문은 이렇게 될 것이다. \u0026ldquo;왜 그걸 쓰고싶어 하나요?\u0026rdquo;\nWhat Problem Does IaC Solve? \u0026ldquo;what\u0026quot;에 대해 생각하기 전에 \u0026ldquo;why\u0026quot;에 먼저 집중해보자. 왜 IaC가 필요한가? 이것이 어떤 문제를 풀어주는가?\nThe Pain of Managing IT Infrastructure 역사적으로 IT infrastructure를 관리하는 것은 수동 프로세스였다. 사람들은 물리적으로 서버를 위치시키고 이를 설정했다. 머신이 OS와 어플리케이션이 원하는 설정으로 되었을 때에, 어플리케이션을 배포할 수 있게 된다. 당연하게도 수동 프로세스는 자주 문제를 일으킨다.\n첫번째 큰 문제는 비용이다. 네트워크 엔지니어에서부터 하드웨어 관리 기술자까지 많은 전문가를 고용해서 프로세스의 각 단계를 수행시키도록 해야한다. 이런 모든 사람에게 돈을 지불해야 하면서 또한 관리되어야 한다. 이는 관리 오버헤드를 야기하고 기업 내의 소통의 복잡성을 증대시킨다. 결과적으로? 돈이 사라진다. 또한 우린 아직 비용을 더 많이 늘리는 데이터센터를 관리하는 것과 빌딩에 대한 이야기를 하지 않았다.\n그 다음 문제는 확장성과 가용성이다. 하지만 결국 모든 것은 속도 문제다. 수동으로 설정하는 것은 너무 느리고, 어플리케이션의 접속은 스파이크를 치고 있는 동안 시스템 관리자는 부하를 관리하기 위해 필사적으로 서버를 세팅할 것이다. 이는 가용성에도 무조건 영향을 미친다. 기업이 백업 서버나 심지어 데이터 센터가 없다면 어플리케이션은 장기간동안 이용불가능해질 것이다.\n마지막으로 가장 중요한 문제는 inconsistency이다. 몇몇 사람들이 수동으로 설정을 배포한다면, 균열은 생길수밖에 없다.\nCloud Computing: A Cure? Cloud computing은 방금 읽었던 고통들을 완화시켜준다. 이는 데이터센터를 구축하고 유지보수하는 것과 그 비용으로부터 자유롭게 해준다.\n그러나 Cloud computing은 만병통치약과는 거리가 멀다. 이것이 infrastructure를 빠르게 셋업하는데에는 도움을 주겠지만 (그래서 고가용성이나 확장성의 문제는 해결해 준다) 이는 inconsistency 이슈를 해결하지는 못한다. 설정을 수행하는 사람이 한명보다 많다면, 불균형이 생길 수 있다.\nInfrastructure as Code: The Missing Piece of the Puzzle IaC 정의를 다시 한번 봐보자.\n Infrastructure as code (IaC)는 IT infrastructure를 설정 파일로 관리한다는 의미이다.\n 정의에서 가져온 핵심은 바로 다음과 같다. IaC 이전에 IT 사람들은 infrastructure에 대해 수동으로 설정을 바꾸어야 했다. 아마 스크립트를 쓰거나 몇몇 작업은 자동화를 시켰겠지만, 그냥 그 정도였다. IaC를 통해 infrastructure의 설정은 코드 파일의 형태를 띄게 되었다. 이는 단순한 텍스트이지만 수정하고 복사하고 분배하는 것이 쉽다. 당신은 다른 소스코드 파일들처럼 source control로 이를 관리할 수 있고 또 그래야 한다.\nInfrastructure as Code Benefits 이제까지 수동으로 infrastructure를 관리하는 것의 문제점들을 알아 보았다. 우리는 여기서 어떻게 cloud computing이 이런 몇가지 문제점을 해결해 주는지, 또 어떤것들은 해결해주지 않는지 알아보았다. 그리고 우리는 IaC가 퍼즐의 마지막 조각이라고 말하며 이 논의를 마쳤다.\n이제 우리는 IaC solution을 사용할 때의 이점에 대해서 알아볼 것이다.\nSpeed IaC의 가장 중요한 이점은 스피드이다. Infrastructure as Code는 script를 실행시켜 빠르게 완전한 infrastructure를 셋업할 수 있게 해준다. 이를 개발환경과 production 환경부터 staging, QA 등등까지 모든 환경에 대해서 할 수 있다. IaC는 전체 소프트웨어 개발 라이프사이클을 효과적으로 만들어준다.\nConsistency 수동 프로세스는 실수를 불어일으키고 시간이 걸린다. 사람은 실수할 수 있다. 우리의 기억은 잘못될 수 있다. 소통은 어렵고 우리는 일반적으로 소통을 어려워한다. 여기서 읽었던 것 처럼 수동으로 인프라를 관리하는 것은 얼마나 열심히 하던지 간에 불균형을 일으키게 된다. IaC는 믿을 수 있는 하나의 소스코드로 이 설정 파일을 관리하여 이를 해결해준다. 이런 방식으로 동일한 설정이 어떤 불균형도 없이 계속해서 배포됨을 보장한다.\nAccountability 이는 빠르고 쉬운 것이다. IaC 설정 파일을 소스 코드 파일처럼 버전화 할 수 있기 때문에 설정들의 변경사항을 추적할 수 있다. 누가 이를 했고 언제 했는지 추측할 필요가 없다.\nMore Efficiency During the Whole Software Development Cycle infrastrucutre as code를 적용하면 infrastructure 아키텍쳐를 어려 단계로 배포할 수 있다. 이는 전체 소프트웨어 개발 라이프 사이클을 더욱 효율적으로 해주어 팀의 생산성을 더 끌어올릴 수 있다.\n프로그래머들이 IaC를 사용하여 sandbox 환경을 생성하여 독립된 공간에서 안전하게 개발할 수 있게 할 수 있다. 같은 방식으로 테스트를 돌리기 위해 production 환경을 복사해야 하는 QA 전문가들도 사용할 수 있다. 결과적으로 배포 시에 infrastructure as code는 하나의 단계가 될 것이다.\nLower Cost IaC의 주된 장점 중 하나는 의심할 여지 없이 인프라 관리의 비용이 줄어든다는 것이다. IaC를 통해 cloud computing을 하면 극적으로 비용을 줄일 수 있다. 이는 하드웨어에 많은 시간을 사용하지 않아도 됨을 의미하고 이를 관리할 사람을 고용하지 않아도 되며 저장할 물리적인 장소를 만들거나 대여하지 않아도 됨을 의미한다. 하지만 IaC는 기회비용이라 부르는 다른 미묘한 방식으로 비용을 더 줄여준다.\n보았듯이 똑똑하고, 높은 급료를 받는 전문가가 자동화 할 수 있는 작업을 수행하는 것은 돈 낭비이다. 이제 모든 포커스는 기업에 더 가치있는 일에 맞춰져야 한다. 그리고 자동화 전략을 사용하면 편하다. 이를 사용하여 수동적이고 느리고 에러가 나기 쉬운 작업을 수행하는 것으로부터 엔지니어들을 해방시켜주고 더 중요한 일에 집중할 수 있게 한다.\nHow Does IaC Work? IaC 툴은 어떻게 작동하는지가 굉장히 다양하지만 이를 일반적으로 두가지 종류로 나눠볼 수 있다. 하나는 imperative approach를 따르는 것이고 다른 하나는 declarative approach를 따르는 것이다. 이 위의 카테고리를 프로그래밍 언어 패러다임과 엮을 수 있다면 완벽하다.\nimperative approach는 순서를 제공하는 것이다. 이는 명령어나 지시사항의 순서를 정의하여 인프라가 최종적인 결과를 가지게 하는 것이다.\ndeclarative approach는 완하는 결과를 정의하는 것이다. 명백하게 원하는 결과로 가는 단계들의 순서를 정의하지 않고, 어떻게 최종 결과가 보여야 하는지만 정의한다.\nLearn Some Best Practices 이제 IaC 전략의 모범사례를 확인해 볼 것이다.\n 코드가 하나의 믿을 수 있는 소스로부터 나온다. 명시적으로 모든 인프라 설정을 설정 파일 안에 작성해야 한다. 설정 파일은 인프라 관리 문제에 대해서 단 하나의 관리포인트이다. 모든 설정 파일에 대해 Version Control을 하라. 이는 말할 필요도 없겠지만, 모든 코드는 source control이 되어야 한다. 인프라 스펙에 대한 약간의 문서화만 필요하다. 이 포인트는 첫번째의 논리적인 결과이다. 설정 파일이 단일 소스이므로, 문서화가 필요하지 않다. 외부 문서는 실제 설정과 싱크가 잘 안맞을 수 있지만 설정 파일은 그럴 이유가 없다. 설정을 테스트하고 모니터한다. IaC는 코드로 모든 코드와 같이 테스트될 수 있다. 따라서 가능하면 테스트해라. IaC에 대한 테스트와 모니터링을 하여 production에 어플리케이션을 배포하기 전에 서버에 문제가 있는지 확인할 수 있다.  Resources At Your Disposal 다음은 IaC를 배우기 좋은 유용한 리소스들이다.\n Wikipedia’s definition Edureka’s Chef tutorial Ibexlabs’s.The Top 7 Infrastructure As Code Tools For Automation TechnologyAdvice’s Puppet vs. Chef: Comparing Configuration Management Tools  Infrastructure as Code Saves You Time and Money IaC는 DevOps 움직임의 중요한 부분이다. cloud computing이 많은 수동 IT 고나리의 문제점을 해결하는 데 첫 번째 단계라고 본다면 IaC는 다음의 논리적인 단계가 될 것이다. 이는 cloud computing의 모든 가능성을 열어주고 개발자와 다른 전문가들로부터 수동적이고 에러가 발생하기 쉬운 업무를 없애준다. 또한 소프트웨어 개발 라이프사이클의 효율성을 늘리고 비용을 절감한다. IaC와 함께 Retrace같은 툴을 쓰는것도 좋다. Retrace는 코드 레벨의 Application Performance Manager 솔루션으로 전체 개발 라이프사이클에서 어플리케이션의 퍼포먼스를 관리하고 모니터하게 할 수 있다. 에러 추적이나 로그 관리, 어플리케이션 메트릭과 같은 또한 많은 다른 기능들을 가지고 있다.\n"
},
{
	"uri": "http://kimmj.github.io/cicd/deploy-strategy/",
	"title": "Deploy Strategy",
	"tags": ["deploy", "cicd", "canary", "blue-green", "roll-out"],
	"description": "",
	"content": "Deploy Strategy 실제 시스템을 운용할 때 중요하게 여겨지는 것 중 하나가 downtime을 없애는 것이다. 새로운 업데이트가 있을 때마다 해당 인스턴스가 동작하지 않는다면, 자주 업데이트 하는 것이 어려워질 수 있습니다. 따라서 deploy strategy를 가지고 어떻게 downtime을 줄이는지 알아보도록 하겠습니다.\nCanary canary deploy는 트래픽 비율을 바꾸어가며 배포하는 전략입니다. 이해하기 편하도록 Kubernetes 환경이라고 생각해 보도록 하겠습니다. (또는 LoadBalancer가 있어서 부하를 분산하고 있다고 생각하면 좋을 것 같습니다.) 이 때 업데이트된 버전을 따로 올리고 트래픽을 old:new = 100:0으로 줍니다. 이러면 새로운 버전이 정상적으로 실행할 준비가 될 때까지는 우리의 인스턴스가 정상적으로 동작하고 있을 것입니다.\n준비가 되었다면, old:new = 90:10처럼 약간의 트래픽을 새로운 버전으로 흘려줍니다. 이 어플리케이션이 만약 웹사이트라면, 전체 유저 중 10%의 사람만이 새로운 버전의 웹사이트를 보게 될 것입니다.\n이 때 각종 통계라던지 테스트를 통해 새로운 버전에 문제가 없는지 확인합니다. 혹시나 문제가 발생하더라도, 10%의 사람만이 문제를 경험하게 될 것입니다. 문제점이 발견되면 다시 old:new = 100:0으로 트래픽을 돌려버리면 이전의 잘 돌아가던 상태로 복구할 수 있습니다.\n이런식으로 새로운 버전의 트래픽을 능동적으로 조금씩 늘려가며 여러 지표를 확인하고 정상적이라고 판단되면 old:new = 0:100으로 변경합니다. 그 다음 이전 버전을 삭제하면 이제 완전히 새로운 버전으로 업데이트 된 것입니다.\n이 방식은 특정 부분을 운용중에 긴급 패치하는 경우 사용할 수 있는 전략입니다. 장애가 발생하더라도 큰 위험 부담이 없기 때문이죠.\n그러나 이를 위해서는 새로운 버전이 얼마나 이전 버전과 호환이 잘 되는지가 중요합니다. 만약 이전 버전과는 너무나도 다른 새로운 버전이 있다면 전체 어플리케이션은 정상적으로 동작하지 않을 수 있습니다.\n             Blue-Green blue-green deploy는 두개의 버전을 동시에 올려놓고, 트래픽을 한번에 바꾸는 전략입니다.\n하나의 인스턴스에 대해 버전을 두가지 올립니다. 이 때, 트래픽의 비율은 old:new = 100:0으로 항상 이전 버전으로만 흐르도록 합니다. 그러다가 어느 순간 old:new = 0:100으로 트래픽을 아예 바꾸어버립니다. 그러면 유저는 항상 새로운 버전만 경험하게 될 것입니다.\n이렇게 새로운 버전을 운용하며 문제점을 파악합니다. canary와는 다르게 문제점이 있을 경우 해당 인스턴스가 아예 중단되어 downtime이 생깁니다. 이럴 때에는 다시 원래대로 트래픽을 old:new = 100:0으로 바꾸면 이전 버전으로 빠르게 roll back이 가능합니다.\n또한 canary에서는 긴급 패치하는 경우, 하나의 인스턴스에 대해서만 행해진다고 하였습니다. 그러나 blue-green의 경우 전체 패키지 또는 어플리케이션에 대해서도 사용할 수 있는 전략입니다. 물론 장비가 2배로 들겠지만, 트래픽만 바꾸면 되니 빠른 roll back이 가능하기 때문에 그 비용을 감수할 수 있습니다.\n당연하게도 하나의 인스턴스만 업데이트하는 경우에도 사용할 수 있습니다. 이 경우에는 다른 인스턴스들과 잘 호환이 되어야 합니다. 반면 전체 어플리케이션에 대해 할 경우 함께 설치되는 인스턴스들끼리의 호환성만 확인하면 됩니다. dependency가 복잡할 경우, 어플리케이션이 빠르게 변화하는 경우 사용해볼 수 있을 것입니다.\n          roll out roll out은 하나의 인스턴스에 대해 Pod 또는 VM이 여러개 떠있다고 가정합니다.\n이 때, 하나씩 순차적으로 새로운 버전으로 변경합니다. downtime을 줄이고 싶다면 업데이트하는 동안에는 트래픽을 흐르지 않도록 하면 될 것입니다.\n이런식으로 순차적으로 새로운 버전으로 변경하여 최종적으로는 모든 노드에 대해 업데이트가 완료될 것입니다.\ncanary와 마찬가지로 다른 버전의 인스턴스들과 잘 동작해야함을 보장해야 합니다. 그래야 두 버전이 동시에 사용되고 있다고 하더라도 정상적으로 어플리케이션이 동작하게 될 것입니다.\n          "
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/pods/",
	"title": "Pods",
	"tags": ["kubernetes", "pod"],
	"description": "",
	"content": "Pod Overview Pod의 이해 Pod는 Kubernetes에서 가장 작은 배포 오브젝트이며 쿠버네티스에서 관리하는 최소 관리 단위입니다. Pod는 cluster 안에서 실행중인 어떤 프로세스를 의미합니다. application container, 스토리지 리소스, 유일한 network ip, container가 어떻게 실행할지를 캡슐화한 것입니다.\n각각의 Pod는 주어진 application에서 단일 인스턴스를 수행합니다. 즉, 한가지 역할을 맡고 있다고 생각하시면 됩니다.. 따라서 application을 수직확장하고 싶다면 각 인스턴스에 대해 여러 Pod를 생성하면 된다. 그러면 동일한 역할을 하는 Pod가 늘어나니, 병렬적으로 처리가 가능할 것입니다.\nPod는 서비스 중에서 서로 연관성이 높은 프로세스를 지원하기 위해 디자인되었습니다. container는 리소스와 의존성들을 공유하고, 서로 통신하며, 언제/어떻게 종료하는지에 대해 서로 조정합니다. Pod 내의 container는 Networking과 Storage를 공유할 수 있습니다.\nPod 안에는 둘 이상의 container가 있을 수 있습니다. 이 때 Pod 내에 여러 container를 두는 것은 container가 정말 강하게 결합될 때 입니다. 예를 들어, 하나의 container는 web server로 shared volume에서 파일을 가져와 호스팅하는 역할을 하고, 나머지 하나의 side-car container는 외부에서 file을 pulling하여 shared volume에 올리는 역할을 하는 것입니다. 이처럼 둘간의 결합성이 큰 경우(여기서는 shared volume일 것입니다) 동일한 Pod에 위치할 수 있습니다.\ninit container는 실제 app container가 시작하기 전에 먼저 작업을 하는 container입니다.\nNetworking 각 Pod 단위로 네트워크 IP 주소를 할당받습니다. 그 내부의 container끼리는 localhost로 통신하게 되고 외부와의 통신을 위해선 공유중인 네트워크 리소스를 어떻게 분배할 지 합의 및 조정해야합니다. 예를 들어 Pod 단위로 생각해보면 하나의 IP를 가지게 되고 expose할 port들을 가지게 될 것입니다. IP는 상관 없지만 port의 경우 특정 container로 binding이 되어야 하기 때문에 각 container는 동일한 port를 expose할 수 없습니다. Pod 관점에서 보면 어느 container로 전달해주어야 하는지 모르기 때문이죠.\nStorage Pod는 공유하는 저장공간을 volumes로 지정합니다. 이렇게 하면 하나의 container만 재시동되는 경우에도 데이터를 보존할 수 있습니다.\nPod로 작업하기 Pod를 사용할 때에는 kubectl create pods처럼 controller 없이 Pod를 생성하는 것은 좋은 생각이 아닙니다. 이렇게 할 경우 Kubernetes의 장점들을 충분히 활용할 수 없습니다. 특히 self-healing을 하지 못하기 때문에 Pod가 떠있던 노드에 장애가 발생하면 Pod는 영영 복구되지 않을 수 있습니다. 반면 controller로 관리할 경우 self-healing을 지원하여 노드에 장애가 발생해도 다른 노드에 해당 Pod를 띄워서 계속하여 서비스를 할 수 있습니다.\nPod의 재시작과 container의 재시작을 혼동하면 안됩니다. Pod는 그 자체만으로 동작하지 않습니다. 오히려 Pod는 삭제되기 전까지 계속 유지가 되는, container가 동작하는 환경이라고 보면 됩니다. Pod를 VM을 사용하는 상황과 비유를 해보자면 Pod는 VM에서 하나의 VM에 실행하는 application들처럼 하나의 논리적 호스트에서 container들을 실행하는 개념입니다.\nPod 내의 container들은 IP주소와 port space를 공유합니다. 그리고 서로 localhost로 통신할 수 있습니다. 반면에 다른 Pod에 있는 container와는 Pod에 할당된 IP를 사용하여 통신할 수 있습니다.\nPod는 container처럼 임시적인 자원이기 때문에 삭제시 reschedule이 아닌 새로운 동일한 spec의 Pod를 새로운 UID(Unique ID)로 생성합니다. 즉, Pod가 삭제되면 그 안에 있는 내용들을 잃어버리게 됩니다. VM에 빗대자면 VM을 생성하는 template을 가지고 새로운 VM을 생성하며, 삭제할 경우 해당 VM을 완전히 삭제한다는 개념입니다. 이 때 volume도 Pod가 삭제되면 삭제됩니다.\nPod Lifecycle Pod의 status 필드는 PodStatus의 phase 필드입니다. Pod의 phase는 간단하게 Pod가 위치한 lifecycle의 상위 개념에서의 요약정보입니다.\nphase 의 value들에는 다음과 같은 것들이 있습니다.\n Pending : Pod가 kubernetes 시스템에 의해 받아들여졌지만 하나 이상의 container image가 생성되지 않은 상태. Running : Pod가 node에 바운드되고 모든 container들이 생성됨. 최소한 하나의 container가 실행 중이거나 시작 또는 재시작 중. Succeeded : 모든 container가 성공적으로 종료되었고, 재시작되지 않음. Failed : 모든 container가 정료되었고, 최소 하나의 container가 failure 상태. Unknown : 어떤 이유로 인해 Pod의 state를 얻어낼 수 없음. 보통 Pod의 호스트와 통신이 안되는 문제.  Container probe kubelet이 container을 진단할 때 사용하는 것입니다. container에서 구현된 Handler를 호출하여 이러한 진단을 수행합니다.\n크게 3가지 probe가 있습니다.\n livenessProbe: container가 실행 중인지 나타냄. liveness probe가 실패하면 kubelet은 container를 죽이고, 이 container는 restart policy를 실행한다. redinessProbe: container가 service requests를 받을 준비가 되었는지 나타냄. readiness probe가 실패할 경우 endpoint controller는 Pod의 IP 주소를 Pod와 매칭되는 Service들의 endpoints에서 삭제한다. initial delay 이전의 default 값은 Failure이다. startupProbe: container 내부의 application이 시작되었는지를 나타냄. 다른 probe들은 startup probe가 성공할 때까지 비활성화 상태.  livenessProbe를 사용해야하는 상황 livenessProbe는 container가 제대로 동작하지 않은 경우 제대로 동작할 수 있을 때까지 재시동하는 목적으로 사용합니다. 따라서 container가 제대로 동작하지 않을 때 실행하는 프로세스가 이미 있다면 굳이 사용하지 않아도 됩니다. 원래 목적대로 재시동을 하고 싶다면 livenessProbe를 지정하고 restartPolicy를 설정합니다.\nreadinessProbe를 사용해야하는 상황 Pod에 request를 보내면 트래픽은 그 내부의 container로 전달됩니다. 이 때, 해당 container가 제대로 동작을 하지 않는다면, 파드에 request를 보냈을 때 비정상적인 응답을 할 것입니다.\n따라서 Pod가 제대로 응답을 보내줄 수 있는 상황에만 해당 Pod로 트래픽을 전달하고 싶다면, readinessProbe를 사용합니다. Kubernetes는 readinessProbe가 실패하면 Service와 연결된 Endpoint에서 해당 Pod를 삭제합니다. 그러면 Service로 흐른 트래픽은 redinessProbe가 실패한 Pod로 흐르지 않게 됩니다.\n단순히 삭제시 트래픽이 안흐르도록 하고 싶다면 굳이 할 필요는 없습니다. 알아서 삭제시 Service와 연결된 Pod의 Endpoint를 삭제하기 때문입니다.\nstartupProbe를 사용해야하는 상황 startupProbe는 위의 두 probe들과는 약간 다른 성격을 가졌습니다. livenessProbe와 함께 사용이 되는데요, container가 initialDelaySeconds + failureThreshold × periodSeconds만큼의 시간이 지난 후에 정상동작을 할 경우(container가 작업을 시작하기까지 시간이 오래 걸리는 경우) livenessProbe에 의해 fail이 발생하고, 재시작 되는것을 막아 deadlock 상태를 방지해줍니다.\nRestart policy PodSpec에서 restartPolicy 필드에는 Always, OnFailure, Never를 사용할 수 있습니다. 그 중 default는 Always입니다.\nrestartPolicy는 Pod 내의 모든 container에 적용됩니다. 또한 restartPolicy는 exponential back-off delay로 재시작됩니다. 즉, 10초, 20초, 40초로 계속해서 일정수준까지 delay가 늘어납니다. 성공적으로 실행되고 나서 10분이 지나면 해당 delay는 초기화됩니다.\nPod lifetime 일반적으로 Pod는 사람 또는 컨트롤러가 명백하게 이를 지우지 않는 이상 유지됩니다. control plane은 Pod의 총 개수가 지정된 threshold를 초과하면(node마다 정해져 있습니다) 종료된 Pod들(Succeeded 또는 Failed)을 삭제합니다.\n컨트롤러는 3가지 타입이 있습니다.\n Job: batch computations처럼 종료될것으로 예상되는 Pod입니다. ReplicationController, ReplicaSet, Deployment: 종료되지 않을 것으로 예상되는 Pod입니다. DaemonSet: 머신마다 하나씩 동작해야하는 Pod입니다.  Init Container Init container는 app image에서 사용할 수 없거나 사용하지 않는 setup script나 utility들을 포함할 수 있습니다. 실제 app container가 시작되기 전에 먼저 필요한 작업들을 수행하는데 사용됩니다.\nPod Specification에서 containers 배열과 같은 개위로 작성하면 Init container를 사용할 수 있습니다.\nInit container는 completion이 되기 위해 실행됩니다. 따라서 complete상태가 되면 재시작되지 않습니다. completion을 위해 실행되므로 당연하게도 readinessProbe는 사용할 수 없습니다.\ninit container는 여러개를 정의했을 경우 kubelet은 이를 순서대로 실행됩니다. 그리고 각 init container는 다음 init container가 실행되기 전에 반드시 성공적으로 종료되어야 합니다.\ninit container가 실패하면 성공할때까지 재시작합니다. 하지만 Pod의 restartPolicy가 Never이면 init container도 재시작하지 않습니다.\ninit container는 app container가 사용할 수 있는 대부분의 필드를 그대로 사용할 수 있습니다. 일반적인 container와의 차이점은 resource에 대해 다르게 관리된다는 것입니다. 자세한 내용은 공식 홈페이지의 docs를 확인하시기 바랍니다.\nInit container 사용하기  Init container는 app image에는 없는 utility나 custom code를 포함할 수 있습니다. Init container는 동일한 Pod 내에 있는 app container와는 다른 filesystem view를 가질 수 있습니다. 따라서 app container는 접근할 수 없는 Secret을 가지고 동작할 수 있습니다. Init container가 성공할 때까지 Pod의 app container들은 생성되지 않습니다. App container보다 안전하게 utility, custom code를 실행시킬 수 있습니다. 따라서 보안 취약점을 줄일 수 있습니다.  메인 app container를 실행할 때 필요한 configuration file에 필요한 value들을 주입할 때 init container를 사용할 수 있습니다.\nDetailed behavior Pod가 시작되는 동안 network와 volume들이 초기화 된 후 init container가 순서대로 실행되게 됩니다. 각 container는 반드시 다음 container가 실행되기 전까지 성공적으로 종료되어야 합니다.\nInit container에 대한 spec 변경은 container image에 대한것만 가능하다. 그리고 Init container는 idempotent1가 성립해야합니다.\nInit container가 실패 시 계속해서 재시작 되는 것을 막으려면 Pod에 activeDeadlineSeconds와 Container에 livenessProbe를 설정하면 막을 수 있습니다.\nResource 다루는 법  모든 init container에 대해 가장 높은 resource request나 limit은 effective init request/limit이라고 정의합니다. Pod의 effective request/limit은 다음보다 커야합니다.  모든 app container의 resource에 대한 request/limit의 합 resource에 대한 effective init request/lmit   effective request/limits를 기준으로 스케쥴링합니다. 즉, init container의 resource는 Pod의 life 동안 사용되지 않음을 의미합니다. Pod의 effective QoS tier에서 QoS tier는 init container와 app container의 QoS tier와 같습니다.  init container가 재시작되는 경우  user가 pod specification을 업데이트 하여 init container의 이미지가 변경되었을 경우입니다. App container image의 변화는 app container만 재시작시킵니다. Pod infrastructure container가 재시작 되었을 때 Init container가 실행됩니다. restartPolicy가 Always로 설정이 되어있는 상태에서 Pod가 재시작 되었을 때 init container가 이전 완료 상태를 저장한 것이 만료되거나 없을 경우 재시작될 수 있습니다.  Disruptions Pod는 원래 누군가가(사람 또는 컨트롤러) 지우지 않는다면, 또는 피할 수 없는 하드웨어, 소프트웨어적인 에러가 아니라면 삭제되지 않습니다.\n여기서 unavoidable인 경우를 involuntary disruptions라고 부릅니다. involuntary disruption에는 다음과 같은 것들이 있습니다.\n hardware failure cluster administrator가 실수로 VM을 삭제 cloud provider나 hypervisor의 장애로 VM이 삭제됨 kernel panic cluster network partition에 의해 node가 cluster에서 사라짐 노드가 out-of-resource여서 pod의 eviction이 실행됨  voluntary disruption은 application이나 cluster administrator에 의해 시작된 동작들입니다. voluntary disruption에는 다음과 같은 것들이 있습니다.\n 해당 Pod를 관리하고 있던 delployment나 다른 controller의 삭제 deployment의 Pod template update가 재시작을 유발함 직접적으로 Pod를 삭제  Cluster Administrator는 다음이 disruption을 유발할 수 있습니다.\n Upgrade를 위한 Draining Node cluster를 scale down 하기 위해 Draining Node 특정 노드에 띄워야 하는 요구사항 때문에 기존에 있던 해당 노드에서 Pod를 제거  Dealing with Disruptions  Pod에게 충분한 양의 resource 할당하기 고가용성을 원할경우 application을 복제하기 application을 rack또는 zone에 분배하기  How Disruption Budgets Work PodDisruptionBudget(PDB)를 각 application에 설정할 수 있습니다. 이는 voluntary disruption 상황에서 동시에 down될 수 있는 pod의 갯수를 제한합니다.\nPodDisruptionBudget(PDB)를 사용하려면 Cluster Manager는 Eviction API를 통해서 Pod를 삭제해야합니다. 즉, 직접 Pod나 Deployment를 삭제하게 되면 PDB를 사용하지 못하게 됩니다. Eviction API를 사용하는 예시에는 kubectl drain가 있습니다.\nPodDisruptionBudget(PDB)는 involuntary disruption 상황에서는 작동하지 않습니다. 하지만 몇개가 종료되는지는 기록하여 budget에 추가합니다.\nRolloing upgrade 때문에 Pod가 삭제되거나 사용 불가능한 상태일 때에도 PodDisruptionBudget(PDB)는 이를 카운트하지만 PDB때문에 제한되지는 않습니다. application의 업데이트 동안 발생한 장애처리는 controller spec에서 정의내린대로 실행합니다.\n  멱등법칙. 여러번 실행하더라도 동일한 결과를 냄. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "http://kimmj.github.io/css/greater-than-sign/",
	"title": "Greater Than Sign",
	"tags": ["css"],
	"description": "",
	"content": "\u0026gt;의 의미 \u0026gt;는 child-combinator 입니다.\n다음의 예시를 통해 정확히 어떤 역할을 하는지 알아보도록 하겠습니다.\n\u0026lt;div\u0026gt; \u0026lt;p class=\u0026#34;some_class\u0026#34;\u0026gt;Some text here\u0026lt;/p\u0026gt; \u0026lt;!--Selected [1] --\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;p class=\u0026#34;some_class\u0026#34;\u0026gt;More text here\u0026lt;/p\u0026gt; \u0026lt;!--Not selected [2] --\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;/div\u0026gt; 위와 같은 예시에서 div \u0026gt; p.some_class는 div 바로 밑에 있는 p.some_class만을 선택합니다. \u0026lt;blockquote\u0026gt;로 감싸진 p.some_class는 선택되지 않습니다.\n이와는 다르게 space만 사용하는 descendant combinator는 두개의 p.some_class 모두를 선택합니다.\n"
},
{
	"uri": "http://kimmj.github.io/hugo/insert-comment/",
	"title": "Hugo에 Comment 추가하기 (Utterance)",
	"tags": ["hugo", "utterance"],
	"description": "",
	"content": "댓글 서비스 선택 블로그를 운영하는데 관심을 가지기 시작하면서, 기본적으로 jekyll이나 hugo에는 댓글 기능이 없다는 것을 알게 되었습니다. static site를 만드는데 사실 댓글을 지원한다는게 이상한 상황이긴 하지요. 그래도 서드파티의 지원을 받으면 댓글 기능이 가능해집니다. 여러 블로그들을 탐방하며 git page 기능을 사용하는 블로그들에도 댓글이 있는것을 항상 봐왔으니까요.\n따라서 댓글을 어떻게 사용하는지 검색해보게 되었습니다. 대표적인 것이 Disqus 입니다. 실제로 많은 사이트들이 Disqus를 기반으로 댓글 기능을 사용합니다.\n저는 이 hugo 기반 블로그를 만드는 데 큰 도움을 준 https://ryan-han.com/post/etc/creating_static_blog/를 보고 Utterance에 대해 접하게 되었으며 개발자에게는 너무나도 친숙한 깃허브 기반이라는 점이 끌려서 이 서비스를 선택하게 되었습니다.\n적용 방법 적용하는 방법은 너무나도 쉽습니다.\n미리 댓글을 위한 레파지토리를 생각해둡니다(기존에 있는 레파지토리 혹은 댓글만을 위한 레파지토리). 저는 사이트를 렌더링해주는 kimmj.github.io 레파지토리로 선택하였습니다. https://utteranc.es/에 접속합니다. 중간쯤에 보이는 utterances app 하이퍼링크를 클릭합니다. 해당 앱을 적절한 레파지토리에 설치합니다. 설치한 레파지토리를 1번에서 접속한 사이트에 양식에 맞게 기입합니다. Blog Post ↔️ Issue Mapping 섹션에서 적절한 것을 선택합니다. 저는 default 옵션을 사용했습니다. Issue Label은 댓글로 생성된 이슈에 label을 달지, 단다면 어떤 label을 달지 선택하는 것입니다. 저는 comment로 작성했습니다. Theme을 선택합니다. 저는 default 옵션을 사용했습니다. 하단에 생성된 코드를 복사합니다. hugo에서 댓글이 보이게 될 위치에 8번에서 복사한 코드를 붙여넣습니다. 저의 경우 custom-comment.html이라는 파일에 붙여넣기 했습니다. 사이트에 제대로 뜨는지 확인합니다.  후기 이렇게 hugo에 댓글 기능을 추가하였습니다. 생각보다 너무나도 간단했네요. 이 댓글 기능을 하자고 생각하고 나서 적용까지 5분정도 걸렸던 것 같습니다. 댓글도 너무나 친숙한 레이아웃이라 정감이 가는 것 같네요.\nReference  https://github.com/Integerous/TIL/blob/master/ETC/Hugo%2BGithub_Page.md https://utteranc.es/  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/tools/tmux/",
	"title": "Tmux",
	"tags": ["tmux", "ubuntu"],
	"description": "",
	"content": "tmux란? tmux는 하나의 화면에서 여러개의 터미널을 키고싶을 때 사용하는 프로그램으로, ubuntu를 설치하면 기본적으로 설치되는 프로그램입니다.\n다음과 같은 구조를 가집니다.\ntmux ├── session │ ├── windows │ │ ├── pane │ │ └── pane │ └── windows │ ├── pane │ └── pane └── session ├── windows │ ├── pane │ └── pane └── windows ├── pane └── pane session 사용법 먼저 가장 큰 단위인 session을 다루는 방법부터 시작해보도록 하겠습니다.\nsession 생성 tmux 위처럼 tmux를 생성할 수 있습니다. 이 경우 tmux session의 이름은 0부터 차례로 증가하는 숫자로 정의됩니다.\n여기에 session의 이름을 사용자가 정의할 수도 있습니다.\ntmux new -s \u0026lt;my-session\u0026gt; 이렇게 이름을 지어놓으면 용도에 따른 session을 구분할 때 좋습니다.\nsession에 접속하기 이번에는 이미 생성된 session에 접속하는 방법입니다.\ntmux a tmux a -t \u0026lt;my-session\u0026gt; tmux attach tmux attach -t \u0026lt;my-session\u0026gt; -t 옵션을 줘서 이름을 지정할 수도 있고, (임의로 생성된 숫자 또한 마찬가지입니다.) 옵션없이 실행할 경우 가장 최근 열린 session으로 접속합니다.\nsession 확인하기 session어떤 것들이 있는지 보려면 ls를 이용하면 됩니다.\ntmux ls 또는 이미 session에 들어간 상태에서, session들의 리스트를 봄과 동시에 미리보기 화면으로 어떤 작업중이었는지도 볼 수 있습니다.\n[prefix] s 여기서 [prefix]는 일반적으로 Ctrl+b를 의미하며, 사용자에 의해 변경될 수 있습니다.\nsession에서 빠져나오기 session을 빠져나오는 방법에는 두가지가 있습니다.\n 완전히 session을 로그아웃하여 session 삭제하기 session이 계속 돌아가는 상태에서 빠져나오기  1번의 경우는 모든 pane에서 log out을 하면 되므로 생략하도록 하겠습니다. 두번째 session이 계속 돌아가는 상태에서 빠져나오는 방법은 다음과 같습니다.\n[prefix] d session 죽이기 tmux 바깥에서 session을 없애려면 들어가서 로그아웃을 통해 끄는 방법도 있을테지만, kill-session이라는 명령어를 통해서도 session을 없앨 수 있습니다.\ntmux kill-session -t \u0026lt;my-session\u0026gt; session 이름 바꾸기 session에 들어가 있는 상태에서 현재 session의 이름을 변경할 수 있습니다.\n[prefix] $ 그러면 상태표시줄에서 session의 이름을 정해줄 수 있습니다.\nwindows 사용법 windows는 session내의 tab과 같은 존재입니다. session은 큰 사용 목적으로 묶어준다면 windows는 그에따라 tab으로 관리할 필요가 있을 때 사용하면 좋습니다. 물론 session과 pane만 가지고 사용해도 되지만, 그보다 더 유연하게 하려한다면 windows도 알아두는 것이 좋습니다.\nwindows의 생성 windows를 관리하고자 한다면, 우선 session에 들어가 있는 상태여야 합니다.\n[prefix] c 위처럼 windows를 생성하고 나면 아래 tmux 상태표시줄에 0:bash- 1:bash*와 같은 형태로 windows가 추가됨을 볼 수 있습니다. 여기서 *는 현재 사용중인 windows를 의미합니다.\nwindows 움직이기 먼저 기본적으로 앞, 뒤로 움직이는 방법입니다.\n[prefix] n # next window [prefix] p # previous window session에서 미리보기를 사용하여 순회했던 것처럼, windows도 list들을 미리보기형식으로 순회할 수 있습니다.\n[prefix] w windows 이름 변경하기 특정 window에 들어가있는 상태에서 해당 window의 이름을 변경할 수 있습니다.\n[prefix] , windows 죽이기 현재 window를 죽이려면 로그아웃을 하는 방법도 있지만, 강제로 죽이는 방법도 존재합니다.\n[prefix] \u0026amp; panes 다루기 pane이란 tmux의 화면을 분할하는 단위입니다. tmux를 사용하는 가장 큰 이유라고 볼 수 있습니다.\npane 분할하기 [prefix] % # vertical split [prefix] \u0026#34; # horizontal split 위와같은 방법으로 pane을 생성할 수 있습니다.\npane 이동하기 먼저 기본적으로 방향키를 이용하여 움직일 수 있습니다.\n[prefix] \u0026lt;방향키\u0026gt; 일정시간이 지나면 pane내에서의 방향키 입력으로 전환되어 pane을 움직일 수 없으므로 빠르게 움직여줍니다.\npane 위치 바꾸기 pane들을 rotate하는 방법입니다.\n[prefix] ctrl+o # 시계방향으로 회전 [prefix] alt+o # 반시계방향으로 회전 또는 하나를 이동할 수도 있습니다.\n[prefix] { # move the current pane left [prefix] } # move the current pane right pane 크게 보기 pane을 여러개 쓰다가 하나만 크게 보고싶은 경우가 있을 수 있습니다.\n[prefix] z 돌아가는 방법 또한 같은 명령어를 통해 할 수 있습니다.\n[prefix] z pane을 새로운 window로 분할하기 특정 텍스트를 마우스로 복사하거나 여러가지 상황에서 새로운 window로 분할하는 것이 편한 경우가 있습니다.\n[prefix] ! 모든 pane에서 동시에 입력하기 tmux를 통해서 여러개의 서버에 ssh 접속을 한 뒤, 동시에 같은 입력을 하게 할 수도 있습니다.\n[prefix] :set synchronize-panes yes (on) [prefix] :set synchronize-panes no (off) Tips tmux에는 기본 내장된 layout이 있습니다. 이를 통해서 pane들을 resize하지 않고 기본 형식에 맞게 쉽게 변경이 가능합니다. 그 중 가장 자주 사용할 수 있는 것은 다음 두가지입니다.\n[prefix] alt+1 # 수직 [prefix] alt+2 # 수평 Reference  https://gist.github.com/MohamedAlaa/2961058 https://gist.github.com/andreyvit/2921703  "
},
{
	"uri": "http://kimmj.github.io/prometheus/federation/",
	"title": "Federation",
	"tags": ["prometheus", "federation"],
	"description": "",
	"content": "What is Federation 영어 의미 그대로는 \u0026ldquo;연합\u0026quot;이라는 뜻입니다. 즉, Prometheus의 Federation은 여러개의 Prometheus에서 Metric을 가져와 계층구조를 만드는 것을 의미합니다.\n위의 그림에서 너무나도 잘 표현이 되어 있습니다. 그림에서 보시면 상위에 있는 Prometheus에서 하위의 Dev, Staging, Production쪽으로 화살표가 간 것을 볼 수 있습니다. 이는 아래에 있는 Prometheus가 http(s)://\u0026lt;url\u0026gt;/federation으로 보여주는 Metric들을 위쪽에 있는 Prometheus에서 scrape하기 때문입니다.\n저의 상황을 설명해드리고 지나가도록 하겠습니다. 저는 Kubernetes Cluster가 Dev(Canary), Staging, Production과 비슷하게 3개 있었습니다. 여기서 Spinnaker를 통해 Dev에 새로운 이미지들을 배포할 것이고, 이에 대한 Metric을 Canary Analysis를 통해 분석하여 Dev로 배포된 이미지가 이전 Staging의 이미지와 어떻게 다른지 등을 점수화하여 Staging 서버에 배포를 할지 말지 결정하도록 해야하는 상황이었습니다.\n이 때, Spinnaker의 설정상 Prometheus를 연동하고 나서 Canary Config를 설정할 때 하나의 Prometheus만 바라보도록 할 수 있었습니다. 따라서 여러대의 Promethus의 Metric을 비교하기 위해서는 여러대의 Prometheus가 가지고 있는 Metric을 상위개념의 Prometheus가 scrape하도록 해야했습니다. 어떻게 해야하는지 검색해본 결과 Federation이라는 기능이 있는 것을 알게 되었습니다.\nHow to configure Prometheus Federation Prerequisites 우선 여러대의 Prometheus가 필요합니다. 그리고 이를 하나로 모아줄 또 다른 Prometheus가 필요합니다.\n저의 경우 docker-compose를 통해 Prometheus를 구동하여 다른 서버의 Prometheus Metric을 가져오도록 설정하였습니다. Install에 관해서는 다른 문서를 참고해 주시기 바랍니다.\nConfiguring federation 다음은 공식 사이트에 나온 federation의 구성입니다.\nscrape_configs: - job_name: \u0026#39;federate\u0026#39; scrape_interval: 15s honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; params: \u0026#39;match[]\u0026#39;: - \u0026#39;{job=\u0026#34;prometheus\u0026#34;}\u0026#39; - \u0026#39;{__name__=~\u0026#34;job:.*\u0026#34;}\u0026#39; static_configs: - targets: - \u0026#39;source-prometheus-1:9090\u0026#39; - \u0026#39;source-prometheus-2:9090\u0026#39; - \u0026#39;source-prometheus-3:9090\u0026#39; job_name job_name은 static_configs[0].targets에 적힌 Prometheus Metric에 어떤 job=\u0026quot;\u0026lt;job_name\u0026gt;\u0026quot;을 줄지 결정하는 것입니다. 이를 통해 저는 Canary, Stage 서버를 구분하였습니다.\nscrape_interval 얼마나 자주 Metric을 긁어올 지 결정하는 것입니다.\nmetrics_path 어떤 path에서 Metric을 가져오는지 설정합니다. 보통의 경우 federation으로 설정하면 됩니다.\nparams 실제로 \u0026lt;PrometheusUrl\u0026gt;/federation으로 접속해보면 아무것도 뜨지 않습니다. /federation은 param로 매칭이 되는 결과만 리턴하며, 없을 경우 아무것도 리턴하지 않습니다. 따라서 이는 필수 필드이고, 원하는 job만 가져오게 하거나 {job=~\u0026quot;.+\u0026quot;}와 같은 방법으로 모든 Metric을 가져오게도 할 수 있습니다.\nstatic_configs 어떤 Prometheus에서 Metric을 가져올지 결정하는 것입니다.\nValidation 위와같이 설정을 한 뒤, Prometheus에 접속하여 Targets에 들어가 봅니다. 리스트에 설정한 job_name들이 떠있고, UP인 상태로 있으면 정상적으로 구성이 된 것입니다.\nReference https://prometheus.io/docs/prometheus/latest/federation/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/canaryanalysis/canary-analysis/",
	"title": "Canary Analysis",
	"tags": ["canary", "canary-update", "spinnaker"],
	"description": "",
	"content": "Spinnaker Canary Analysis Spinnaker에는 Canary Analysis라는 자동 분석 도구가 있습니다. Kayenta라는 micro service를 사용하는데, 이를 통해 자동으로 canary deploy가 괜찮은 버전인지를 확인해 줍니다.\n그러나 이 툴은 Spinnaker에서 사용하기에 여간 어려운 것이 아닙니다. 제일 먼저 봉착하는 난관은 바로 \u0026ldquo;어떻게 Canary Analysis를 활성화 하는가?\u0026ldquo;입니다.\n이곳에 방법이 나와있지만, 사실 저도 엄청 많이 헤멨습니다. 저는 bare-metal 환경에서 Kubernetes cluster를 구축하였었고, aws나 azure, gcp는 사용하지 못하는 상황었습니다. (물론 지금도 집에서 VM으로 로컬에 구성하였지만, cloud platform은 언제나 과금때문에 꺼려지게 됩니다.)\n이런 상황에서 어떻게 Canary Analysis를 활성화했는지부터, 어떻게 이를 통해 Metric을 비교하는지까지 한번 알아보도록 하겠습니다.\nPrerequisites 첫번째로 metric service를 선택해야 합니다. 여러가지가 있을 수 있겠지만, 저는 로컬에서 사용할 수 있는 Prometheus를 사용할 것입니다.\n두번째로 가져온 metric의 결과들을 저장해 놓을 storage service가 필요합니다. 저는 Spinnaker를 구성할 때 minio를 storage service로 구축을 했었으므로, 여기에서도 마찬가지로 minio를 통해 데이터를 저장할 것입니다.\nHow to enable Canary Analysis 제일 먼저 hal command를 통해서 Canary Analysis를 활성화시켜야 합니다.\nhal config canary enable 그다음엔 Prometheus를 canary analysis에 사용되도록 설정할 것입니다.\nhal config canary prometheus enable hal config canary prometheus account add my-prometheus --base-url http://192.168.8.22/9090 이처럼 Prometheus 콘솔창이 보이는 url을 입력하면 됩니다. 저는 docker-compose를 통해서 9090 port로 expose 시켰으므로 위와같이 적어주었습니다.\n그 다음에는 metric provider를 설정합니다. 앞서 말했듯이 여기서는 Prometheus를 사용할 것입니다.\nhal config canary edit --default-metrics-store prometheus hal config canary edit --default-metrics-account my-prometheus 위의 두 설정으로 Prometheus가 default metric store로 선택되었습니다. 이는 물론 나중에 canary configuration에서 원하는 것으로 선택할 수도 있습니다. 또한 default account도 my-prometheus라는 이름으로 선택해 주었습니다.\n이번엔 default storage account를 설정할 것입니다. 공식 docs에서는 minio에 관련된 설정방법이 잘 나와있지 않습니다.\n하지만 hal command를 잘 보시면 어떻게 해야할지 감이 약간은 잡히실 것입니다.\n--api-endpoint에는 minio의 url을 적고, --aws-access-key-id에는 minio의 ID였던 minio를 입력합니다. 그리고 --aws-secret-access-key는 minio의 PW인 minio123을 입력합니다.\nhal config artifact s3 account add my-minio \\  --api-endpoint http://192.168.8.22:9000 \\  --aws-access-key-id minio \\  --aws-secret-access-key minio123 그 뒤에는 위에서 입력했던 my-minio storage account를 Canary Analysis에 사용하도록 설정하면 됩니다.\nhal config canary edit --default-storage-account my-minio Validate 제대로 설정을 마쳤으면 pipeline의 config에서 Feature 탭에 Canary가 추가된 것을 볼 수 있습니다. 이를 활성화하면 본격적으로 Canary Analysis를 사용할 수 있습니다.\nReference https://www.spinnaker.io/guides/user/canary/\nhttps://www.spinnaker.io/guides/user/canary/config/\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/change-hostname/",
	"title": "Hostname 변경하기",
	"tags": ["hostname", "ubuntu"],
	"description": "",
	"content": "hostname을 바꾸는 일은 흔치 않지만 최초 셋업할 때 많이 사용하곤 합니다.\n# hostnamectl set-hostname \u0026lt;host name\u0026gt; hostnamectl set-hostname wonderland 변경 후 터미널을 끄고 재접속을 하면 변경된 사항을 볼 수 있습니다.\nhostname "
},
{
	"uri": "http://kimmj.github.io/hugo/ibiza/font-change/",
	"title": "Font Change",
	"tags": ["font", "hugo"],
	"description": "",
	"content": "Ibiza 프로젝트를 진행하는데 폰트가 마음에 들지 않았습니다. 따라서 저는 폰트를 변경하기로 마음먹었습니다.\n먼저, 폰트 설정을 어디서 하는지 알아낼 필요가 있었습니다.\nfind . | grep font 결과를 보니, theme 폴더 안에 제가 사용하는 hugo-theme-learn 테마에서 static/fonts/ 폴더에 폰트들을 저장해두고 있었습니다. 그렇다면 어느 파일에서 어떤 폰트를 사용한다고 설정할까요?\nhugo-theme-learn폴더로 이동하여 어디에 사용되는지 확인해보았습니다.\ngrep -ri \u0026#34;font\u0026#34; 결과가 길게 나오는데요, 여기서 static/css/theme.css 안에 폰트에 대한 설정을 한 것이 보였습니다. 그 파일을 보니, @font-face라는 설정이 보이네요. 여기서 Work Sans라는 폰트를 불러오고 있었습니다.\n이 폰트를 Noto Sans CJK KR이라는 폰트로 바꾸려고 합니다. 따라서 먼저 폰트를 다운로드 받아야 합니다.\n다운로드 페이지 : https://www.google.com/get/noto/#sans-kore\n여기에서 다운로드 버튼을 눌러 폰트를 다운받습니다.\ncurl -o noto-mono.zip https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKkr-hinted.zip 이를 my-custom-theme 폴더 내의 static/fonts 폴더 안에다가 압축해제할 것입니다.\nmv noto-mono.zip mj-custom-theme/static/fonts unzip noto-mono.zip rm noto-mono.zip README 그러고나서 font-face 설정을 바꾸어 보도록 하겠습니다. 처음에는 이 설정으로 폰트가 정말 바뀌는지 확인해보기 위해 현재 사용중인 폰트의 url 부분을 Noto Mono 폰트로 변경해보았습니다.\n@font-face { font-family: \u0026#39;Work Sans\u0026#39;; font-style: normal; font-weight: 500; src: url(\u0026#34;../fonts/NotoSansMonoCJKkr-Bold.otf?#iefix\u0026#34;) format(\u0026#34;embedded-opentype\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Bold.otf\u0026#34;) format(\u0026#34;woff\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.svg#WorkSans\u0026#34;) format(\u0026#34;svg\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.ttf\u0026#34;) format(\u0026#34;truetype\u0026#34;); } 하지만 예상과 다르게 변경되지 않았습니다. 확인해보니 이는 font-weight이라는 설정때문이었습니다. body에 대한 font-weight은 300으로 설정이 되어있었고, 따라서 제가 설정한 폰트가 아닌 font-weight: 300인 폰트를 선택하게 된 것입니다.\n다시한번 body쪽의 font-weight을 500으로 바꾸어 실험해보았습니다.\nbody { font-family: \u0026#34;Work Sans\u0026#34;, \u0026#34;Helvetica\u0026#34;, \u0026#34;Tahoma\u0026#34;, \u0026#34;Geneva\u0026#34;, \u0026#34;Arial\u0026#34;, sans-serif; font-weight: 500; line-height: 1.6; font-size: 18px !important; } 그러자 제가 원하는 폰트로 변경이 된 것을 확인하였습니다. 다시 위로 돌아가서 @font-face 설정을 저의 폰트 이름으로 변경하고 제가 원래 하려던 폰트로 변경하였습니다. font-weight: 300으로 다시 돌려놓았고, 새로운 폰트를 font-weight: 300으로 주었습니다.\n@font-face { font-family: \u0026#39;Noto Mono Sans CJK KR \u0026#39;; font-style: normal; font-weight: 300; src: url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.eot?#iefix\u0026#34;) format(\u0026#34;embedded-opentype\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.woff\u0026#34;) format(\u0026#34;woff\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.svg#NotoSansMonoCJKkr\u0026#34;) format(\u0026#34;svg\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.ttf\u0026#34;) format(\u0026#34;truetype\u0026#34;); } 여기서 보시면 format 속성들이 많은 것을 볼 수 있습니다. 이는 브라우저별로 지원하는, 지원하지 않는 폰트들에 대해서 처리를 해주기 위한 것입니다. 저는 폰트를 다운로드 받았을 때 otf 포멧밖에 없었습니다. 따라서 다른 포멧들로 변경해 줄 필요가 있었습니다.\nfont converter : https://onlinefontconverter.com/\n위 사이트에서 제가 필요한 eot, woff, woff2, svg, ttf 파일들로 변환후 저장했습니다.\n이제 body의 css에서 font-family 맨 앞에 앞서 정의한 폰트를 지정해줍니다.\nbody { font-family: \u0026#34;Noto Sans Mono CJK KR\u0026#34;, \u0026#34;Work Sans\u0026#34;, \u0026#34;Helvetica\u0026#34;, \u0026#34;Tahoma\u0026#34;, \u0026#34;Geneva\u0026#34;, \u0026#34;Arial\u0026#34;, sans-serif; font-weight: 300; line-height: 1.6; font-size: 18px !important; } 확인해보니 제대로 적용이 되었네요.\nbefore font change after font change Reference  https://wit.nts-corp.com/2017/02/13/4258 https://aboooks.tistory.com/153 https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/webfont-optimization?hl=ko  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/network/netplan/",
	"title": "Netplan으로 static IP 할당받기",
	"tags": ["ubuntu-18.04", "netplan", "static-ip"],
	"description": "",
	"content": "유선 static IP 할당 다음과 같이 static-IP-netplan.yaml을 작성합니다.\nnetwork: version: 2 ethernets: enp3s0: dhcp4: no dhcp6: no addresses: [ 192.168.1.26/24 ] gateway4: 192.168.1.1 nameservers: addresses: [ 8.8.8.8, 8.8.4.4 ] 하나씩 살펴보도록 하겠습니다.\n ethernetes: 유선랜 설정입니다. enp3s0 설정을 사용할 랜카드입니다. dhcp4, dhcp6: dynamic으로 IP를 할당받는 dhcp를 disable한 것입니다. addresses: 사용할 IP 및 CIDR입니다. gateway4: IP가 사용하는 gateway입니다. nameservers: dns 주소입니다. 8.8.8.8과 8.8.4.4를 사용합니다.  WIFI static IP 할당 wifis: wlp2s0: dhcp4: no dhcp6: no addresses: [ 192.168.8.26/24 ] gateway4: 192.168.8.30 nameservers: addresses: [ 8.8.8.8, 8.8.4.4 ] access-points: \u0026#34;my-SSID\u0026#34;: password: \u0026#34;my-password\u0026#34; 하나씩 살펴보도록 하겠습니다.\n wifis: wifi에 대한 설정입니다. wlp2s0: 무선 랜카드입니다. access-points: 사용할 wifi에 대한 설정입니다. \u0026quot;my-SSID\u0026quot;: wifi의 SSID 즉, wifi 이름입니다. password: 해당 wifi의 비밀번호를 적으면 됩니다.  만약 ubuntu server를 사용한다면, wifi 관련 패키지는 초기에 설치되지 않습니다. 따라서 wpasupplicant를 설치해주어야 합니다.\nsudo apt install wpasupplicant "
},
{
	"uri": "http://kimmj.github.io/hugo/hugo-with-html/",
	"title": "HUGO로 HTML이 되지 않을 때 가능하게 하는 방법",
	"tags": ["hugo", "html"],
	"description": "",
	"content": "Hugo는 markdown을 기본적으로 사용하지만 html을 이용해서 좀 더 다양하게 커스터마이징이 가능한 장점도 가지고 있습니다.\n하지만 저는 처음에 html 코드를 사용하게 되면 \u0026lt;!-- raw HTML omitted --\u0026gt;와 같은 줄로 대치가 되곤 했습니다. 구글링 결과 이는 Hugo의 버전이 0.60.0으로 되면서부터 기본적으로 disable 시켰기 때문입니다.\n따라서 다음과 같이 조치를 하면 간단하게 해결이 가능합니다.\n[markup.goldmark.renderer] unsafe= true 위와 같은 설정을 config.toml에 추가하기만 하면 됩니다. 추가를 한 뒤 다시 확인해보면 정상적으로 html 코드가 적용된 모습을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/tips/pipeline-expressions/",
	"title": "Pipeline Expressions",
	"tags": ["spinnaker", "pipeline"],
	"description": "",
	"content": "Spinnaker는 배포를 자동화할 때 사용합니다. 그렇기 때문에 자동화를 위해선 다른 곳에서 사용된 값들을 가지고 와야할 필요성이 생기기도 합니다.\n이 문서에서는 그럴 때 사용할 수 있는 pipeline function에 대해 알아보도록 하겠습니다.\npipeline functions pipeline에서 다른 pipeline의 값들 불러오기 Note: Pipeline expression syntax is based on Spring Expression Language (SpEL).\n 위의 Note에도 적었듯이, Spinnaker는 SpEL을 기반으로 Expressions를 사용합니다. SpEL에 대해 이미 잘 알고있다면 너무나도 좋겠지만, 저는 익숙하지가 않았기 때문에 많은 시행착오를 거쳐서 습득을 하게 되었습니다.\n기본적으로 ${ expression }의 형태를 가지게 됩니다.\n여기서 한가지 기억해 두어야 할 것은 nested가 되지 않는다는 것입니다. 즉, ${ expression1 ${expression2} }가 되지 않습니다.\n언제 pipeline expression을 사용하나요 pipeline expression은 Spinnaker UI로는 해결할 수 없는 문제들을 해결하여줍니다. 예를 들어 특정 stage가 성공했는지의 여부에 따라 stage를 실행할지, 말지 결정하는 방법을 제공해 줍니다. 또는 가장 최근에 deploy된 pod를 알아낸다거나, spinnaker를 통한 canary analysis를 할 때 비교할 두가지 대상을 선택하기 위해 사용할 수도 있습니다.\nSpinnaker는 모든 파이프라인을 JSON 형태로도 관리할 수 있기 때문에, UI에는 없는 값들도 입력할 수 있습니다. 이렇게 좀 더 유연한 방법으로 Spinnaker를 이용하고자 한다면 pipeline expression은 꼭 알아두어야 합니다.\n원하는 값을 어떻게 찾나요 pipeline이 구동되고 나면, Details를 누르고 Source를 눌렀을 때 해당 pipeline의 실행결과가 json형태로 출력됩니다. 이를 VS Code나 다른 편집기를 이용하여, json으로 인식하게 한 뒤, 자동 들여쓰기를 하면 보기 좋게 만들어줍니다.\n이를 통해서 어떤 값을 내가 사용할 지 확인하여 pipeline expression을 작성하면 됩니다.\n내가 작성한 pipeline expression은 어떻게 테스트하나요 작성한 pipeline expression을 테스트하기 위해 파이프라인을 구동한다는 것은 끔직한 일입니다. Spinnaker는 이를 테스트하기 위해 API endpoint를 제공합니다. 즉, 파이프라인을 다시 구동시키지 않고도 어떤 결과값이 나오는지 확인할 수 있다는 것을 의미합니다.\n테스트 방법은 간단합니다. 다음과 같이 curl을 통해 endpoint로 테스트하면 됩니다.\nPIPELINE_ID=[your_pipeline_id] curl http://api.my.spinnaker/pipelines/$PIPELINE_ID/evaluateExpression \\  -H \u0026#34;Content-Type: text/plain\u0026#34; \\  --data \u0026#39;${ #stage(\u0026#34;Deploy\u0026#34;).status.toString() }\u0026#39; 여기서 api.my.spinnaker는 Gate의 Service를 보고 포트를 참조하여 작성하면 됩니다. 기본값은 localhost:8084입니다. 이렇게 하면 Deploy라는 stage가 성공했을 때 다음과 같은 결과를 볼 수 있습니다.\n{\u0026#34;result\u0026#34;: \u0026#34;SUCCEEDED\u0026#34;} Spinnaker가 expression을 통해 결과를 만들어내지 못한다면 다음과 같이 에러와 로그가 발생합니다.\n{ \u0026#34;detail\u0026#34;: { \u0026#34;{ #stage(\\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Failed to evaluate [expression] Expression [{ #stage( #root.execution, \\\u0026#34;Deploy\\\u0026#34;).status.toString() ] @0: No ending suffix \u0026#39;}\u0026#39; for expression starting at character 0: { #stage( #root.execution, \\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34;, \u0026#34;exceptionType\u0026#34;:\u0026#34;org.springframework.expression.ParseException\u0026#34;, \u0026#34;level\u0026#34;:\u0026#34;ERROR\u0026#34;, \u0026#34;timestamp\u0026#34;:1531254890849 } ] }, \u0026#34;result\u0026#34;:\u0026#34;${#stage(\\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34; } Reference Spinnaker Docs: https://www.spinnaker.io/guides/user/pipeline/expressions/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/tips/",
	"title": "Tips",
	"tags": [],
	"description": "",
	"content": "Spinnaker Tips spinnaker를 운영하며 생기는 팁들을 모아보았습니다.\n Pipeline Expressions     "
},
{
	"uri": "http://kimmj.github.io/ansible/create-vm-with-ansible-libvirt/",
	"title": "Create Vm With Ansible Libvirt",
	"tags": ["ansible", "libvirt"],
	"description": "",
	"content": "Ansible은 어떠한 프로세스를 자동화 할 때 사용할 수 있는 툴입니다. 그리고 libvirt는 linux 환경에서 qemu를 이용하여 VM을 생성할 때 사용하는 python 모듈입니다.\n이 두가지를 합하여 Ansible을 통해 VM을 생성하는 방법에 대해 알아보도록 하겠습니다.\nansible-role-libvirt-vm 참조 Github : https://github.com/stackhpc/ansible-role-libvirt-vm\n위의 Github 프로젝트는 libvirt를 ansible에서 사용할 수 있도록 만든 오픈소스입니다. 이를 이용하여 ansible-playbook을 통해 VM을 생성해 볼 것입니다.\n이를 로컬에 clone 합니다.\ngit clone https://github.com/stackhpc/ansible-role-libvirt-vm 테스트 환경 저는 Ubuntu 18.04.3 Desktop을 사용하고 있습니다. 그리고 설치에 사용될 iso는 제 포스트에서 작성한 적이 있었던 preseed.cfg를 이용한 자동 설치 이미지입니다. 따라서 이미지를 넣고 부팅만 하면 실행할 수 있습니다.\nplay.yaml  저는 이러한 play.yaml 파일을 사용하였습니다.\n여기서 cdrom을 사용하였는데, 이미지는 baked-ubuntu.iso를 사용하였습니다.\n또한 장비들에 대한 설정을 xml로 추가적으로 하고싶어서 xml_file을 설정해 주었습니다.\nxml_file또한 업로드 해두었습니다.\n 네트워크는 설정을 빼놓을 경우 설치중에 확인창이 발생하여 기본적으로 NAT를 사용하도록 하였습니다. 이는 필요에 따라 변경을 해야 합니다. 또한 enable_vnc의 경우 virt-manager를 통해 상황의 경과를 확인하고 싶어서 추가하였습니다.\n위의 파일들을 workspace에 두시면 됩니다.\nTest 이렇게까지 한 뒤 play.yaml이 있는 위치에서 시작합니다.\n그러면 ansible-playbook은 ansible-role-libvirt-vm이라는 role을 해당 위치에서 검색하고, 실행이 될 것입니다.\nansible-playbook play.yaml 실행 중 sudo 권한이 필요하다고 할 수도 있습니다. 이럴 경우 sudo su로 잠시 로그인 후 exit로 빠져나오시면 에러가 발생하지 않습니다.\n확인 virt-manager를 통해 GUI 환경에서 실제로 잘 되고 있는지 확인할 수 있습니다.\nvirt-manager preseed.cfg를 사용한 이미지라면 30초 후 설치 언어가 자동으로 영어로 설정이 되면서 계속해서 설치가 진행될 것입니다.\n마치며 vm을 생성하는 일이 잦다면, 이 또한 굉장히 귀찮은 일이 아닐 수 없습니다. 소규모가 아닌 대규모로의 확장성을 생각한다면 당연히 자동화를 하는 것이 올바른 접근이라고 생각합니다.\nVM 설치 자동화의 방법이 여러가지가 있을 것이고 이 방법 또한 그 여러가지 방법 중 하나입니다.\n더 좋은, 더 편한 방법이 있다면 알려주시면 감사하겠습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/unattended-ubuntu/",
	"title": "추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기",
	"tags": ["ubuntu-18.04-server", "preseed.cfg"],
	"description": "",
	"content": "어디에 좋을까 Ubuntu Server를 설치하기 위해서는 많은 추가 입력이 있어야 합니다. 사용자가 어떻게 설치하기를 원하는지 모르기 때문에, 또 다양한 옵션을 사용자가 선택하기 위해서는 어찌보면 당연한 것이겠지요. 하지만 만약 똑같은 설정을 사용할 것인데, 여러대의 서버에 OS를 설치하는 상황이라고 생각해보면 정말 암울합니다. 온전히 시간을 OS 설치에만 투자하자니 이건 간단한 업무로 인해 다른 업무를 보지 못하게 됩니다. 또 다른 업무와 동시에 하자니 다음 입력창이 뜰 때인지 한번씩 확인해 주어야 합니다.\n따라서 어차피 같은 설정을 한다면, 이러한 설정을 미리 해 놓는 방법이 Ubuntu iso 파일 내부에 있을 것이라고 추측했습니다. 분명 누군가가 이런 불편함을 해결했으리라 생각했죠. 다행이 몇번의 구글링을 통해 preseed.cfg라는 파일이 제가 말했던 사용자의 입력을 미리 정해놓는 파일이라는 것을 알 수 있었습니다.\n이 preseed.cfg 파일을 잘만 활용한다면, 서버에 OS를 설치할 때 불필요한 시간 낭비를 줄일 수 있을 것입니다.\n차라리 VM이었다면, 그냥 VM을 복사해서 IP나 MAC, hostname 같은 것들만 변경해도 됐을 수 있습니다. 하지만 preseed.cfg를 이해하게 되면 언제 어디서든 내가 원하는 설정을 해주는 우분투 설치 파일을 만들 수 있을 것입니다.\n사전 준비 먼저, 설정을 넣어줄 Ubuntu 18.04 Server가 필요합니다. 물론 Ubuntu 18.04 Desktop에도 적용이 될 것으로 보입니다. (검색했을 때 대부분이 Desktop 설치 이미지에 관한 내용이었으니까요.)\n여기서 중요한 점은 live라고 적혀있는 이미지가 아니어야 합니다. live가 붙은 것은 인터넷으로 파일들을 다운로드 받게 되고, 그럴 경우 오프라인 설치가 필요한 환경에서는 적합하지도 않고 작업할 때 필요한 파일 또한 없습니다.\n두번째로 중요한 점은 amd64입니다. 처음에 잘못받고 arm64를 다운받았었는데, 내부 파일들의 폴더 명도 다르고 동작방식도 달라 구글링을 통해 amd64 이미지를 따로 받았습니다.\npreseed.cfg 작성 제가 설정했던 preseed.cfg 파일은 다음과 같습니다.\n 설명은 후에 추가하도록 하겠습니다.\niso 파일 생성하기 크게 순서를 정한다면 이렇게 됩니다.\n initrd.gz를 압축해제한 뒤 preseed.cfg 관련 정보를 initrd.gz에 추가 다시 initrd.gz로 압축 md5sum을 통한 checksum 재생성 genisoimage를 통한 부팅용 이미지 생성  그러나 preseed.cfg를 수정할 때마다 이를 반복하는 것은 여간 귀찮은 일이 아닐 수 없습니다. 그래서 저는 이를 bakeIsoImage.sh이라는 간단한 shell 프로그램으로 만들어서 iso파일을 생성하도록 하였습니다.\n 설치 테스트 위의 방식대로 진행을 했다면 baked-ubuntu.iso라는 파일이 생성되었을 것입니다. 이를 virt-manager나 virtual box등을 통해 가상머신을 생성하여 설치 테스트를 합니다.\n설치를 하면서 아무런 입력을 하지 않았다면, 원래의 의도대로 잘 설치가 된 것이라고 볼 수 있겠네요.\n마치며 preseed.cfg라는 엄청나게 유용한 방법이 있음에도 불구하고, 공식적인 가이드 자체가 많이 없는 상황입니다. 어떤 옵션들이 있는지도 잘 모르고, 설명도 자세히 되어있지 않았습니다. 단지 주어진 것이라고는 공식 문서에서 예시로 제공하는 preseed.cfg 파일 하나와, 다른사람들이 작성해 놓은 파일들 뿐이었습니다.\n저 또한 입력없이 설치하는 우분투 설치 이미지를 만들기 위해 고군분투했습니다. 누군가가 이 글을 통해서 환경에 맞는 설정을 해주는 우분투 설치 이미지를 생성하여 자동화를 할 수 있게된다면 정말 좋을 것 같습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/how-to-edit-boot-parameter-during-install/",
	"title": "Ubuntu 설치 시 Boot Parameter를 수정하기",
	"tags": ["ubuntu", "install", "boot parameter"],
	"description": "",
	"content": "Ubuntu 설치할 때 boot parameter가 필요한 상황이 간혹 발생할 수 있습니다.\n특히 저의 경우, preseed.cfg를 수정하기 위해 인스톨러가 질의하는 것이 preseed.cfg의 어떤것과 대응이 되는지를 보기 위해 DEBCONF_DEBUG=5라는 옵션을 boot parameter로 주어야 했습니다. 이 때 사용할 수 있는 방법을 소개드립니다.\n먼저 평소와 같이 ubuntu를 설치하기 위해 설치 이미지를 삽입합니다. 그 다음에는 언어를 선택하시면, 다음으로 넘어가기 전에 메뉴가 뜹니다.\n이 상태에서 F6을 누르시면 옵션을 선택할 수 있고, 이 때 ESC키를 누르면 boot parameter가 하단에 보일 것입니다. 여기서 원하는 boot parameter를 입력하면 됩니다.\n이 때, 위아래 방향키를 누르게 되면 입력했던 내용이 사라지게 됩니다. 따라서 미리 맨 위 install ubuntu에 커서를 올리고 수정하시기 바랍니다.\ninstall 시에 설정으로 넣어버리기 preseed.cfg로 미리 질문에 대한 답을 다 정할 수 있었듯이, boot parameter 또한 미리 설정할 수 있습니다. 해당 파일은 iso 파일을 압축해제 하였을 때, /isolinux/txt.cfg 파일 내에 있습니다.\ngrep -ri \u0026#39;initrd\u0026#39; . 이렇게 검색해 보았을 때 quiet ---이라고 적힌 것들이 있는데, --- 뒤에가 boot parameter로 쓰이는 것들입니다.\nvim으로 /isolinux/txt.cfg 파일을 열고 원하는 설정을 기입하면 됩니다.\n이렇게 원하는 boot parameter를 적었다면, 다시 md5sum을 통해 체크섬을 만들어주어야 합니다. 이에 대한 내용은 앞선 [포스트]({% post_url 2020-01-05-unattended-ubuntu %})에서도 확인할 수 있으니 bakeIsoImage.sh 스크립트를 참조하여 md5sum을 하고 iso 파일을 만들면 됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/how-to-use-sudo-without-password/",
	"title": "sudo를 password 없이 사용하기",
	"tags": ["sudo", "passwordless", "ubuntu"],
	"description": "",
	"content": "/etc/sudoers는 sudo를 사용할 수 있는 파일입니다. 이 파일을 열어보면 다음과 같은 글이 적혀 있습니다.\n Please consider adding local content in /etc/sudoers.d/ instead of directly modifying this file\n 즉, 직접 이 파일을 수정해서 sudo 권한을 주지 말고, /etc/sudoers.d/ 폴더 내에 파일을 추가하라는 의미입니다.\n이 곳에는 /etc/sudoers와 마찬가지로 계정에 대한 설정을 추가할 수 있습니다. 그리고 /etc/sudoers에서는 \u0026ldquo;NOPASSWD\u0026quot;라는 옵션을 주어 password없이 타 계정의 권한을 가지게 만들 수 있습니다.\n이 두가지를 종합하여 내 linux 계정이 sudo 명령어를 입력할 때, 즉 root 권한을 가지게 될 때 password를 입력하지 않도록 설정할 수 있습니다.\nexport ACCOUNT=$(whoami) echo \u0026#34;$ACCOUNTALL = (root) NOPASSWD:ALL\u0026#34; | sudo tee /etc/sudoers.d/$ACCOUNT sudo chmod 0440 /etc/sudoers.d/$ACCOUNT 이제 sudo 명령어를 쳐도 더 이상 password를 입력하라는 출력이 뜨지 않습니다.\n"
},
{
	"uri": "http://kimmj.github.io/english/",
	"title": "English",
	"tags": [],
	"description": "",
	"content": "English  HIMYM    Season1    Episode01     Episode02       "
},
{
	"uri": "http://kimmj.github.io/tags/cka/",
	"title": "cka",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/cks/",
	"title": "cks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/jenkins/",
	"title": "jenkins",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/workspace/",
	"title": "workspace",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/service/",
	"title": "service",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/targetport/",
	"title": "targetport",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/docker/",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/docker-registry/",
	"title": "docker-registry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/insecure-registry/",
	"title": "insecure-registry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ansible/",
	"title": "ansible",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ansible-for-devops/",
	"title": "ansible-for-devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/login-message/",
	"title": "login-message",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/motd/",
	"title": "motd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu/",
	"title": "ubuntu",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/bgimg-darken/",
	"title": "bgimg-darken",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/css/",
	"title": "css",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/google-analytics/",
	"title": "google-analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/git/",
	"title": "git",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/git-secret/",
	"title": "git-secret",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/github/",
	"title": "github",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/harbor/",
	"title": "harbor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/reboot/",
	"title": "reboot",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/tmux/",
	"title": "tmux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/sudo/",
	"title": "sudo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/oh-my-zsh/",
	"title": "oh-my-zsh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/zsh/",
	"title": "zsh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/playbook/",
	"title": "playbook",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/base64/",
	"title": "base64",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/editor/",
	"title": "editor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/file/",
	"title": "file",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/log/",
	"title": "log",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/stern/",
	"title": "stern",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/gitignore/",
	"title": "gitignore",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/linux/",
	"title": "linux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/port/",
	"title": "port",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pipe/",
	"title": "pipe",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/watch/",
	"title": "watch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/alias/",
	"title": "alias",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/bridge/",
	"title": "bridge",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/container/",
	"title": "container",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/docker-compose/",
	"title": "docker-compose",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/passwordless/",
	"title": "passwordless",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ssh/",
	"title": "ssh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/tunneling/",
	"title": "tunneling",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ansible-playbooks/",
	"title": "ansible-playbooks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/install/",
	"title": "install",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/infrastructure-as-code/",
	"title": "infrastructure-as-code",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/blue-green/",
	"title": "blue-green",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/canary/",
	"title": "canary",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/cicd/",
	"title": "cicd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/deploy/",
	"title": "deploy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/roll-out/",
	"title": "roll-out",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pod/",
	"title": "pod",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/concepts/",
	"title": "concepts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/prometheus/",
	"title": "prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/utterance/",
	"title": "utterance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/holiday/",
	"title": "holiday",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/federation/",
	"title": "federation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/canary-update/",
	"title": "canary-update",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/spinnaker/",
	"title": "spinnaker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/air-gaped/",
	"title": "air-gaped",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/minio/",
	"title": "minio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/instll/",
	"title": "instll",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/kubeadm/",
	"title": "kubeadm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/overview/",
	"title": "overview",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/hostname/",
	"title": "hostname",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/font/",
	"title": "font",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/halyard/",
	"title": "halyard",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/proxy/",
	"title": "proxy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/netplan/",
	"title": "netplan",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/static-ip/",
	"title": "static-ip",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu-18.04/",
	"title": "ubuntu-18.04",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/html/",
	"title": "html",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pipeline/",
	"title": "pipeline",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/blog/",
	"title": "blog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/record/",
	"title": "record",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/libvirt/",
	"title": "libvirt",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/preseed.cfg/",
	"title": "preseed.cfg",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu-18.04-server/",
	"title": "ubuntu-18.04-server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/boot-parameter/",
	"title": "boot parameter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]