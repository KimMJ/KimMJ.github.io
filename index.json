[
{
	"uri": "http://kimmj.github.io/jenkins/install/",
	"title": "Jenkins Install",
	"tags": ["jenkins", "install"],
	"description": "",
	"content": "Configure docker-compose.yaml 다음과 같이 docker-compose.yaml 파일을 적절한 디렉토리에 생성합니다.\nversion: \u0026#39;2\u0026#39; services: jenkins: image: \u0026#39;jenkins/jenkins:lts\u0026#39; ports: - \u0026#39;38080:8080\u0026#39; - \u0026#39;38443:8443\u0026#39; - \u0026#39;50000:50000\u0026#39; volumes: - \u0026#39;jenkins_data:/var/jenkins_home\u0026#39; volumes: jenkins_data: driver: local driver_opts: type: none device: $PWD/jenkins_data o: bind jenkins_data는 jenkins가 사용할 데이터들입니다. 이를 local에 폴더로 만들어줍니다.\nmkdir jenkins_data Start Jenkins 이제 docker-compose 명령어를 통해 실행합니다.\nsudo docker-compose up -d http://$IP:38080 으로 접속할 수 있습니다. 로컬에 설치하셨다면 http://localhost:38080으로 접속하면 됩니다.\nJenkins 세팅 처음에 다음과 같은 화면을 볼 수 있습니다.\n여기서 /var/jenkins_home/secrets/initialAdminPassword의 내용을 아래 칸에 입려하라고 지시합니다.\n저희는 앞서 $PWD/jenkins_data를 /var/jenkins_home에 binding했으므로, $PWD/jenkins_data/secrets/initialAdminPassword를 확인하면 됩니다. 또는 docker container에 직접 접속해서 해당 path로 이동 후 값을 확인해도 됩니다.\n이제 다음과 같은 화면이 뜰 것입니다.\n여기서는 왼쪽의 Install suggested plugins를 선택해줍니다.\n다음과 같이 설치가 진행됩니다.\n위의 plugin 설치과정이 완료되고 나면 다음과 같은 admin 계정 생성에 관한 화면이 나옵니다.\n알맞게 입력을 한 뒤 저장하면 접속 URL을 작성하라고 합니다. default로 적어져 있으니 확인후 변경이 필요하면 바꾸면 됩니다.\n여기까지 하면 기본 설정은 다 끝났습니다. Jenkins가 준비될 때까지 잠시 기다리면 home 화면이 뜹니다.\nReference  https://hub.docker.com/_/jenkins/  "
},
{
	"uri": "http://kimmj.github.io/prometheus/install/",
	"title": "Install Prometheus",
	"tags": ["install", "prometheus"],
	"description": "",
	"content": "Prometheus Prometheus는 opensource monitoring system입니다. 음악을 하는 사람들이 많이 이용하는 사이트인 SoundCloud에서 개발된 오픈소스입니다.\nPromQL이라는 Query문을 사용하여 metric을 수집할 수 있습니다. 자세한 내용은 나중에 따로 포스트를 작성하도록 하겠습니다.\n이 문서에서는 docker-compose를 통해 간단하게 prometheus를 설치해볼 것입니다. 또한 prometheus와 뗄레야 뗄 수 없는 단짝 Grafana도 함께 설치할 것입니다.\nPrometheus의 설치 제가 docker-compose를 선호하는 이유는 너무나도 간단하게, dependency가 있는 어플리케이션을 설치할 수 있기 때문입니다. 아래에 예시에서도 잘 드러나있습니다.\n저는 monitoring/ 폴더 아래에 docker-compose.yaml이라는 이름으로 파일을 생성했습니다.\nversion: \u0026#39;3\u0026#39; networks: back-tier: services: prometheus: image: prom/prometheus restart: unless-stopped volumes: - ./config/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; networks: - back-tier grafana: image: grafana/grafana:6.5.3 ports: - 9080:3000 user: \u0026#34;0\u0026#34; # $(id -u) https://community.grafana.com/t/new-docker-install-with-persistent-storage-permission-problem/10896/5 depends_on: - prometheus volumes: - ./grafana/provisioning/:/etc/grafana/provisioning/ - ./grafana_data:/var/lib/grafana networks: - back-tier 위의 예시에서 services.grafana.user=\u0026quot;0\u0026quot;으로 설정이 되어있는 것이 보이실 것입니다. 이 부분을 $(id -u)의 결과값으로 변경하시면 됩니다. 관련된 내용은 옆에 주석에서도 확인이 가능합니다.\n이렇게 하고난 뒤, 설치하는 방법은 너무나도 간단합니다.\ncd monitoring docker-compose up -d 잠시 내리고 싶을땐 다음과 같이 하면 됩니다.\ndocker-compose down 설정상, Prometheus는 9090 포트를 통해서 expose 되어있습니다. 또한 Grafana는 9080 포트를 사용하고 있습니다. Grafana의 기본 ID/PW는 admin/admin입니다.\n설정 Prometheus를 사용하기 위해서는 prometheus.yml이라는 파일을 관리해주어야 합니다. 이곳에서 어느 서비스의 metric을 가지고 올지 설정할 수 있습니다.\n해당 파일은 docker-compose.yaml을 참조해 보았을 때, monitoring/config/prometheus.yml을 수정하면 된다는 것을 알 수 있습니다. 수정 후에는 docker-compose down을 통해 잠시 내렸다가 docker-compose up -d를 통해 올리면 수정사항이 반영됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "Kubernetes Install Install Kubernetes on bare-metal\n Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm     "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/overview/",
	"title": "Overview",
	"tags": ["kubernetes", "install", "overview"],
	"description": "",
	"content": "저의 vm으로 구성한 클러스터를 설명드리고자 합니다.\n Cloud Provider: Kubernetes on-prem (4 VMs)  1 for master (4GB Mem, 2 CPU) 3 for worker (each 8GM Mem, 4 CPU)   Kubernetes 1.17.0 Storage Class: Ceph OS: Ubuntu 18.04.2 Server Internal Network: VirtualBox Host-Only Ethernet Adapter (192.168.x.0/24) External Network: Bridge to adapter (192.168.y.0/24)  한번에 쳐야하는 명령어가 많기 때문에, tmux를 사용해서 여러개의 pane을 생성하고 각각에 대해 ssh로 접속하였습니다.\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Spinnaker Installation spinnaker를 설치해 볼 것입니다.\n쉽지 않았던 여정들을 기록하려고 합니다.\n Overview     Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/overview/",
	"title": "Overview",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Overview of install Spinnaker 어떻게 Spinnaker를 설치 및 배포하는지 알아보도록 하겠습니다.\n가장 먼저 최소 사양을 확인해보도록 하겠습니다.\n링크 : https://www.spinnaker.io/guides/developer/getting-set-up/#system-requirements\n 램 18 GB CPU 4코어 Ubuntu 14.04, 16.04, 18.04  Spinnaker 자체가 클라우드 환경에만 배포가 가능하기 때문에, 아마도 \u0026ldquo;전체 클라우드를 합하여 저정도면 된다\u0026quot;를 의미하는 것 같습니다.\n설치 방법은 두가지로 나뉩니다.\n 테스트를 목적으로 Helm Chart를 통한 설치 실제로 사용할 목적으로 halyard를 통한 설치  저는 여기서 2번 halyard를 통한 설치를 해보려고 합니다.\n전체적인 프로세스를 먼저 설명드리자면 다음과 같습니다.\n halyard 설치 Cloud Provider(클라우드 제공자) 선택 배포 환경 선택 Storage Service 선택 배포 및 접속 config 백업하기  그리고 저는 다음과 같은 환경에서 테스트를 할 예정입니다.\n Cloud Provider: Kubernetes on-prem (4 VMs)  1 for master (4GB Mem, 1 CPU) 3 for worker (each 8GM Mem, 4 CPU)   Environment: Distributed installation on Kubernetes Storage Service: Minio Deploy and Connect: expose by NodePort OS : Ubuntu 18.04.2 Server  "
},
{
	"uri": "http://kimmj.github.io/",
	"title": "Ibiza",
	"tags": [],
	"description": "",
	"content": "Ibiza  CoreDNS     Harbor    Private Docker Registry 오픈소스: Harbor란     Harbor 설치      Git    git-secret을 통한 github 파일 암호화     Gitignore 설정      Python    [번역]Python을 통해 이쁜 CLI 만들기      Docker    http를 사용하는 docker registry를 위한 insecure registry 설정     Docker를 sudo없이 실행하기     [docker-compose] container에서 다른 container로 접속하기      Jenkins    Jenkins Install     Workspace@2를 변경하기 - Workspace List 설정 변경      IaC    [번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials      CICD    Deploy Strategy      CSS    background image 어둡게 하기     Greater Than Sign      Prometheus    Install Prometheus     Federation      Kubernetes    Install    Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm      CKA Study    02 Core Concepts     03 Scheduling     04 Logging and Monitoring     05 Application Lifecycle Management     06 Cluster Maintenance     07 Security     08 Storage     09 Networking     10 Kubernetes the Hard Way     12 End to End Tests on a Kubernetes Cluster     13 Troubleshooting     14 Other Topics      Concepts    Controllers Overview     Services     Pods      CKA: Certified Kubernetes Administrator 취득 후기     [번역] 쿠버네티스에서의 Port, TargetPort, NodePort     Stern을 이용하여 여러 pod의 log를 한번에 확인하기      Hugo    Ibiza    Font Change      Hugo에 Google Analytics 적용하기     Hugo에 Comment 추가하기 (Utterance)     HUGO로 HTML이 되지 않을 때 가능하게 하는 방법      Spinnaker    Installation    Overview     Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment      CanaryAnalysis    Canary Analysis      Tips    Pipeline Expressions       Ansible    Create Vm With Ansible Libvirt      Ubuntu    Tools    Tmux      Network    Netplan으로 static IP 할당받기      Ubuntu의 Login Message 수정하기     reboot 후에 tmux를 실행시켜 원하는 작업을 하기     oh-my-zsh에서 home key와 end key가 안될 때 해결방법     Ubuntu에서 Base64로 인코딩, 디코딩하기     Editor(vi)가 없을 때 파일 수정하기     열려있는 포트 확인하기     pipe를 사용한 명령어를 watch로 확인하기     watch를 사용할 때 alias 이용하기     password 없이 ssh 접속하기     SSH Tunneling 사용법     Gateway를 이용하여 SSH 접속하기     Hostname 변경하기     추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기     Ubuntu 설치 시 Boot Parameter를 수정하기     sudo를 password 없이 사용하기      "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/02-core-concepts/",
	"title": "02 Core Concepts",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": " Controller는 Kubernetes의 brain과 같다.  ReplicaSets  Pod가 죽게 되면 사용자가 접근을 할 수 없게 된다. 따라서 여러개의 파드를 띄워 하나가 죽어도 나머지가 동작하도록 해야한다. Replication Controller는 여러개의 파드를 띄울 수 있도록 도와준다. 이를 High Availability라고 한다. 하나의 파드만 관리한다고 해서 쓸모없는게 아니라 이는 하나가 죽으면 다시 하나를 실행시키는 방식으로 동작한다. 로드가 늘어나면 파드를 늘릴 수 있다. Replica Controller와 Replica Set은 비슷하지만 다르다.  Replica Controller는 Replica Set으로 대체되었다.   Replica Controller  apiVersion=v1 spec.template에 파드의 스펙을 정의한다.  파드를 정의할때의 yaml에서 apiVersion과 kind같은것만 빼고 나머지 정의들을 spec.template에 적으면 사용가능하다.     Replica Set  apiVersion=apps/v1 selector를 작성해야한다. -\u0026gt; Replica Controller과의 차이점. 어떤 파드를 컨트롤하는지 적어야한다. label을 적어서 apply시 생성했던 파드가 아니더라도 관리할 수 있도록 한다.    Deployments  roll back, rolling upgrade 각 파드는 Replica Set으로 관리된다.  이를 Deployment가 감싼다. rolling upgrade, roll back, scale out, pause, resume 등을 사용할 수 있음   Definition  ReplicaSet에서 Deployment로만 변경하면 된다. ReplicaSet과 Deployment는 크게 다르지 않다.   commands  Certification Tips  kubectl run을 이용하여 yaml template을 생성하라. Create an NGINX pod  kubectl run --generator=run-pod/v1 nginx --image=nginx   Generate POD Manifest YAML file (-o yaml). Don't create it(\u0026ndash;dry-run)  kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml   Create a deployment  kubectl create deployment --image=nginx nginx   Generate Deployment YAML file (-o yaml). Don't create it(\u0026ndash;dry-run) with 4 Replicas (\u0026ndash;replicas=4)  kubectl create deployment --image=nginx nginx --dry-run -o yaml \u0026gt; nginx-deployment.yaml Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment 즉, deployment를 create할 때는 먼저 생성해놓고 replicas를 조정해야한다. (--replicas 옵션이 없음)    Namespaces  각 네임스페이스에는 그 안에서 소비할 수 있는 리소스들이 있다. default 네임스페이스는 쿠버네티스가 처음 만들어질 때 생성되는 것. 쿠버네티스는 네트워킹이나 DNS와 같은 내부적인 목적으로 파드와 서비스를 만든다. 이러한 것들을 유저가 사용하는 공간과 다르게 두어 실수로 삭제하지 않도록 만들어준다.  kube-system이다.   kube-public은 모든 유저가 사용할 수 있는 리소스들이 있다. 작은 공간에서 사용할 경우 default만 써도 되지만 큰 기업으로 가면 네임스페이스를 고려해야 한다. 클러스터 내에서 dev와 prod를 다른 네임스페이스를 두고 만들수도 있다. 각 네임스페이스는 누가 무엇을 할 수 있는지에 관한 정책을 가지고 있다. 또한 네임스페이스별로 리소스 쿼터를 설정하여 최소한의 서비스를 할 수 있도록 한다. 네임스페이스에 내에 있는 리소스끼리는 이름만 가지고 접근이 가능하다. 다른 네임스페이스에 있는 것에 접근하려면 네임스페이스를 알려주어야 한다.  servicename.namespace.svc.cluster.local   이러한 형태로 dns가 설정되기 때문. cluster.local: domain svc: subdomain, service yaml이 특정 네임스페이스에만 뜰 수 있도록 하고싶다면 이를 manifest에 옮기면 된다. 기본 네임스페이스를 변경하고 싶으면 kubectl config set-context $(kubectl config current-context) --namespace=dev 와 같은 방식으로 조정 리소스 쿼터는 kind=ResourceQuota로 설정하면 된다.  Service  내 외부의 다양한 컴포넌트와 통신을 가능하게 해준다. 어플리케이션을 loose-coupling하게 해준다. 클러스터 내부의 파드와 통신하는 방법  ssh로 클러스터 노드에 접속 후 curl을 통해 파드의 아이피로 직접 통신 가능 ssh 없이 통신을 하기 위해서는 노드 안에서 포워딩을 도와주는 무언가가 필요  서비스는 노드의 포트를 listen하고 이를 파드의 포트로 포워딩한다. 이 방식은 NodePort라고 알려진 것.   ClusterIP  클러스터 내부에 virtual IP를 만들어 서로 통신하게 만듬.   LoadBalancer  cloud provider 내부에서 서비스에 대한 로드밸런싱을 사용하게 해줌.     [30008]node - service [80] - [80]pod 형태일 때  targetPort로 파드가 떠있는 80포트를 지정.  서비스가 요청을 포워딩할 곳.   port는 서비스 자체의 포트.  서비스 관점에서의 포트 노드 내에서 virtual server처럼 작동. 자신의 IP를 가지고 있고, 이를 cluster ip라고 부름.   nodePort는 노드에서 외부로 expose하는 포트 기본적으로 노드포트 범위는 30000~32767   spec에는 type과 port를 작성. 포트에서 필수 필드는 port 하나.  targetPort는 안적으면 port와 동일 nodePort는 안적으면 범위내에서 랜덤 생성   label과 selector로 파드를 연결한다. 서비스와 멀티 파드가 연결되어 있으면 랜덤 알고리즘으로 로드밸런싱을 한다. 모든 클러스터의 노드로 노드포트를 expose한다.  Service Cluster IP  service들간에 통신을 하는 올바른 방법은 서비스의 아이피를 이용하는 것이다.  파드의 아이피는 변경될 수 있기 때문에.   랜덤으로 로드를 분배한다. 이를 클러스터 아이피라고 한다. targetPort는 파드가 expose하는 포트 port는 서비스가 사용하는 포트  Certification Tips  파일을 직접 만드는 것은 어렵기 때문에 imperative command를 사용해서 파일템플릿을 생성하는 것이 좋다. --dry-run과 -o yaml을 자주 사용하면 좋다. 파드  kubectl run --generator=run-pod/v1 nginx --image=nginx kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml   Deployment  kubectl create deployment --image=nginx nginx kubectl run --generator=deployment/v1beta1 nginx-image=nginx --dry-run --replicas=4 -o yaml   Important  kubectl create deployment는 --replicas 옵션이 없다.  kubectl scale 명령어로 크기를 조정해야한다.   kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml \u0026gt; nginx-deployment.yaml kubectl create deployment --image=nginx nginx --dry-run -o yaml \u0026gt; nginx-deployment.yaml   Service  kubectl expose pod redis --port 6379 --name redis-service --dry-run -o yaml  자동으로 파드와 매칭되는 라벨을 사용하게 됨.   kubectl create service clusterip redis --tcp=6379:6379 --dry-run -o yaml  app=redis라는 셀렉터가 있다고 가정함. 따라서 배포 전 selectordp rhdp   kubectl expose pod nginx --port=80 --name nginx-service --dry-run -o yaml  수동으로 nodeport 설정 넣어줘야 함.   kubectl create service nodePort nginx --tcp=80:80 --node-port=30080 --dry-run all  셀렉터 지정 필요     kubectl expose를 더 추천.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/install-kubeadm/",
	"title": "Install Kubeadm",
	"tags": ["install", "kubeadm"],
	"description": "",
	"content": "kubeadm은 Kubernetes cluster의 설정들을 관리하는 툴입니다.\nPrerequisites 먼저, 몇가지 전제사항이 있습니다.\n  모든 노드의 MAC 주소와 product_uuid가 달라야 합니다. ifconfig -a와 sudo cat /sys/class/dmi/id/product_uuid를 통해 알 수 있습니다.\n  network adapter를 확인합니다. 하나 이상의 network adapter가 있고, Kubernetes component들이 default route로 통신이 불가능하다면 IP route를 설정하여 Kubernetes cluster 주소가 적절한 adapter를 통해 이동할 수 있도록 해주는 것이 좋습니다.\n  iptables를 사용하는지 확인 Ubuntu 19.04 이후버전부터는 nftables라는 것을 사용한다고 합니다. 그러나 이는 kube-proxy와 호환이 잘 안되기 때문에 iptables를 사용해야한다고 하고 있습니다.\n다음과 같이 설정할 수 있습니다.\nsudo update-alternatives --set iptables /usr/sbin/iptables-legacy sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy sudo update-alternatives --set arptables /usr/sbin/arptables-legacy sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy   필수 포트 확인\nControl-plane node(s)\n   Protocol Direction Port Range Purpose Used By     TCP Inbound 6443* Kubernetes API server All   TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd   TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 10251 kube-scheduler Self   TCP Inbound 10252 kube-controller-manager Self    Worker node(s)\n   Protocol Direction Port Range Purpose Used By     TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 30000-32767 NodePort Services** All    여기서 * 표시가 있는 것은 수정이 가능한 사항이라고 합니다. (API server의 port, NodePort Service로 열 수 있는 port의 범위)\n  container runtime 설치 여기에서는 Docker를 사용할 것입니다. Docker 홈페이지에는 script를 통해 최신 버전의 Docker를 다운로드 받는 방법을 제공합니다.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh   kubeadm, kubelet, kubectl 설치 Kubernetes에 필요한 세가지 툴을 설치하도록 하겠습니다. 간단하게 설명드리자면 kubeadm은 Kubernetes를 관리하는 툴이라고 생각하면 되고 kubelet은 명령을 이행하는 툴이라고 생각하면 됩니다. kubectl은 Kubernetes cluster와 통신하는 툴입니다.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 마지막에 apt-mark를 통해 kubelet, kubeadm, kubectl의 버전을 고정시켰습니다.\n  이렇게 kubelet, kubeadm, kubectl을 설치하였으면 마지막으로 swap 영역을 제거할 것입니다. Kubernetes는 swap 영역이 없는 것이 필수 항목입니다. 따라서 다음의 명령어로 swap 영역을 삭제합니다.\nsudo swapoff -a 이는 현재 세션에서만 동작하고 재부팅시 해제됩니다. 따라서 /etc/fstab을 열고 swap 부분을 주석처리합니다.\n#/swapfile none swap sw 0 0 Reference https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/\nhttps://kubernetes.io/docs/concepts/cluster-administration/networking/\nhttps://docs.projectcalico.org/v3.11/getting-started/kubernetes/\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/install-halyard/",
	"title": "Install Halyard",
	"tags": ["spinnaker", "install", "halyard", "proxy"],
	"description": "",
	"content": "halyard란? halyard는 Spinnaker를 배포할 때 사용하는 CLI 툴입니다.\nhalyard는 Spinnaker 관련 설정들의 validation, 배포한 환경 백업, 설정 추가 및 변경에 사용됩니다.\n설치 방법 선택하기 총 2가지 방법으로 halyard를 설치할 수 있습니다.\n Debian/Ubuntu나 macOS에 직접 설치하기 Docker 사용하기  Spinnaker Docs에서는 실제 Production 환경이라면 직접 설치하는 방법을, 그게 아니라 간단하게 사용하려면 docker를 사용해도 된다고 하고 있습니다.\n그리고 한가지의 옵션이 더 있습니다.\n 인터넷이 되지 않는 환경 (프록시나 방화벽 등으로 halyard를 통한 설치가 어려운 경우)  이 글을 작성하고 있는 환경은 인터넷이 잘 되는 환경입니다. 그리고 두가지 모두 시도해 보도록 하겠습니다.\nDebian/Ubuntu나 macOS에 직접 설치하기 공식 Docs에서 halyard는 다음과 같은 환경에서 동작한다고 말하고 있습니다.\n Ubuntu 14.04, 16.04 or 18.04 (Ubuntu 16.04 requires Spinnaker 1.6.0 or later) Debian 8 or 9 macOS (tested on 10.13 High Sierra only)  이제 직접 설치를 시작해보도록 하겠습니다.\n시작하기 전에, halyard를 설치하기 위해서는 root 계정이 아닌 계정이 필요합니다. 만일 root만 있다면 spinnaker를 위한 계정을 생성해 줍니다.\nadduser spinnaker 위처럼 생성한 계정에 sudoers 권한을 줍니다.\nadduser spinnaker sudo 최신 버전의 halyard 다운로드 Debian/Ubuntu:\ncurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/debian/InstallHalyard.sh macOS:\ncurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/macos/InstallHalyard.sh 설치 sudo bash InstallHalyard.sh 확인 hal -v 추가사항 . ~/.bashrc를 실행하여 bash completion 활성화\n여기서 proxy 환경이라면 halyard의 jvm에 proxy 옵션을 추가해주어야 합니다.\nvi /opt/halyard/bin/halyard을 통해 halyard의 jvm 옵션을 추가할 수 있습니다.\nDEFAULT_JVM_OPTS=\u0026#39;\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34; \u0026#34;-Dspring.config.additional-location=/opt/spinnaker/config/\u0026#34; \u0026#34;-Dhttps.proxyHost=\u0026lt;proxyHost\u0026gt; -Dhttps.proxyPort=\u0026lt;proxyPort\u0026gt;\u0026#34; \u0026#34;-Dhttp.proxyHost=\u0026lt;proxyHost\u0026gt; -Dhttp.proxyPort=\u0026lt;proxyPort\u0026gt;\u0026#34;\u0026#39; 위의 설정에서 다음과 같이 proxy를 추가해줍니다. 그 다음 halyard를 재시동합니다.\nhal shutdown hal config 아랫줄의 hal config는 의도적으로 halyard를 구동시키기 위함입니다.\ndocker로 halyard 사용하기 다음의 명령어는 공식 docs에서 제공하는 명령어입니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm \\  -v ~/.hal:/home/spinnaker/.hal \\  -d \\  gcr.io/spinnaker-marketplace/halyard:stable kubernetes로 배포하려 할 경우, kubectl 명령어에서 사용할 kubeconfig 파일이 필요합니다. 이 또한 -v 옵션으로 주어야 합니다. 그리고 그 kubeconfig 파일을 읽도록 설정해야 합니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm \\  -v ~/.hal:/home/spinnaker/.hal \\  -v ~/.kube:/home/spinnaker/.kube \\  -e KUBECONFIG=/home/spinnaker/.kube/config \\  -d \\  gcr.io/spinnaker-marketplace/halyard:stable 사실 5번째 줄의 -e KUBECONFIG=/home/spinnaker/.kube/config은 없어도 default로 들어가있는 설정입니다. 하지만 혹시나 위에서 /home/spinnaker/.kube가 아닌 다른곳을 저장공간으로 둔다면 아래의 설정도 바뀌어야 합니다.\n프록시 환경이라면 다음과 같이 JAVA_OPT를 추가해주어야 합니다.\ndocker run -p 8084:8084 -p 9000:9000 \\  --name halyard --rm -d \\  -v ~/.hal:/home/spinnaker/.hal \\  -v ~/.kube:/home/spinnaker/.kube \\  -e http_proxy=http://\u0026lt;proxy_host\u0026gt;:\u0026lt;proxy_port\u0026gt; \\  -e https_proxy=https://\u0026lt;proxy_host\u0026gt;:\u0026lt;proxy_port\u0026gt; \\  -e JAVA_OPTS=\u0026#34;-Dhttps.proxyHost=\u0026lt;proxy_host\u0026gt; -Dhttps.proxyPort=\u0026lt;proxy_port\u0026gt;\u0026#34; \\  -e KUBECONFIG=/home/spinnaker/.kube/config \\  gcr.io/spinnaker-marketplace/halyard:stable 그래도 안된다면.. 아마도 관리가 엄격한 네트워크를 사용하고 계실 것이라고 예상됩니다. 저 또한 그랬으니까요.\nhalyard는 설정 및 버전등의 정보를 bucket (Google Cloud Storage)로 관리한다고 합니다. 따라서 이곳으로 연결이 되지 않는다면 validation, version list 등의 상황에서 timeout이 날 것입니다.\nSpinnaker에서는 이를 확인하기 위해 gsutil을 사용하여 bucket의 주소인 gs://halconfig에 연결할 수 있는지 확인해보라고 합니다.\n또는 curl을 이용해서도 확인이 가능합니다.\n먼저 gsutil은 google storage 서비스에 접속하는 CLI 툴입니다. Docs에서 설치방법을 확인하여 설치할 수 있습니다.\n설치가 완료되었다면 gsutil로 접속이 가능한지부터 확인합니다.\ngsutil ls gs://halconfig 두번째로 curl을 사용하는 방법입니다.\ncurl storage.googleapis.com/halconfig 결과물들이 나온다면 정상적으로 bucket에는 접속이 가능한 것입니다. hal config 명령어가 성공하지 않지만 bucket에 접속이 가능한 케이스는 아직 보지 못했습니다.\n우선 여기까지 해서 bucket에 접속이 불가능하다고 판단이 되면, 인터넷이 없는 환경에서 설치하는 방법을 고려해보아야 합니다. 또는 googleapis.com url로 proxy에서 사이트가 차단되었는지 확인하고, SSL도 해제할 경우 해결될 수도 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/03-scheduling/",
	"title": "03 Scheduling",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Manual Scheduling  how scheduling works  podSpec에 nodeName 필드를 채워넣으면 해당 노드로 파드가 뜬다. default로는 비워져있음. 스케줄링은 알고리즘에 의해 파드를 띄울 노드를 선택하고 나면 nodeName 필드를 채운다. 스케줄러가 없으면 파드가 계속해서 pending 상태에 있게 된다. 따라서 스케줄러가 없으면 단순히 nodeName을 채우면 될 것이다. runtime에 nodeName을 변경할 수 있는데, 이는 kind=Binding object를 binding API로 POST 요청을 보내는 방식으로 가능하다. binding definition에 target.name=\u0026lt;NodeName\u0026gt;으로 설정하면 된다. 이를 JSON 형식으로 보내면 된다.  따라서 yaml을 json으로 변경해야한다.      Labels and Selectors  각 리소스에 대해 label로 속성을 부여하고 selector로 선택할 수 있다. metadata.labels에 key-value 형태로 제공 ReplicaSet을 예로 들 때, spec.template.metadata.labels에 있는 정보는 파드에 대한 label이다.  metadata.labels에 있는 레이블은 ReplicaSet에 대한 label이다. spec.selector.matchLabels은 생성할 파드와 연결해주는 것이다.   label와 selector는 object를 그룹화하고 선택할 때 사용된다.  annotations는 inflammatory purpose를 위해 기록되는 것이다. integration purpose로도 사용됨.    Taints and Tolerations   사람이 벌레를 퇴치하는 것에 비유\n 벌레가 싫어하는 스프레이를 사람에게 뿌리면 사람은 taint 처리가 됨. 벌레는 이를 견디지 못해서 사람에게 갈 수 없다. (intolerant) 만약 이 스프레이에 내성이 있는 벌레가 있다면 사람에게 다가갈 수 있을 것이다. (tolerant)    즉, 사람에게 taint 처리를 하고 벌레에게 해당 taint에 대해 tolerant 속성을 주면 벌레가 사람에게 다가갈 수 있다.\n  Taints: kubectl taint nodes node-name key=value:taint-effect\n taint-effect에는 NoSchedule,PreferNoSchedule,NoExecute가 있다. kubectl taint nodes node1 app=blue:NoSchedule    podSpec에서 다음과 같이 설정\nspec: tolerations: - key: \u0026#34;app\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;blue\u0026#34; effect: \u0026#34;NoSchedule\u0026#34;   taint, toleration 설정을 했다고 해서 파드가 항상 taint쪽으로 가는 것은 아니다.\n  toleration이 없는 파드들이 taint 처리된 노드로 뜨지 못할 뿐이다.\n  파드를 특정 노드로 할당하고 싶다면 이는 node affinity를 사용해야 한다.\n  kubectl describe node node-name | grep Taint\n  Node Selectors  특정한 노드로 파드가 뜨게 하는 방법에는 여러개가 있다.  nodeSelector 사용하기  nodeSelector는 key=value를 사용하고, 이는 노드에 할당된 label이다. 제약사항: equal 상황에서만 사용할 수 있음      Node Affinity  spec.affinity.nodeAffinity에 지정  requiredDuringSchedulingIgnoreDuringExecution  nodeSelectorTerms  matchExpressions  In을 사용하면 key값에 대해 복수개의 value중 만족하는 것이 있는 경우에 적용할 수 있다. NotIn을 사용하여 key=value를 만족하지 않는 것에 대해 적용할 수 있다. Exists를 사용하여 key값이 있는지 확인할 수 있다. value가 필요 없음.         requiredDuringSchedulingIgnoreDuringExecution과 preferredDuringSchedulingIgnoreDuringExecution이 있음. required는 스케쥴링 시 없으면 기다림. preferred는 없으면 딴데 띄움. DuringExecution의 경우 Ignore만 있으며 노드의 label이 삭제되더라도 evict 하지 않음.  Taints and Tolerations vs Node Affinity  Taints \u0026amp; Tolerations만 설정할 경우 뜨기 희망하지 않는 다른 노드에 뜰 수도 있다. Node Affinity만 설정하면 다른 파드가 나의 노드에 뜰 수도 있다. 따라서 둘 다 사용해야 우리가 원하는 어떤 파드만 어떤 노드에 뜰 수 있도록 하는 상황을 이용할 수 있을 것이다.  Resource Requirements and Limits  노드에 자원이 충요하지 않으면 pending이 된다. Resource Request: Minimum Requirement 1 CPU라는 의미는 1vCPU와 같다.  AWS에서는 1vCPU, GCP와 Azure에서는 1Core, 또는 1 Hyperthread. Mi 처럼 i가 들어가는 것들은 2^10 단위, M만 들어가는 것은 10^3 단위   도커는 원래 노드의 cpu를 모두 사용할 수 있을 정도로 cpu 사용량에 제한이 없다. default로 쿠버네티스는 컨테이너에 대해 1vCPU 제약을 둔다. 메모리 또한 같다. 512Mi가 default vCPU의 경우 쿠버네티스가 CPU 사용량에 제한을 둘 수 있다. 하지만 Memory의 경우 그럴 수 없기 떄문에 초과하면 죽인다. 기본으로 Request와 Limit을 설정하는 것들은 LimitRange를 설정하면 된다.  Edit POD and Deployments  이미 있는 파드에 대해 다음의 필드 이외에 수정이 불가능.  spec.containers[*].image spec.initContainers[*].image spec.activeDeadlineSeconds spec.tolerations   Deployment 안에 있는 파드스펙은 수정이 모두 수정이 가능하며 자동으로 파드를 삭제하고 새로 띄운다.  DaemonSets  DaemonSets은 ReplicaSet과 유사. 각 노드당 하나의 파드를 띄울 수 있도록 한다. 클러스터의 모든 노드에 최소 하나의 파드가 떠있는 것을 보장한다. 대표적으로 kube-proxy가 있다. ReplicaSet과 작성방법이 유사. 작동 원리  파드를 생성하면서 각 노드에 해당하는 nodeName을 설정한다. 1.12까지 이런식으로 사용되었다. 1.12부터 default schedular와 node affinity를 사용하도록 변경되었다.    Static Pods  kube-apiserver, kube-scheduler, controller, ETCD cluster가 없다면? master 조차 없다면? 하나의 worker node만 있을 때 kubelet이 할 수 있는 것이 있을까? kubelet은 노드를 독립적으로 관리할 수 있다. kubelet이 아는것은 파드를 생성하는 것 뿐이다. kube-apiserver 없이 kubelet에게 어떻게 podSpec을 전달할 수 있을까? 노드의 /etc/kubernetes/manifests에 파드 yaml을 넣으면 된다.  kubelet은 이 디렉토리를 주기적으로 감지하여 파드를 생성한다. 또한 파드가 살아있는지 확인한다. 파드가 죽으면 재시동한다. 파일에 변경사항이 있으면 쿠버네티스는 파드를 다시 생성한다. 디렉토리에서 파일을 삭제하면 파드도 삭제된다. 즉, API Server에 의해 관리되지 않는 파드이다. 이를 Static POD라고 한다. 이 방법으로 파드만 생성해야한다. replicaset이나 deplpoyment, service는 생성할 수 없음. kubelet은 파드만 관리할 수 있어서 파드만 생성가능하다. --pod-manifest-path로 kubelet을 시작할 때 인자로 넣어주어야 한다. --config 옵션으로 yaml 파일을 넣을 수 있다.  그 안에서 staticPodPath를 통해 디렉토리를 넣을 수 있다.     다른 워커/마스터가 있더라도 static pod 사용 가능.  kubectl로도 볼 수 있음. kubelet은 클러스터일 때 static pod에 대한 것을 kube-apiserver에도 미러링한다. 단, kube-apiserver는 read only이다. 수정 또는 삭제는 불가능하며 노드의 manifest folder를 수정해야만 한다. 파드의 이름은 뒤에 자동으로 노드이름이 추가된다.   static pod는 kuberentes control plane에 의존하지 않음. 따라서 control plane 그 자체를 배포할 때 사용한다. control plane들을 pod manifest path에 넣으면 kubelet이 알아서 생성해준다. 이것이 kubeadm 툴이 클러스터를 생성하는 방법이다. DaemonSet은 클러스터의 모든 노드에 파드를 띄우는 방법  control plane의 개입이 있음.   Static pod는 kubelet이 자체생성하는 것  Multiple Schedulers  쿠버네티스는 여러개의 스케줄러를 동시에 가지고 있을 수 있다. 파드나 Deployment를 생성할 때 쿠버네티스에게 특정 스케줄러를 사용하도록 지정할 수 있다. kube-scheduler binary를 다운로드하고 이를 옵션을 통해 서비스로서 동작하도록 실행한다. kube-scheduler는 스케줄러 이름을 결정하는 것이고 default-scheduler가 default이다. kubeadm은 kube-scheduler를 파드 형태로 배포한다.  manifest 폴더에서 확인 가능. 여기서 파일을 복사하고 --scheduler-name을 내 스케줄러의 이름으로 변경하면 됨.   multimaster 상황에서 HA 구성이 되어있을 경우 --lock-object-name을 통해 leader-election을 조정한다? podSpec에서 shedulerName에 scheduler의 이름을 명시하면 해당 스케줄러를 사용한다. kubectl get events를 통해 어떤 scheduler가 사용되었는지 알 수 있다. scheduler의 로그는 스케줄러 파드의 로그 확인한다.  Tips   cd /etc/systemd/system/kubelet.service.d/ cat 10-kubeadm.conf 여기에서 KUBELET_CONFIG_ARGS를 확인할 수 있다.\n  "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-cloud-providers/",
	"title": "Choose Cloud Providers",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Spinnaker를 배포할 환경을 설정해 주어야 합니다. 여기에서는 제가 구축한 local kubernetes cluster를 사용할 것입니다.\n먼저 2가지가 필요합니다.\n kubeconfig 파일 kubeconfig 파일은 일반적으로 ~$HOME/.kube/config 파일을 의미합니다. 저는 local kubernetes cluster로 이동하여 해당 파일을 halyard를 위한 vm으로 복사하였습니다. kubectl CLI 툴  이제 hal config 명령어를 통해 kubernetes cluster를 추가합니다.\nhal config provider kubernetes enable CONTEXT=$(kubectl config current-context) hal config provider kubernetes account add wonderland \\  --provider-version v2 \\  --context $CONTEXT hal config features edit --artifacts true "
},
{
	"uri": "http://kimmj.github.io/kubernetes/installation/create-a-single-control-plane-cluster-with-kubeadm/",
	"title": "Create a Single Control Plane Cluster With Kubeadm",
	"tags": ["kubeadm", "instll"],
	"description": "",
	"content": "이 문서에서는 Master 노드 한대로 클러스터를 구성하는 방법에 대해 알아보도록 하겠습니다.\n먼저, 파드 네트워크에 사용할 add-on을 선정합니다. 그런뒤 kubeadm init을 할 때 필요로 하는 사항이 있는지 확인해야 합니다.\n저는 Calico를 사용할 것입니다. Calico는 kubeadm init에서 --pod-network-cidr=192.168.0.0/16을 해주거나, 나중에 calico.yml 파일에서 적절하게 수정해주어야 한다고 합니다. 저는 Pod Network에 사용될 IP 대역을 10.1.0.0/16 대역을 사용하고자 합니다. 그러기 위해 kubeadm init을 --pod-network-cidr=10.1.0.0/16 옵션을 통해 실행할 것입니다.\nkubeadm init --pod-network-cidr=10.1.0.0/16 몇분 후 설치가 완료될 것입니다. 그러면 아래에 kubeadm join 이라면서 어떻게 다른 node들을 join 시키는지 설명이 되어 있습니다.\n클러스터를 사용하려면 다음과 같은 명령어를 통해서 kubectl에서 접근이 가능하도록 해야합니다.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 그 다음엔 각 worker node로 접속하여 설치시 나왔던 kubeadm join 명령어를 복사하여 입력합니다. 그런 뒤 master node로 접속하여 kubectl get nodes를 입력하고 결과를 확인합니다. 성공했을 경우 node들이 보일 것입니다.\n이제 calico를 설치하도록 합니다.\ncurl -o calico.yaml https://docs.projectcalico.org/v3.8/manifests/calico.yaml 이후 calico.yaml에서 CALICO_IPV4POOL_CIDR 부분을 수정해줍니다.\n- name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.1.0.0/16\u0026#34; Reference https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/04-logging-and-monitoring/",
	"title": "04 Logging and Monitoring",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Monitor Cluster Componets  쿠버네티스에서 자체 제공하는 것은 없으나 다음과 같은 것들이 있다.  Metrics Server Prometheus Elastic Stack DataDog Dynatrace   metric server는 각 쿠버네티스의 노드와 파드의 메트릭을 모아서 메모리에 저장한다. metric server는 유일한 in-memory monitoring solution이다.  데이터를 저장하지 않아서 이전 자료를 보지 못한다.   kubelet은 cAdvisor를 포함한다.  파드로부터 퍼포먼스 메트릭을 수집하여 메트릭 서버로 전송한다.   metric server가 설치되면 kubectl top node, kubectl top pods를 사용하여 메트릭을 볼 수 있다.  Managing Application Logs  도커에서 stdout으로 로깅을 하는 상황  docker logs 명령으로 로그를 볼 수 있음.   파드에 여러개의 컨테이너가 있으면 하나를 지정해야 로그를 볼 수 있다.  "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-your-environment/",
	"title": "Choose Your Environment",
	"tags": ["install", "spinnaker"],
	"description": "",
	"content": "Spinnaker를 배포하는 방법에는 3가지가 있습니다. Kubernetes 환경에 배포하기, local debian으로 배포하기, local git으로 배포하기가 있습니다.\n여기에서는 Kubernetes 환경에 배포하기를 진행할 것입니다.\nACCOUNT=wonderland hal config deploy edit --type distributed --account-name $ACCOUNT 위와같이 설정하면 됩니다. ACCOUNT는 kubernetes cluster를 추가할 때 사용했던 이름을 사용하면 됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/coredns/",
	"title": "CoreDNS",
	"tags": [],
	"description": "",
	"content": "CoreDNS  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/05-application-lifecycle-management/",
	"title": "05 Application Lifecycle Management",
	"tags": ["kubernetes", "cks"],
	"description": "",
	"content": "Rolling Updates and Rollbacks  kubectl rollout status deployment/myapp-deployment로 롤아웃 상태 확인 가능 kubectl rollout history deployment/myapp-deployment로 히스토리 확인 가능 Recreate: old를 모두 죽인 뒤 new를 생성 Rolling Update: old를 하나씩 죽이고 하나씩 new를 생성 kubectl set image deployment myapp-deployment nginx=nginx:1.9.1로 이미지 수정 가능 StrategyType이 Recreate면 old의 replica가 모두 0으로 줄고난 뒤 new의 replica를 늘린다. 반면 RollingUpdate일 경우 old를 하나씩 죽이고 new를 하나씩 늘린다. Rollback을 할때는 반대로 동작한다. kubectl run을 통해 파드를 생성하면 deployment가 생성된다.  Commands  CKA에는 굳이 필요 없을 수 있다.. container는 OS를 구동하려고 만들어진 것이 아니라 특정한 태스크를 실행하기 위해 만들어졌다.  앱이 crash가 나면 컨테이너는 종료된다.   Dockerfile을 보면 CMD로 어떤 프로세스를 시작할지 결정한다. 도커는 기본적으로 컨테이너를 시작할 때 터미널 연결을 하지 않는다. 따라서 CMD가 bash일 경우 종료된다. CMD에 입력할 때에는 executable command와 parameter를 하나의 리스트안에 담으면 안된다.  CMD[\u0026quot;sleep\u0026quot;, \u0026quot;5\u0026quot;] vs CMD[\u0026quot;sleep 5\u0026quot;] CMD는 overwrite되는 값이다.   ENTRYPOINT는 CMD와 미슷하지만, 도커 실행시 뒤에 적는 추가적인 글자들은 entrypoint의 parameter가 되어 들어간다.  appending을 하지 않으면? sleep만 전달된다. 따라서 default를 넣고 싶다면 ENTRYPOINT와 CMD를 함께 쓴다. 아예 덮어쓰고 싶다면 docker run --entrypoint ... 형식으로 entrypoint를 지정한다.    Commands and Arguments  파드에 arguments를 넣고 싶으면 pod.spec.containers[].args에 넣으면 된다. Dockerfile에서 CMD는 default parameter이다. args는 Dockerfile에서 CMD를 변경한다. ENTRYPOINT를 수정하고 싶다면 command를 수정하면 된다.  Configure Environment Variables in Applications  환경변수를 넣으려면 env에 넣으면 되고 이는 array이다. key-value pair로 넣을수도 있지만 ConfigMap이나 Secrets에서 value를 가져올 수도 있다.  valueFrom으로 가져온다.    Configuring ConfigMaps in Applications  ConfigMap은 key-value pair data를 저장하는 리소스이다. 파드가 생성되면 ConfigMap 안에 있는 정보들이 environment로 들어가서 파드 내에서 사용할 수 있게 된다. 사용법은 먼저 생성하고 이를 파드 내에 inject하는 것이다. 간단 생성: kubectl create configmap \u0026lt;config-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; 파일에서: kubectl create configmap \u0026lt;config-name\u0026gt; --from-file=\u0026lt;path-to-file\u0026gt; 환경변수 전체를 가져올때 사용가능, 또는 단일 value만을 가져올 수도 있음 아니면 volumes에서 configMap에 있는 데이터를 통해 volume mount도 가능.  Configure Secrets in Applications  app 내에서 사용하는 값들을 configMap으로 관리할 수 있다. 여기서 공개하기 어려운 값들이 있을 수 있는데 이를 Secret으로 관리한다. 이게 Environment Value로 들어간다. 사용법: 생성하고 파드에 inject kubectl create secret generic \u0026lt;secret-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; kubectl create -f 로도 생성 가능.  key-value pair 그러나 그냥 저장하면 보안 취약. encode할 것. kubectl describe secrets로 조회되지 않음. kubectl get secrets -o yaml으로 조회해야 함. hash value로 나옴. (base64)   환경변수, 단일 ENV, Volume으로 마운트도 가능. 각 Key값에 해당하는 파일이 생성되며 그 내용이 value값이다. 그러나 secret은 그 자체로 보안이 뛰어난 것이 아니다. 보호하는 방법  secret은 vcs로 관리하지 않기 ETCD에 암호화해서 저장하기   참조:  https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ https://blog.outsider.ne.kr/1266/    Multi Container PODs  micro service로 변경하면서 작은 단위로 코드를 관리하고 배포할 수 있게 되었다. scale up, scale down을 각 마이크로 서비스 단위로 할 수 있다. 그 중에서 두 pair가 반드시 함께해야하는 경우가 있을 수 있다. 개발은 따로 하지만 항상 짝을 이뤄야 하는 경우 multi container를 사용하면 된다. 같은 lifecycle을 가짐. localhost로 통신 가능. 같은 storage volume에 접근. 따라서 volume sharing이나 파드간 서비스를 설정하지 않아도 된다.  Multi-container PODs Design Patterns  logging같은 것을 하기 위해 sidecar pattern으로 multicontainer 사용하기 adpater 패턴 ambassador 패턴 이 내용은 CKAD에서.  initContainers  파드 내의 컨테이너들은 함께 죽고 산다. 이 중 하나라도 fail이 되면 파드는 재시작된다. 파드가 생성되고나서 한번만 실행하고 싶은 동작들은 initContainers로 관리할 수 있다. 다른 컨테이너처럼 initContainers 안에서 설정이 가능하다. 여러개의 initContainer가 있으면 순서대로 실행하고, 순서대로 성공해야한다. 하나의 initContainer라도 실패하게 되면 파드를 계속해서 재시작한다.  Tips  readiness, liveness는 CKAD에서  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/",
	"title": "CKA Study",
	"tags": [],
	"description": "",
	"content": "CKA Study  02 Core Concepts     03 Scheduling     04 Logging and Monitoring     05 Application Lifecycle Management     06 Cluster Maintenance     07 Security     08 Storage     09 Networking     10 Kubernetes the Hard Way     12 End to End Tests on a Kubernetes Cluster     13 Troubleshooting     14 Other Topics     "
},
{
	"uri": "http://kimmj.github.io/harbor/",
	"title": "Harbor",
	"tags": [],
	"description": "",
	"content": "Harbor  Private Docker Registry 오픈소스: Harbor란     Harbor 설치     "
},
{
	"uri": "http://kimmj.github.io/git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "Git  git-secret을 통한 github 파일 암호화     Gitignore 설정     "
},
{
	"uri": "http://kimmj.github.io/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": "Python  [번역]Python을 통해 이쁜 CLI 만들기     "
},
{
	"uri": "http://kimmj.github.io/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Docker  http를 사용하는 docker registry를 위한 insecure registry 설정     Docker를 sudo없이 실행하기     [docker-compose] container에서 다른 container로 접속하기     "
},
{
	"uri": "http://kimmj.github.io/jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "Jenkins  Jenkins Install     Workspace@2를 변경하기 - Workspace List 설정 변경     "
},
{
	"uri": "http://kimmj.github.io/iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Infrastructure as Code  [번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials     "
},
{
	"uri": "http://kimmj.github.io/cicd/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "CICD  Deploy Strategy     "
},
{
	"uri": "http://kimmj.github.io/css/",
	"title": "CSS",
	"tags": [],
	"description": "",
	"content": "CSS  background image 어둡게 하기     Greater Than Sign     "
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/controllers-overview/",
	"title": "Controllers Overview",
	"tags": ["kubernetes", "concepts"],
	"description": "",
	"content": "Contents 이 포스트에서는 Kubernetes의 Controller들에 대해서 알아보도록 하겠습니다. 가장 작은 단위인 Container부터, 상위 개념인 Deployment, StatefulSet까지 다루어 보도록 하겠습니다.\n Containers Pods ReplicaSets Deployments StatefulSets  Monolithic vs. Microservice 우선 Monolithic과 Microservice에 대해서 짚고 넘어가도록 하겠습니다.\nMonolithic의 개념은 하나의 큰 어플리케이션을 말합니다. 여러 사람이 개발을 하고 나서 하나의 큰 패키지로 빌드하고 이를 배포하죠. 간단한 서비스라면 문제가 발생하지는 않겠지만, 점점 코드의 수가 늘어나고 거대해질 수록 문제점이 생깁니다. 예를 들면 빌드시간이 오래걸린다던지, scale-out을 하기 힘들다던지 하는 문제가 있겠네요.\n반면 Microservice는 하나의 큰 어플리케이션을 여러 조각으로 쪼갠 것을 의미합니다. 각 조각(microservice)는 자신의 역할이 있고, 이를 잘 수행하면 됩니다. 전체적인 관점에서 보면 어플리케이션은 microservice들간의 통신으로 행해진다고 보면 될 것 같습니다.\nMonolithic에 비해 Microservice는 몇가지 장점들이 있습니다.\n scale-out이 용이합니다.\nMonolithic에 비해 microservice의 단위는 작기 때문에 scale-out 하는데 시간도 오래 걸리지 않고, 간단합니다. 빠른 배포가 가능합니다.\nMonolithic에서는 사이즈가 커질수록 빌드하는 시간이 점점 길어진다고 설명했었습니다. 이는 곧 요즘처럼 트렌드라던지 상황이 급변하는 상황에서 약점이 될 수 밖에 없습니다. 반면 Microservice는 작은 조각이기 때문에 빌드하는 시간이 Monolithic에 비해 뛰어날 수밖에 없습니다. 그리고 이를 배포하는 시간도 매우 줄어들게 되죠. 문제가 발생하였을 때 영향이 적습니다.\n쉽게 우리가 가장 잘 알고있는 게임중 하나인 LoL을 가지고 설명해 보도록 하겠습니다. 만약 LoL을 플레이하고 싶은데, 상점에 에러가 있어서 이용하지 못한다면 어떻게 되나요? LoL이 Monolithic이었다면 게임 플레이가 막혀서 엄청난 원성을 샀을 것입니다. 하지만 LoL또한 Microservice이기 때문에 문제가 발생한 곳만 이용하지 못할 뿐, 나머지 서비스는 정상적으로 이용이 가능합니다. 따라서 복구하기도, 운영하기도 훨씬 쉽습니다.  이렇게 보면 무조건 Microservice만 해야하는 것처럼 보이기도 합니다. 하지만 세상일이 모두 그렇듯 여기에도 정답은 없습니다. 자신이 개발하고자 하는 어플리케이션의 특성을 잘 파악해서 한가지를 선택하고 개발하는 것이 좋은 방향이 될 것 같습니다.\nConatiners 이제 본격적인 설명으로 넘어가보도록 하겠습니다.\nDocker를 사용해 보았다면 container도 친숙한 개념이 될 것 같습니다. container는 VirtualBox처럼 가상화를 하지만, OS까지 가상화하는 것이 아니라 host OS 위에서 커널을 공유하는 방식으로 가상화를 합니다. 어려운 말일수도 있지만 간단하게 말하자면 VirtualBox같은 hypervisor에 의한 가상화보다 훨씬 가벼운 방법으로 가상화를 할 수 있다고 생각하시면 됩니다. 여기서 가볍다는 의미는 빠르게 생성/삭제할 수 있고 용량도 작다는 의미입니다.\nKubernetes는 이런 Docker와 같은 container runtime 기반으로 동작합니다. 그리고 각 container는 어플리케이션 내에서 자신의 역할을 수행하는 것들입니다. Docker를 통해 컨테이너를 동작시키는 것처럼, Kubernetes도 컨테이너를 Docker같은 container runtime의 힘을 빌려 동작시킵니다.\nPods Pod는 Kubernetes에서 어플리케이션을 관리하는 단위입니다.\n하나의 Pod는 여러개의 Container로 구성될 수 있습니다. 즉, 너무나 밀접하게 동작하고 있는 여러 Container를 하나의 Pod로 묶어 함께 관리하는 것입니다.\n이 때, 하나의 Pod 내에 존재하는 모든 Container들은 서로 localhost를 통해 통신할 수 있습니다.\nKubernetes에서 Pod는 언제 죽어도 이상하지 않은 것으로 취급됩니다. 동시에 언제 생성되어도 이상하지 않은 것, 어디에 떠있어도 이상하지 않은 것이죠. 그 만큼 어플리케이션 개발자는 Pod를 구성할 때 하나의 Pod가 장애가 나는 경우에도 어플리케이션이 제대로 동작할 수 있도록 만들어야 합니다. 언제 없어졌다가 생성될지 모르니까요.\nReplicaSet ReplicaSet은 Pod를 생성해주는 녀석입니다.\n단일 Pod를 그냥 생성해도 되지만 그렇게 할 경우 Kubernetes가 주는 여러 이점들, 예를 들어 auto healing, auto scaling, rolling update 등을 이용하지 못합니다. 때문에 Pod를 관리해주는 Controller가 필요하게 되는데 이것이 ReplicaSet입니다.\nReplicaSet은 자신이 관리해야하는 Pod의 template을 가지고 있습니다. 그리고 주기적으로 Kubernetes를 주시하며 내가 가지고 있는 template에 대한 Pod가 원하는 숫자만큼 잘 있는지 확인합니다. 부족하다면 Pod를 더 생성하고 너무 많으면 Pod를 삭제합니다.\nReplicaSet은 Pod를 label을 기준으로 관리합니다. 자신이 가지고 있는 matchLabels와 일치하는 Pod들이 자신과 관련된 Pod라고 인식하는 것이죠. 따라서 label을 운영중에 바꾸는 일은 웬만해선 피해야 합니다. 고아가 발생하여 어느 누구도 관리해주지 않는 Pod가 남게 될 수 있기 때문입니다.\n또한 ReplicaSet은 자신이 생성한 파드들을 \u0026lt;ReplicaSet의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;으로 생성합니다. 따라서 Pod의 이름만 보고도 어떤 ReplicaSet이 생성했는지 알 수 있게되죠.\nDeployments 위에서는 ReplicaSet에 대해서 이야기해 보았습니다. Deployments는 ReplicaSet보다 상위 개념의 Controller입니다.\n만약 Pod의 template을 수정하는 경우가 생긴다면 어떻게 해야 할까요? 예를 들어 cpu 할당량을 바꾼다던지, image를 다른 이미지로 변경한다던지 하는 경우가 발생한다면요. ReplicaSet만 존재한다면 이런 상황에서 새로운 template을 가지고 ReplicaSet을 생성하고, 기존의 ReplicaSet을 삭제하거나 replicas: 0으로 변경하여 ReplicaSet만 남아있고 실제 Pod는 없도록 해야합니다.\n그렇다면 새로 만든 template에 오류가 있어서 이전 버전으로 돌아가고 싶다면 어떻게 해야할까요? 이전에 작성했던 ReplicaSet의 replicas를 늘리고, 현재 올라가있던 ReplicaSet의 replicas를 줄이면 될 것 입니다. 하지만 이는 일일이 기억해야하는 크나큰 단점이 있겠죠.\n때문에 ReplicaSet을 관리해주는 Deployment가 필요합니다. Deployment는 ReplicaSet을 생성하고 이 ReplicaSet이 Pods를 생성하도록 만듭니다. 그리고 자신이 관리하는 ReplicaSet의 labels를 자신의 matchLabels로 일치시켜 구분합니다.\n그렇다면 Pod의 template이 변경되는 상황엔 이번에는 어떻게 적용이 될까요?\nDeployment는 새로운 ReplicaSet을 생성하고, 기존 ReplicaSet의 replicas를 0으로 줄입니다. 그러면서 자신이 생성했던 ReplicaSet들을 revision으로 관리하죠. 이렇게 되면 사용자는 kubectl 명령어만 가지고 이전 template을 가진 Pod로 변경할 수 있습니다.\n또한 Deployments가 ReplicaSet을 생성할 때는 \u0026lt;Deployment의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;으로 생성합니다. 위에서 ReplicaSet도 Pod를 생성할 때 \u0026lt;ReplicaSet의 이름\u0026gt;-\u0026lt;hash 값\u0026gt;이라고 설명했었습니다. 따라서 Deployments에 의해 만들어진 Pod들은 \u0026lt;Deployment의 이름\u0026gt;-\u0026lt;ReplicaSet에 대한 hash 값\u0026gt;-\u0026lt;Pod에 대한 hash 값\u0026gt;과 같은 형태를 띄게 됩니다.\nStatefulSets StatefulSet은 좀 특이한 녀석입니다.\nPod는 어디에 떠있어도 이상하지 않은 것이라고 언급했었습니다. 그런데 StatefulSet은 그렇지 않습니다. 이름에 걸맞게 이전의 상태를 그대로 보존하고 있어야 합니다. 따라서 여러 제약사항들이 생기기도 합니다.\n여기에서는 Pod와의 관계만 알아보고 넘어가도록 하겠습니다.\nStatefulSets는 Deployments와는 다르게 ReplicaSet을 생성하지 않습니다. 대신 자신이 직접 Pod를 생성합니다. 이 때 Pod의 label 속성을 자신의 matchLabels과 일치시켜 자신이 관리하고 있는 Pod를 구분합니다.\n또한 이름도 hash값을 사용하지 않고 0부터 시작하는 숫자를 사용합니다. 즉, \u0026lt;StatefulSets의 이름\u0026gt;-\u0026lt;0부터 오름차순\u0026gt;의 이름을 가지는 Pod를 생성합니다.\n만약 생성해야하는 Pod의 template에 변화가 있다면 어떻게 해야할까요?\nStatefulSets는 Deployments와 다르게 변경할 수 있는 부분에 제약이 있습니다. 이 경우 절대 StatefulSet을 update할 수 없습니다. 따라서 기존 StatefulSet을 삭제하고 다시 kubectl apply와 같은 명령어로 새로운 template을 가지고 생성해야합니다.\nRollback에도 제약사항이 있습니다.\nRollback시 StatefulSet은 Pod가 완전히 Ready상태가 되길 기다립니다. 그런데 만약 StatefulSet이 생성한 Pod가 ImagePullbackOff같은 에러에 빠지게 된다면, 또는 영원히 rediness probe에 의해 Ready 상태가 되지 않는다면 StatefulSet의 rollback은 멈춰버립니다. 이는 Known Issue로 수동으로 해당 Pod를 삭제하는 방법밖에 없습니다.\n따라서 웬만하면 설계를 할 때 StatefulSet을 지양하는 것이 Kubernetes의 Design에 더욱 맞는 방향일 것입니다.\n마치며.. 이렇게 Container, Pods, ReplicaSets, Deployments, StatefulSets의 관계를 중심으로 알아보는 시간을 가졌습니다. 혹시나 잘못된 부분이 있다면 언제든지 댓글 또는 메일로 알려주시면 감사하겠습니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/",
	"title": "Concepts",
	"tags": [],
	"description": "",
	"content": "Kubernetes Concepts  Controllers Overview     Services     Pods     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "Ubuntu Tools  Tmux     "
},
{
	"uri": "http://kimmj.github.io/prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "Prometheus  Install Prometheus     Federation     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/canaryanalysis/",
	"title": "CanaryAnalysis",
	"tags": [],
	"description": "",
	"content": "Spinnaker Canary Analysis Canary Analysis는 Spinnaker에서 자동으로 분석을 통해 새로운 버전에 문제가 없는지 확인해주는 pipeline입니다.\n Canary Analysis     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/choose-a-storage-service/",
	"title": "Choose a Storage Service",
	"tags": ["install", "spinnaker", "minio"],
	"description": "",
	"content": "Spinnaker들의 데이터를 저장할 공간입니다.\n여러가지 옵션들이 있지만, 저는 local로 운용할 수 있는 minio를 통해 데이터를 저장해 볼 것입니다.\nminio를 docker-compose를 통해 쉽게 배포하도록 할 것입니다. 먼저, docker-compose를 설치합니다.\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 그 뒤 minio의 docker-compose.yaml을 만듭니다.\nversion: '3.7' services: minio: image: minio/minio:RELEASE.2020-01-16T22-40-29Z volumes: - ./data:/data ports: - \u0026quot;9000:9000\u0026quot; environment: MINIO_ACCESS_KEY: minio MINIO_SECRET_KEY: minio123 command: server /data healthcheck: test: [\u0026quot;CMD\u0026quot;, \u0026quot;curl\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;http://localhost:9000/minio/health/live\u0026quot;] interval: 30s timeout: 20s retries: 3 docker-compose를 통해서 deamon으로 실행합니다.\ndocker-compose up -d 이제 halyard와 연동을 하도록 합니다.\n먼저, ~/.hal/default/profiles/front50-local.yml 파일을 다음과 같이 생성합니다.\nspinnaker: s3: versioning: false 그 다음 다음의 명령어로 연동을 합니다.\nENDPOINT=http://10.0.2.4:9000 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 echo $MINIO_SECRET_KEY | hal config storage s3 edit --endpoint $ENDPOINT \\  --access-key-id $MINIO_ACCESS_KEY \\  --secret-access-key hal config storage edit --type s3 "
},
{
	"uri": "http://kimmj.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes Kubernetes는 deploy, scaling, 그리고 컨테이너화된 애플리케이션의\nmanagement를 자동화 해주는 open source container orchestration engine입니다.\n Install    Overview     Install Kubeadm     Create a Single Control Plane Cluster With Kubeadm      CKA Study    02 Core Concepts     03 Scheduling     04 Logging and Monitoring     05 Application Lifecycle Management     06 Cluster Maintenance     07 Security     08 Storage     09 Networking     10 Kubernetes the Hard Way     12 End to End Tests on a Kubernetes Cluster     13 Troubleshooting     14 Other Topics      Concepts    Controllers Overview     Services     Pods      CKA: Certified Kubernetes Administrator 취득 후기     [번역] 쿠버네티스에서의 Port, TargetPort, NodePort     Stern을 이용하여 여러 pod의 log를 한번에 확인하기     "
},
{
	"uri": "http://kimmj.github.io/hugo/ibiza/",
	"title": "Ibiza",
	"tags": [],
	"description": "",
	"content": "Hugo Ibiza Ibiza는 이 블로그를 만드는 프로젝트입니다.\n Font Change     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Ubuntu Network  Netplan으로 static IP 할당받기     "
},
{
	"uri": "http://kimmj.github.io/hugo/",
	"title": "Hugo",
	"tags": [],
	"description": "",
	"content": "Hugo fast static website engine\n Ibiza    Font Change      Hugo에 Google Analytics 적용하기     Hugo에 Comment 추가하기 (Utterance)     HUGO로 HTML이 되지 않을 때 가능하게 하는 방법     "
},
{
	"uri": "http://kimmj.github.io/spinnaker/",
	"title": "Spinnaker",
	"tags": [],
	"description": "",
	"content": "CI/CD Spinnaker Spinnaker는 Kubernetes 환경에서 배포 자동화를 위해 만들어진 툴입니다.\n배포하려는 클러스터가 GKE인지, EKS인지, On-Premise 환경인지 상관없이 하나의 툴로 배포하기 위해 만들어졌습니다.\n이 툴 자체가 MSA 구조로 만들어져있습니다.\n Installation    Overview     Install Halyard     Choose Cloud Providers     Choose Your Environment     Choose a Storage Service     Deploy and Connect     Install in Air Gaped Environment      CanaryAnalysis    Canary Analysis      Tips    Pipeline Expressions      "
},
{
	"uri": "http://kimmj.github.io/ansible/",
	"title": "Ansible",
	"tags": [],
	"description": "",
	"content": "Ansible  Create Vm With Ansible Libvirt     "
},
{
	"uri": "http://kimmj.github.io/ubuntu/",
	"title": "Ubuntu",
	"tags": [],
	"description": "",
	"content": "Ubuntu Ubuntu에서 배운 것들을 기록하는 공간입니다.\n Tools    Tmux      Network    Netplan으로 static IP 할당받기      Ubuntu의 Login Message 수정하기     reboot 후에 tmux를 실행시켜 원하는 작업을 하기     oh-my-zsh에서 home key와 end key가 안될 때 해결방법     Ubuntu에서 Base64로 인코딩, 디코딩하기     Editor(vi)가 없을 때 파일 수정하기     열려있는 포트 확인하기     pipe를 사용한 명령어를 watch로 확인하기     watch를 사용할 때 alias 이용하기     password 없이 ssh 접속하기     SSH Tunneling 사용법     Gateway를 이용하여 SSH 접속하기     Hostname 변경하기     추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기     Ubuntu 설치 시 Boot Parameter를 수정하기     sudo를 password 없이 사용하기     "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/06-cluster-maintenance/",
	"title": "06 Cluster Maintenance",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "OS Upgrades  Node가 Down된 상태에서 5분이 지나면 pod는 terminate가 된다.  Dead로 간주   ReplicaSet으로 관리되면 다른곳에 파드를 띄워준다. kube-controller-manager에 pod-eviction-timeout이 기본적으로 5분이 설정되어 있다. 다시 online으로 오게되면 blank node로 뜨게 된다. ReplicaSet으로 관리되지 않았던 파드는 삭제되고 재생성되지 않는다. 따라서 kubectl drain node를 통해 파드를 이주시키고 cordon을 통해 스케줄되지 않도록 한다. 그 다음에 노드를 down시키고 살리면 된다. 산 뒤에도 이전에 처리한 cordon이 남아있게 되는데 이를 삭제하기 위해선 kubectl uncordon을 해야한다. 자동으로 이주되었던 파드가 돌아오지 않는다. cordon은 unschedulable로 처리하는 작업이다.  Kubernetes Software Versions  Release Number는 3가지로 구분  MAJOR, MINOR, PATCH MINOR 버전은 Feature, Functionalities 위주 PATCH는 Bug Fixes 위주   stable한 버전(MINOR의 업그레이드)는 매달 일어남. alpha 버전은 default로 disable되어 있는 feature. 버그 등이 있을 수 있음 beta 버전은 잘 테스트 되었고 default로 enable되어 있는 feature. stable로 통합됨. release package에는 모든 controller component가 패키징되어있음.(동일한 버전으로) ETCD cluster나 CoreDNS같은 controller component가 아닌 것은 포함되어있지 않음.  supported version을 명시해둠.    Cluster Upgrade Process  웬만하면 Controller Component는 동일한 버전을 가지고 있어야 한다. kube-apiserver가 1.x 버전이면 controller-manager와 kube-scheduler는 1.x ~ 1.x-1 버전이 될 수 있다.  kubelet과 kube-proxy는 1.x-2 ~ 1.x kubectl은 1.x-1 ~ 1.x+1   kubernetes는 최근 3버전만 support한다. 업그레이드 시 MINOR 버전 하나씩 업그레이드 하는 것을 권장한다. Cloud provider로 관리되고 있으면 클릭만 해서 업그레이드가 가능하다. kubeadm과 같은 툴을 사용할 경우 tool에 내장된 업그레이드 플랜을 사용하면 된다. scratch로 구성했을 경우 수작업으로 업그레이드 해야한다. 강의에서는 kubeadm 사용. 클러스터 업그레이드는 두가지 major step으로 나뉨.  마스터 노드 업그레이드  업그레이드 시 Control plane component들 (apiserver, scheduler, controller manager)는 잠시 끊긴다. 그렇다고 해서 워커노드에 영향이 있다는 것을 의미하지는 않는다. 마스터 노드가 다운이 되기 떄문에 클러스터에 접속이 불가능하다. 마스터 노드가 업그레이드 되더라도 워커노드들의 controller component와의 통신이 지원된다.(버전 하나차이)   워커 노드 업그레이드  한번에 업그레이드 하기  유저가 접속할 수 없음. 다운타임 발생   하나씩 업그레이드 하기  다운타임 없음.   새 버전의 노드 새로 추가  public cloud 사용할 떄 좋음. 하나씩 추가.       kubeadm은 upgrade 명령어가 있음. kubeadm upgrade plan  kubeadm은 kubelet의 업그레이드를 하지 않음. 따라서 스스로 업그레이드 해야함. 또한 kubeadm의 업그레이드를 먼저 해야함. apt-get upgrade 명령어시 다른 패키지도 업그레이드가 되기 때문에 apt-get install 명령어로 단일 패키지만 업그레이드 시킨다. apt-get install -y kubeadm=\u0026lt;version\u0026gt; kubeadm upgrade apply \u0026lt;version\u0026gt; 이후에도 여전히 kubectl get nodes로 조회한 것은 이전 버전이 나온다. 이는 kubelet의 버전을 표시하기 때문이다. 설치 방법에 따라 kubelet이 설치 되었을수도, 아닐수도 있다. scratch 방식으로 설치했을 떄에는 kubelet을 설치하지 않는다. apt-get install -y kubelet=\u0026lt;version\u0026gt; systemctl restart kubelet kubectl get nodes시 업그레이드된 버전이 보임. 워커노드의 업그레이드  워커노드를 업그레이드 하려면 drain을 통해 노드를 비워야 한다. cordon을 통해 unschedulable로 변경 apt-get install -y kubeadm=\u0026lt;version\u0026gt; apt-get install -y kubelet=\u0026lt;version\u0026gt; kubeadm upgrade node config --kubelet-version \u0026lt;version\u0026gt; systemctl restart kubelet 이를 각 노드를 순회하며 실행한다.     apt-mark kubelet kubectl kubeadm을 통해 자동업그레이드가 안되도록 설정한다.  Backup and Restore Methods  Backup Candidates  Resource Configuration ETCD Cluster Persistent Volumes   Resource Configuration  Declarative방식으로 사용하여 저장할 것을 권장. yaml파일로 구성하는 것. 이런 파일을 github등으로 저장할 것. kube-apiserver로부터 모든 리소스를 가져와서 백업할 수도 있음. kubectl get all --all-namespace -o yaml \u0026gt; all-deploy-services.yaml 그러나 몇가지 리소스는 빠져있음. VELERO라는 툴로 모든 리소스를 저장할 수 있음.   ETCD Cluster  cluster의 state를 저장한다. master node에 배포된다. --data-dir=/var/lib/etcd의 형태로 모든 데이터를 저장하는 path를 지정한다. Backup  ETCD는 etcdctl에서 snapshot command를 제공한다. backup하기 위해서는 service kube-apiserver stop을 해야한다.  ETCD cluster를 restart를 하게 되는데 kube-apiserver가 이에 대해 의존성을 가지고 있기 때문이다.     Restore  etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup ... --initial-cluster-token etcd-cluster-1 ... 이 정보들은 original file에서 가져와야 한다. 여기서 설정한 --data-dir과 --initial-cluster-token으로 기존의 etcd.service를 채워야 한다. systemctl daemon-reload를 통해 설정을 다시 읽는다 systemctl etcd restart service kube-apiserver start     ETCD cluster에 접근할 수 없으면 kube-apiserver를 통한 백업이 가장 나은 방법이다. 두 방법 모두 장/단점이 있다.  Working with ETCDCTL  실습에서 ETCD는 master 노드에서 static pod로 v3가 배포되어 있다. backup과 restore를 위해서는 ETCDCTL_API=3을 설정해야 한다. 환경변수로 추가하면 된다. TLS 기반으로 만들어진 ETCD Cluster이기 떄문에 --cacert, --cert, --endpoints=[127.0.0.1:2379], --key는 필수이다.  Tips  실제 시험 환경에서는 어떤것이 문제인지 알 수 없으니 kubectl describe pod를 통해 디버깅을 해야한다. https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster/ https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md/ https://www.youtube.com/watch?v=qRPNuT080Hk/  "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/deploy-and-connect/",
	"title": "Deploy and Connect",
	"tags": ["spinnaker", "install"],
	"description": "",
	"content": "드디어 마지막 절차입니다.\n먼저 어떤 버전을 설치할지 확인후 설정합니다.\nhal version list 작성 기준으로 최신 버전이 1.17.6이므로 이를 설정합니다.\nhal config version edit --version 1.17.6 halyard를 NodePort로 노출시키기 위해 api와 ui에 base url을 부여합니다.\nhal config security ui edit --override-base-url http://192.168.8.22:30100 hal config security api edit --override-base-url http://192.168.8.22:30200 이제 본격적으로 deploy를 하도록 합니다.\nhal deploy apply 그 후 Spinnaker를 NodePort로 서비스합니다.\nkubectl patch svc spin-deck -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;}]\u0026#39; kubectl patch svc spin-gate -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;}]\u0026#39; kubectl patch svc spin-deck -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;: 30100}]\u0026#39; kubectl patch svc spin-gate -n spinnaker --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;: 30200}]\u0026#39; 이제 Spinnaker로 접속하여 확인합니다. url은 http://:30100 입니다.\n여기까지 했으면 Spinnaker를 Kubernetes에서 사용할 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/07-security/",
	"title": "07 Security",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Kubernetes Security Primitives  host와 cluster 자체의 security  당연히 host는 그 자체로 안전해야하고, root access는 disable 되어야 하며 password based authentication은 disabled되고 SSH key based authentication만 가능해야 한다. Kubernetes를 실행시키고 있는 호스트에 대한 physical, virtual infrastructure에 대한 보안도 필요하다.   쿠버네티스와 관련된 security  kube-apiserver는 모든것을 하는 주체가 되기 때문에 이곳에 대한 보안부터 챙겨야 한다. Who can access? / What can they do? API 서버에 누가 접근을 할 수 있게 할 것인가?  Files - Username and Passwords Files - Username and Tokens Certificates External Authentication providers - LDAP Service Accounts   What can they do  RBAC Authorization ABAC Authorization Node Authorization Webhook Mode   TLS Certificates  모든 controller component들은 TLS Encryption이 되어있다.   Cluster 내의 application에서의 communication  모든 파드는 클러스터 내의 모든 파드로 access 할 수 있다. Network Policies를 통해 access를 제한할 수 있다.      Authentication  Admin, Developers, End Users, Third Party가 클러스터에 접근한다. 내부 컴포넌트에서의 통신에 보안성을 추가하는 방법과 authentication, authorization 메카니즘으로 클러스터에 접근하는 것에 security management를 할 것이다. 그 중 authentication 메커니즘에 대해 집중할 것이다. End user에 대한 보안은 어플리케이션 내에서 해야한다. 두가지 방식으로 구분해보면 관리자, 개발자같은 사람인 User와 자동화 프로세스 등을 하는 로봇인 Service Accounts로 구분지을 수 있다. 쿠버네티스는 유저관리같은 것을 원래 지원하지는 않는다. 하지만 Service Account는 쿠버네티스가 지원한다. kube-apiserver는 authenticate user -\u0026gt; proccessing 의 순서로 동작한다. authenticate machanisms  Static Password File Static Token File Certificates Identity Services(LDAP)   static password, token file  kube-apiserver에 csv형태로 \u0026lt;password\u0026gt;,\u0026lt;user name\u0026gt;,\u0026lt;user ID\u0026gt;를 저장한뒤 전달하면 이를 사용할 수 있다.  --basic-auth-file=user-details.csv 옵션을 kube-apiserver에 추가하여 적용. restart 필요 kubeadm을 사용했다면 pod definition file을 수정해야 한다. 이는 curl 사용 시 -u \u0026quot;user1:password123\u0026quot;의 형태로 사용할 수 있다. csv 파일에 4번째 column을 추가할 수 있는데 이는 \u0026lt;group ID\u0026gt;를 의미한다.   이와 비슷하게 csv 형태로 \u0026lt;token\u0026gt;,\u0026lt;user name\u0026gt;,\u0026lt;user ID\u0026gt;,\u0026lt;group ID\u0026gt;를 저장할 수 있다.  --token-auth-file=user-details.csv 형태로 적용 가능 curl 사용 시 --header \u0026quot;Authorization: Bearer \u0026lt;token\u0026gt;\u0026quot; 형태로 적용이 가능.     static file로 설정하는 것은 추천하는 방법은 아니다. consider volume mount while providing the auth file in a kubeadm setup.  TLS Basics  transaction에서 두 party를 guarantee하기 위해 사용된다. user가 web server에 접근한다고 생각해보면 둘 간의 통신이 encrypt 되었음을 보장하기 위해 TLS를 사용한다. 데이터를 서버에 전송할 때 key를 통해 encrypt 되기 때문에 이를 받는 서버 또한 key를 가지고 있어야 복호화가 능하다.  여기서 암호화/복호화에 동일한 key를 사용하는 것을 Symmetric Encription이라고 한다. 해커가 key를 전송하는 과정 중에 이를 탈취해서 복호화할 가능성이 있다.   asymmetric encryption  private key와 public key 두개로 구성 간단하게 생각한다면 private key - public lock의 구성이라고 보면 된다. private key는 나만 가지고 있고 public key는 공개하여 누구나 암호화할 수 있도록 한다.   SSH를 asynmmetric으로 접근하기  ssh-keygen을 통해 private key와 public key를 생성한다. 서버를 public key를 통해 암호화한다.  public key를 서버에 추가하면 된다. ~/.ssh/authorized_keys에 추가. 그 뒤 ssh -i \u0026lt;private key file\u0026gt; user@server로 접속하면 된다.   더 많은 서버에 접속한다면 pulic key만 서버들에 대해 복사하면 된다. 다른 사람도 접근하고자 한다면 private key를 추가하면 된다.   symmetric encryption에서 key를 함께 보낼 때 key를 해커가 탐지할 수 있다.  이를 asymmetric encryption으로 key를 암호화한 뒤 보내면 해결 가능. private key 생성: openssl genrsa -out my-bank.key 1024 public key 생성: openssl rsa -in my-bank.key -pubout \u0026gt; mybank.pem   유저가 https로 서버에 최초 접근하면 public key를 서버로부터 받게 된다.  이 때 자신의 symmetric key를 서버의 Public key로 encryption한 뒤 전송한다. 이를 받으면 서버는 자신의 private key로 복호화를 진행한다. 해커가 이를 가져가도 어떤것을 할 수 없다. 이를 통해 서버와 유저는 symmetric key를 통해 서로 통신한다.   해커는 replica 서버를 만들어서 유저의 symmetric key를 탈취하려 할 것이다.  해커의 서버를 보면 key와 함께 certificate를 전송한다. 이 certificate 안에는 Issuer가 있다.  이는 그 서버의 identity의 유효성을 검증하는데 매우 중요하다.   Subject은 그 certificate에서 인증하는 서버를 나타낸다.   certificate는 누구나 만들 수 있기 때문에 누가 certificate를 sign하는지가 중요하다.  스스로 만들었다면 자신이 sign할 것이고 이를 self-signed certificates라고 한다. 이 self-signed certificates는 안전하지 않은 인증서이다. 따라서 해커가 사용하는 certificates도 self-signed certificates이다.   웹 브라우저는 자동으로 이런 self-signed certificates에 대해 경고를 해준다. 다른 3자에게 인증을 받고자 한다면 Certificates Authority(CA)가 필요하다.  이를 위해서는 Certificate Signing Request(CSR)을 전송해야한다. openssl req -new -key my-bank.key -out my-bank.csr -subj \u0026quot;/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com CA는 이를 적절히 확인하고 sign 후 응답을 준다. 해커가 CA로부터 인증을 받으려 하면 validation에서 fail이 날 것이다. 그러면 web browser에서 올바른 certificate임을 간주한다.   Web browser는 어떻게 CA가 올바른 CA라고 판정을 하는가  CA가 만약 fake CA라면? 어떻게 certificate에 서명된 CA가 진짜 CA라고 판정할 수 있는가. CA는 private key를 브라우저 내에 심어놓는다. 이를 통해 web browser는 자신에게 온 sign이 진짜 CA가 전송한 것인지 확인이 가능해진다.   PKI(Public Key Infrastructure) nameing  Public Key는 보통 *.crt, *.pem 형식이다. Private Key는 보통 *.key, *-key.pm  보통 key라는 단어를 섞는다.      TLS in Kubernetes - Certificate Creation  certificate 생성 방법: easyrsa, openssl, cfssl certificate 생성 절차:  public key 생성: openssl genrsa -out ca.key 2048 certificate signing request: openssl req -new -key ca.key -subj \u0026quot;/CN=KUBERNETES-CA\u0026quot; -out ca.csr sign certificates: openssl x509 -req -in ca.csr -singkey ca.key -out ca.crt   client certificate 생성 절차(admin user)  private key 생성: openssl genrsa -out admin.key 2048 certificate signing request: openssl req -new -key admin.key -subj \u0026quot;/CN=kube-admin/O=system:masters\u0026quot; -out admin.csr  반드시 kube-admin일 필요는 없고 kubectl에 사용되는 이름이면 된다.   sign certificates: openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt  ca.key로 certificate를 validate한다.   admin user라는 것을 certificate에 담으려면 group을 추가하면 된다.  system-master를 사용해야 한다. certificate sigining request에 /O=system:masters를 추가하여 나타냄.     system component들은 certificate에서 앞에 SYSTEM:이라는 prefix가 필요하다.  kube-scheduler kube controller manager kube-proxy   여기서 생성한 certificate를 사용하는 방법  curl \u0026lt;url\u0026gt; --key \u0026lt;private key\u0026gt; --cert \u0026lt;certificate\u0026gt; --cacert ca.crt 또는 kubeconfig에 이 내용을 추가한다.   CA certificate는 우리가 만든 것이기 때문에 파라미터로 넣을때 같이 넣어줘야 인증이 원할하게 된다. kube-apiserver는 IP, domain name이 다양하다.  kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local 이런 것들이 모두 certificate에 기록되어야 한다. 방법  certificate signing request시 config 파일을 두어 여기에다가 기록해둔다.  [req] req_extensions = v3_req [v3_req] basicConstraints = CA:FALSE keyUsage = nonRepudiation, subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 10.96.0.1 IP.2 = 172.17.0.87  그 다음 openssl req -new -key apiserver.key -subj \u0026quot;/CN=kube-apiserver\u0026quot; -out apiserver.csr -config openssl.cnf처럼 config 파일을 지정한다.     kubelet의 경우 노드마다 떠있게 되는데 여기에 대한 certificate의 name은 노드의 이름을 따온다.  kubelet-config.yaml에 certificate에 대한 정보를 지정한다. 또한 system:node: prefix를 넣는다. 그리고 SYSTEM:NODES 그룹에 넣어야 한다.    View Certificate Details  이미 설치된 클러스터 내에서 certificate을 확인하는 방법 쿠버네티스가 어떤 방식으로 설치되어 있는지 알아야 한다. scartch로 구성을 할 경우 모든 certificate을 스스로 생성한다. kubeadm같은 automation tool을 이용할 경우 모든 certificate을 알아서 구성해준다. kubeadm  /etc/kubernetes/manifests/kube-apiserver.yaml 여기서 --client-ca-file, --etcd-cafile, --etcd-certfile, --etcd-keyfile, --kubelet-client-certificate, --kubelet-client-key, --tls-cert-file, --tls-private-key 확인. --tls-cert-file=/etc/kubernetes/pki/apiserver.crt로 설정되어 있을 경우 다음의 명령어로 조회 가능.  openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout     journalctl -u etcd.service -l로 로그 확인  Certificates API  새로운 admin이 들어오면, Certificates Request를 작성하게 한 뒤 기존 admin에게 전송하고, 이를 CA 서버에 전달하여 certificate을 생성한 뒤 되돌려준다.  이 경우 certificate이 expire되면 다시 이 프로세스대로 반복해야한다.   CA 서버가 있으면 여기에 접속할 수 있는 사람들이 certificate를 마음껏 생성할 수 있다.  매우 안전한 서버에다가 구성해야한다. certificate에 접속하고자 할 경우 반드시 그 서버를 통해야 한다.   쿠버네티스는 Certificate API를 보유하고 있어서 다음과 같은 작업을 할 수 있다.  admin이 Certificate Singing Request를 받으면 마스터 노드에 접속해서 직접 sign을 받는 것이 아니라, API를 통해서 한다. CertificateSigningRequest Object를 생성한다. 이를 통해 Request를 받고 Approve한 뒤 User에게 Certificate를 전송한다.   Flow  openssl genrsa -out jane.key 2048 openssl req -new -key jane.key -subj \u0026quot;/CN=jane\u0026quot; -out jane.csr 이를 base64로 인코딩해서 request에 담는다. kubectl get csr을 통해 조회해보면 status.certificates에 결과가 나온다. 이 결과를 base64로 decode한다.   csr에 관련된 것은 controller manager에서 관리한다.  Kubeconfig  일반적으로 curl 명령어를 통해 kube-apiserver와 통신을 할 수 있다. * curl https://my-kube-playground:6443/api/v1/pods \\  --key admin.key \\  --cert admin.crt \\  --cacert ca.crt  kubectl을 이용할 때는 다음과 같이 사용할 수 있다. * kubectl get pods --server my-kube-playground:6443 \\  --client-key admin.key \\  --client-certificate admin.crt \\  --certificate-authority ca.crt  이를 매번 치는 것은 귀찮으니 이를 kubeconfig 파일에 넣을 수 있다.   kubeconfig의 default directory는 $HOME/.kube/config 3가지 섹션으로 나뉨.  cluster  접속할 클러스터   users  클러스터에 접속하는 유저   contexts  클러스터와 유저를 연결하는 것     이미 존재하는 user를 골라야 한다. kubeconfig에서 contexts에 특정한 네임스페이스를 사용하도록 지정할 수 있다.  API Groups  쿠버네티스에서 api는 각각의 목적에 따라 그룹으로 구성되어 있다. 크게 core 그룹과 named 그룹으로 나뉜다. core group  /api -\u0026gt; /v1 -\u0026gt; namespaces, pods \u0026hellip;   새로운 feature같은 것들은 named 그룹  /apis -\u0026gt; /apps, extensions, /networking.k8s.io, storage.k8s.io, /authentication.k8s.io, certificates.k8s.io 이 /apps, /extensions 등을 API group이라고 한다. 그 아래에 해당하는 api들을 resources라고 부르고, 각 resource는 특정한 verbs를 가지고 있다.   api 문서를 보면 그룹에 관한 자세한 정보가 있다 kube-apiserver로 요청을 보내면 사용할 수 있는 api 버전들을 보여준다. 그러나 certificates들을 기입해야 한다. 이를 안하려면 kubectl proxy를 사용하면 된다. 그러면 certificates를 사용하지 않을 수 있다. kubeproxy와 kubectl proxy는 다른 것이다.  Role Based Access Controls  kind=Role을 통해 어떤 api group, resources, verbs를 사용할 수 있는지 정의할 수 있다. core 그룹을 사용하려면 api group을 공백으로 두면 된다. 이렇게 정의한 role을 유저에게 적용시키려면 kind=RoleBinding이라는 것을 정의하면 된다. kubectl get roles 및 kubectl get rolebindings를 통해 확인할 수 있다. kubectl auth can-i를 통해 어떤 것을 할 수 있는지 확인할 수 있다.  --as를 통해 user를 특정할 수 있다. --namespace를 통해 네임스페이스에 관해서도 볼 수 있다.   resourceNames 필드를 통해 role이 적용되는 resource의 이름을 지정할 수 있다.  Cluster Roles  Role과 RoleBindings는 네임스페이스 아래에 있다. 네임스페이스를 지정하지 않으면 default 네임스페이스에 대해서 적용된다. 노드는 clusterwide resource이기 때문에 특정 네임스페이스에 대해서 지정할 수 없다. 리소스는 네임스페이스 또는 클러스터 스코프로 정의된다. cluster scoped  nodes, PV, clusterroles, clusterrolebindings, certificatesigningrequests namespaces   kubectl api-resources --namespaced=true, kubectl api-resources --namespaced=false cluster wide인 리소스에 대해서 Role과 RoleBindings를 설정하는 방법은? ClusterRole을 설정한다.  이걸로 전체 네임스페이스에 관한 resource에 대한 권한을 가지기 위해 설정할 수도 있다.    Image Security  private repository를 구축해서 외부로 노출되지 않은 이미지 저장소를 만들 수 있다. image path를 full path를 써서 사용하면 된다. credential이 필요하다면 secret을 만들어 docker-registry type을 사용하면 된다. 이는 built-in type이다.  이를 podSpec의 imagePullSecrets에 추가하면 된다.    Security Contexts  securityContexts를 지정하면 기존 docker image에 정의된 security context를 overwrite할 수 있다.  capability를 설정하여 특정한 capability를 부여할 수 있다. 이는 파드가 아닌 컨테이너 단위로만 설정 가능하다.    Network Policy  web - api - db가 있을 때 웹을 기준으로 user로부터 들어오는 트래픽을 ingress traffic, app server로 나가는 트래픽을 egress traffic이라고 한다.  요청에 대한 응답은 크게 관련 없음.   파드는 라우팅같은 세팅을 통해 어떤 파드에도 접근할 수 있어야 한다. network policy는 리소스로, 파드에 대해 특정한 룰을 통과하는 트래픽만 흐르게 해주고 나머지는 블럭처리한다. labels로 network policy와 pod를 연결시킨다. Flannel은 network policies를 지원하지 않는다.  Tips  Service Account는 CKAD의 영역.  "
},
{
	"uri": "http://kimmj.github.io/spinnaker/installation/install-in-air-gaped-environment/",
	"title": "Install in Air Gaped Environment",
	"tags": ["install", "spinnaker", "air-gaped"],
	"description": "",
	"content": "이번에는 인터넷이 되지 않는 환경에서 어떻게 Spinnaker를 설치하는지에 대해 알아보도록 하겠습니다.\n먼저 halyard에서 언제 인터넷과 통신하는지를 대강 추려보도록 하겠습니다.\n Spinnaker의 version.yaml을 불러와서 최신의 halyard 버전과 최신 Spinnaker의 버전들을 보여줍니다.  gs://halconfig/version.yml   설치하고자 하는 Spinnaker의 버전을 선택하면, 그에 따른 배포에 필요한 yaml들을 불러옵니다.  gs://halconfig/bom/VERSION.yml gs://halconfig/MICRO_SERVICE/TAG.yml   deploy를 하기 위해 Google Cloud Repository에서 이미지를 가지고 옵니다.  gcr.io/spinnaker-marketplace/SERVICE   마지막으로 dependency가 있는 몇가지 서비스를 Google Cloud Repository에서 가지고옵니다. (consul, redis, vault)  gcr.io/kubernetes-spinnaker/SERVICE    여기서 local 설정으로 변경이 가능한 것은 2020.01.20 현재 1,2,3번 항목들입니다. 이것들을 어떻게 인터넷이 되지 않는 환경에서 설치가 가능하도록 설정하는지에 대해 알아보겠습니다.\ngsutil로 gs://halconfig 파일들을 로컬에 복사하기 우선 인터넷이 잘 되는 서버가 하나 필요합니다. 이 서버에서 우리는 필요한 BOM(Bill of Materials)를 미리 다운로드 할 것입니다.\ngsutil이 설치되어 있어야 합니다.\ngsutil -m cp -r gs://halconfig . 이렇게하면 로컬에 halconfig라는 폴더가 생겼을 것입니다. 이를 인터넷이 안되는 halyard가 설치된 서버로 복사합니다. 이때, halconfig 폴더 내의 내용들은 ~/.hal/.boms/ 폴더 내에 복사합니다.\n$ ls ~/.hal/.boms/ bom clouddriver deck echo fiat front50 gate igor kayenta monitoring-daemon orca rosco versions.yml 여기서 rosco/master 폴더로 들어가면 packer.tar.gz라는 폴더가 있습니다. 이를 rosco 폴더로 옮기고 압축을 해제합니다.\nmv ~/.hal/.boms/rosco/master/packer.tar.gz ~/.hal/.boms/rosco cd ~/.hal/.boms/rosco tar xvf packer.tar.gz halyard에서 gcs의 version.yml이 아닌 로컬의 version.yml을 참조하도록 설정 기본적으로 halyard는 gs://halconfig/version.yml을 참조하려 할 것입니다. 이를 local:이라는 접두사를 붙여 로컬을 바라보게 할 수 있습니다.\nhal config version edit --version local:1.17.4 그리고 halyard가 gcs를 바라보지 않도록 설정합니다.\n# /opt/spinnaker/config/halyard-local.yml spinnaker: config: input: gcs: enabled: false 그러고 난 뒤, 각 서비스들의 BOM도 로컬을 바라보게 설정해야 합니다. 아까 위에서 1.17.4 버전을 사용한다고 했으니, 해당 yaml파일을 열고 local: 접두사를 추가합니다.\nartifactSources: debianRepository: https://dl.bintray.com/spinnaker-releases/debians dockerRegistry: gcr.io/spinnaker-marketplace gitPrefix: https://github.com/spinnaker googleImageProject: marketplace-spinnaker-release dependencies: consul: version: 0.7.5 redis: version: 2:2.8.4-2 vault: version: 0.7.0 services: clouddriver: commit: 024b9220a1322f80ed732de9f58aec2768e93d1b version: local:6.4.3-20191210131345 deck: commit: 12edf0a7c05f3fab921535723c8a384c1336218b version: local:2.13.3-20191210131345 defaultArtifact: {} echo: commit: acca50adef83a67e275bcb6aabba1ccdce2ca705 version: local:2.9.0-20191029172246 fiat: commit: c62d038c2a9531042ff33c5992384184b1370b27 version: local:1.8.3-20191202102650 front50: commit: 9415a443b0d6bf800ccca8c2764d303eb4d29366 version: local:0.20.1-20191107034416 gate: commit: a453541b47c745a283712bb240ab392ad7319e8d version: local:1.13.0-20191029172246 igor: commit: 37fe1ed0c463bdaa87996a4d4dd81fee2325ec8e version: local:1.7.0-20191029183208 kayenta: commit: 5dcec805b7533d0406f1e657a62122f4278d665d version: local:0.12.0-20191023142816 monitoring-daemon: commit: 59cbbec589f982864cee45d20c99c32d39c75f7f version: local:0.16.0-20191007112816 monitoring-third-party: commit: 59cbbec589f982864cee45d20c99c32d39c75f7f version: local:0.16.0-20191007112816 orca: commit: b88f62a1b2b1bdee0f45d7f9491932f9c51371d9 version: local:2.11.2-20191212093351 rosco: commit: 269dc830cf7ea2ee6c160163e30d6cbd099269c2 version: local:0.15.1-20191202163249 timestamp: \u0026#39;2019-12-12 14:34:16\u0026#39; version: 1.17.4 이렇게 설정하면 echo를 예로 들 때 ~/.hal/.boms/echo/2.9.0-20191029172246/echo.yml을 참조하게 될 것입니다.\n배포에 필요한 이미지들을 private registry에 불러오기 이제 실제 배포에 필요한 이미지를 로컬로 복사해두어야 합니다. 저는 내부에서 사용하는 docker registry에다가 저장해 둘 것입니다. 인터넷이 되는 서버에서 다음과 같이 작업하면 됩니다.\ndocker pull gcr.io/spinnaker-marketplace/SERVICE:TAG docker tag gcr.io/spinnaker-marketplace/SERVICE:TAG private-docker-registry/repository-name/SERVICE:TAG docker push private-docker-registry/repository-name/SERVICE:TAG 이렇게 private registry로 저장을 해 두었을 경우 VERSION.yml 파일에서 dockerRegistry 항목을 수정합니다.\nartifactSources: debianRepository: https://dl.bintray.com/spinnaker-releases/debians #dockerRegistry: gcr.io/spinnaker-marketplace dockerRegistry: private-docker-registry/repository-name gitPrefix: https://github.com/spinnaker googleImageProject: marketplace-spinnaker-release 또는 docker pull을 이용해서 이미지를 다운받고, 이를 docker save 명령어를 통해 tar.gz 파일로 변환한 뒤, Kubernetes의 모든 워커노드에서 이를 이리 docker load 하는 방법도 있습니다. 이렇게 하면 이미 로컬에 있는 이미지이기 때문에 외부로 접속하지 않습니다.\ndocker pull gcr.io/spinnaker-marketplace/SERVICE:TAG docker save -o SERVICE.tar.gz gcr.io/spinnaker-marketplace/SERVICE:TAG scp SERVICE.tar.gz TARGET_IP:~/path/to/target ssh TARGET_IP docker load -i ~/path/to/target/SERVICE.tar.gz 이번에는 dependency와 관련된 이미지를 불러와야 합니다. 먼저 Image Registry를 변경하기 위해서는 다음과 같이 조치합니다.\n ~/.hal/default/service-settings/redis.yml파일을 생성합니다. 다음과 같이 작성합니다. artifactId: private-docker-registry/repository-name/redis-cluster:v2   이 다음에는 마찬가지로 image를 pull하고 이를 private docker registry로 push합니다.\ndocker pull gcr.io/kubernetes-spinnaker/SERVICE:TAG # redis-cluster:v2 docker tag gcr.io/kubernetes-spinnaker/SERVICE:TAG private-docker-registry/repository-name/SERVICE:TAG docker push private-docker-registry/repository-name/SERVICE:TAG 또는 이미지를 tar로 묶어서 복사하는 방법도 있습니다.\ndocker pull gcr.io/kubernetes-spinnaker/SERVICE:TAG docker save -o SERVICE.tar.gz gcr.io/kubernetes-spinnaker/SERVICE:TAG scp SERVICE.tar.gz TARGET_IP:~/path/to/target ssh TARGET_IP docker load -i ~/path/to/target/SERVICE.tar.gz Image Registry가 kubernetes-spinnaker로 변경된 것을 주의하시면 됩니다.\nDeploy 여기까지 왔으면 모든 준비작업은 끝났습니다. 이제 배포만 하면 됩니다.\nhal deploy apply Reference https://www.spinnaker.io/guides/operator/custom-boms/\nhttps://github.com/spinnaker/spinnaker/issues/3967#issuecomment-522306893\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/08-storage/",
	"title": "08 Storage",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Introduction to Docker Storage  docker에는 두가지 컨셉의 stroage가 있다.  storage drivers volume drivers    Storage in Docker  file system  /var/lib/docker에 docker가 사용하는 파일들이 있다.   docker는 layered architecture라서 caching을 사용할 수 있다. 이렇게 해서 이미지를 실행시키면 해당 이미지에 사용된 layer들은 Read Only이다. 컨테이너 상에서 실행하는 것들은 container layer로, Read Write이다. Read Only상의 파일을 수정하면 도커는 자동으로 이를 복사해서 수정한다. 여기서 영구적으로 보관하고 싶은 것들이 있다면, volume을 사용해야한다. /var/lib/docker/volumes를 만드록 docker run -v \u0026lt;volume\u0026gt;:\u0026lt;path\u0026gt;의 형태로 사용한다.  absolue path를 통해서도 mount가 가능하다.   strorage driver는 layer를 관리한다. (mount)  Volume Driver Plugins in Docker  volume은 volume drivers가 할당한다. default = local (/var/lib/docker/volumes)  Container Storage Interface (CSI)  쿠버네티스에서 스토리지를 확장하는 방법. 쿠버네티스에서만 사용하는 규격이 아니다.  Volumes  파드에 volume을 할당해서 container의 파일을 저장할 수 있다. 원하는 stroage type을 사용해서 특정 스토리지 서비스를 이용할 수 있다.  Persistent Volumes  파드마다 volume을 할당하는 것은 매우 힘든 일이다. 좀 더 나은 방법: Persistent Volumes 사용하기  admin이 pv를 생성하고, pod는 필요시에 이를 가져다가 사용하기 persistent volume claim을 통해 요청한다.    Persistent Volume Claims  admin은 Persistent Volume을 생성하고, user는 Persistent Volume Claim을 생성하여 PV를 사용한다. PVC가 생성되면 쿠버네티스는 Bind 과정을 통해 알맞은 PV를 선택한다. 모든 PVC는 하나의 PV에 할당된다. label기반으로 bind도 할 수 있다. PVC를 했는데 bind될 것이 없으면 Pending 상태가 된다. default로 persistentVolumeReclaimPolicy=Retain이라서 PVC가 삭제되어도 PV의 데이터는 남아있다.  Reuse되지 않는다. Delete로 설정하면 자동으로 삭제된다. Recycle로 설정하면 다시 사용할 수 있지만 권장하지 않는다.    Tips  Storage Class나 StatefulSets에 관한 것은 CKAD의 범위.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/09-networking/",
	"title": "09 Networking",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Switching Routing  Switching  컴퓨터 두대를 스위치를 통해 연결하는 방법. 연결된 스위치의 인터페이스에 ip를 할당한다. 동일한 네트워크로만 트래픽을 보내준다.   Routing  라우터는 스위치로 연결된 두 네트워크 대역을 연결해준다. 각 네트워크 대역에 대해서 아이피를 동시에 가지고 있다.   Gateway  어떤 아이피 대역에 대해서 어떤 라우터를 통해서 가게 할지 결정하는 것 외부 인터넷과 연결하고 싶다면 먼저 인터넷을 라우터에 연결하고, 이 곳으로 route 설정을 하면 된다. default gateway는 Destination=0.0.0.0인 것. gateway가 0.0.0.0이라면 라우터가 필요없다는 의미.   기본적으로 /proc/sys/net/ipv4/ip_forward가 0으로 설정되어 자신에게 오는 트래픽이 아닌 경우 포워딩하지 않는다.  DNS  /etc/hosts에 domain name을 넣을 수 있다. DNS server에 domain name을 정리하고, 각 컴퓨터에서 이를 조회하도록 한다. /etc/resolv.conf에 nameserver를 넣어서 서버에 있는 dns를 사용할 수 있다. domain name lookup할 때 local 파일부터 찾고 그 다음에 nameserver에서 검색한다. DNS server에 Forward All to 8.8.8.8로 public internet의 DNS를 default로 참조하도록 할 수 있다. apps.google.com으로 조회를 하면 root DNS, .com DNS, Google DNS의 순서로 조회하여 IP를 가져오고, 이를 캐싱하게 된다. /etc/resolv.conf에 search를 mycompany.com으로 등록하게 되면 ping web을 입력했을 때 web.mycompany.com으로 검색한다. Record Types  A: IPv4 AAAA: IPv6 CNAME: name alias (name to name mapping)    Network Namespaces  쿠버네티스는 네임스페이스를 기준으로 나누어져 있다. 네임스페이스 안에서는 서로의 프로세스가 무엇인지 볼 수 있다. 컨테이너에서 실행한 것은 호스트에서 프로세스 확인이 가능하다.  process ID는 컨테이너 내부와 바깥이 서로 다르다.   컨테이너가 생성되면 네트워크 네임스페이스를 호스트에서 생성한다.  컨테이너는 그 안에서 Routing Table과 ARP Table을 가진다.   리눅스 호스트에서 network namespace를 생성하려면 ip netns add red와 같이 입력하면 된다.  ip netns로 리스트 확인 가능.   ip link를 입력하면 interface를 볼 수 있는데 이를 네임스페이스 아래에서 실행하고자 한다면 다음과 같이 하면 된다.  ip netns exec red ip link 또는 ip -n red link   나머지도 비슷하다.  ip netns exec red arp ip netns exec red route   네트워크 네임스페이스 두개를 연결하려면 파이프를 연결하면 된다(가상의 랜선이라고 생각하면 된다.)  ip link add veth-red type veth peer name veth-blue ip link set veth-red netns red ip link set veth-blue netns blue ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue ip -n red link set veth-red up ip -n blue link set veth-blue up ip netns exec red ping 182.168.15.2   virtual switch를 통해 네트워크 네임스페이스를 연결할 수 있다.  linux bridge, open vswitch가 있다. ip link add v-net-0 type bridge ip link set dev v-net-0 up ip -n red link del veth-red \u0026lt;- 위에서 생성한 연결을 끊는 것. ip link add veth-red type veth peer name veth-red-br ip link add veth-blue type veth peer name veth-blue -br ip link set veth-red netns red ip link set veth-red-br master v-net-0 ip link set veth-blue netns blue ip link set veth-blue-br master v-net-0 ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue ip -n red link set veth-red up ip -n blued link set veth-blue up 이를 호스트의 네트워크와 연결할 수 있다.  virtual switch(bridge)는 호스트의 자원. ip addr add 192.168.15.5/24 dev v-net-0을 추가하면 연결이 가능   외부 인터넷과 연결하려면 호스트의 eth0와 연결을 해야한다. gateway를 추가. ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5  이 상태에서는 ping이 되지 않음. private ip를 달고 나가기 때문에 외부에서는 인식하지 못하는 IP. 따라서 NAT를 해야한다. iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE     외부에서 network namespace로 들어오려면?  2가지 해법이 있음.  직접 ip 할당하고 라우팅하기 iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT      Docker Networking  docker run에서 --network none을 하면 컨테이너는 어떤 곳과도 통신할 수 없다. --network host를 사용하여 port를 열면 host의 port가 열린다. --network bridge는 default로 도커 컨테이너끼리 통신이 가능하다. docker는 실행시 default로 brdige를 생성한다. docker network ls를 통해 확인 가능.  ip link를 통해 docker0를 볼 수 있다.  ip link add docker0 type bridge를 통해 생성한 것과 동일.   ip addr을 통해 보면 ip를 확인할 수 있다. (brdige의 ip)   docker run을 하면 도커는 네트워크 네임스페이스를 생성한다.  ip netns를 보면 확인이 가능하다. docker inspect \u0026lt;container id\u0026gt;를 통해서 어떤 네트워크 네임스페이스인지 확인이 가능하다.   host에서 ip link를 하면 네임스페이스와 bridge(docker0)를 연결한 것이 보인다.  ip -n \u0026lt;network namespace\u0026gt; link를 하면 컨테이너에서 brdige에 연결한 것이 보인다. ip -n \u0026lt;network namespace\u0026gt; addr을 하면 컨테이너의 아이피를 확인할 수 있다.   컨테이너의 port를 호스트의 port와 연결하려면 -p 8080:80과 같은 옵션을 사용하면 된다.  이는 nat를 사용하여 할 수 있다. ip tables -t nat -A DOCKER -j DNAT --dport 8080 --to-destination 172.17.0.3:80 iptables -nvL -t nat을 통해 확인 가능.    CNI  bridge program은 특정한 task를 쭉 실행해준다. bridge add \u0026lt;cid\u0026gt; \u0026lt;namespace\u0026gt;를 통해 할 수 있다.  쿠버네티스에서도 이 방식을 사용한다.   container runtime간에 제대로 동작을 하는지 확인하기 위한 표준이 생겼다.  bridge도 CNI의 plugin 중 하나이다.   CNI 표준  Container Runtime은 반드시 network namespace를 생성해야한다. container가 연결되어야 하는 network를 확인해야한다. container runtime은 컨테이너가 생성될 때 network plugin(brdige)를 호출해야한다. container runtime은 컨테이너가 삭제될 때 network plugin(brdige)를 호출해야한다. JSON 형식으로 network configuration을 할 수 있어야 한다.   Plugin 표준  add/del/check에 대한 command line arguments를 제공해야한다. contianer id, network 등에 대한 파라미터를 제공해야한다. 파드에 할당되는 IP를 관리해야한다. 특정한 형식으로 결과를 리턴해야한다.   Plugin에는 다음과 같은 것들이 있다.  bridge vlan ipvlan macvlan windows dhcp host-local   그러나 docker는 CNI 표준을 지키지 않는다.  자신만의 독자적인 Container Network Model을 사용한다. 따라서 docker run --network=cni-brdige nginx와 같이 사용할 수는 없다. 하지만 그렇다고 해서 docker에서 아주 못사용하는 것은 아니다.  docker run --network=none nginx처럼 우선 생성한 뒤 수동으로 plugin을 실행하면 된다. 이 방식을 쿠버네티스에서 사용한다.      Cluster Netwrking  쿠버네티스는 마스터, 워커 노드가 있고, 이들이 네트워크로 연결이 되어야 한다. 이들은 서로 hostname과 MAC address가 달라야 한다. 마스터 노드는 특정한 포트를 열어야 한다.  kube-apiserver에서 사용하기 위한 6443 포트 kube-scheduler에서 사용하기 위한 10251 포트 kube-controller-manager에서 사용하기 위한 10252 포트 ETCD에서 사용하기 위한 2379, multi-master일 경우 2380도 열어야 한다.   또한 워커 노드는 kubelet을 위한 10250 포트를 열어야 한다.  그리고 service expose를 위한 30000-32767까지의 포트를 열 수 있어야 한다.    Pod Networking  pod layer에서의 네트워킹이 중요하다. 파드는 수백개가 생성이 될 수 있는데 이들끼리 서로 통신이 되어야 하고, 아이피가 할당되어야 한다. 쿠버네티스는 built-in solution으로 이를 해결하지 않는다. 다음이 구현되어야 한다.  모든 파드는 IP address를 가지고 있다. 모든 파드는 같은 노드 안에서의 파드와 통신할 수 있어야 한다. 모든 파드는 다른 노드에 있는 파드와 NAT가 아닌 방법으로 통신할 수 있어야 한다.   이를 구현한 네트워킹 모델들이 있다. (flannel, calico 등) 강의에서는 이전에 배운 네트워크 네임스페이스를 기반으로 어떻게 동작하는지를 알아볼 것이다.  각 노드들은 192.168.1.0 대역을 가지고 있다고 가정한다. 각각의 노드에 대해서 bridge를 생성한다. 그 다음 bridge를 up 한다. 각 노드에 대해서 10.244.1.0/24, 10.244.2.0/24, 10.244.3.0/24의 네트워크 대역을 가지도록 한다. 그 뒤 컨테이너가 생성될때마다 특정 스크립트를 생성해서 아이피를 할당하고 노드 내부 bridge와 연결한다. 그 다음 각 노드의 파드끼리 route 설정을 넣어준다.   이런 것들을 CNI를 통해서 해결한다. kubelet은 각 노드에 있는 컨테이너의 생성/삭제를 관리한다.  따라서 컨테이너가 생성될 때 kubelet을 실행할 때 사용된 arguments에 있는 CNI를 확인하고, script를 실행한다.    CNI in kubernetes  CNI plugin은 각 kubelet을 실행할 대 arguments로 등록할 수 있다. --cni-bin-dir에서 설정한 디렉토리에는 CNI의 binary 파일들이 있다. --cni-conf에는 어떤 CNI를 사용할지 알파벳 순서로 골라서 사용한다.  CNI weave  파드에서 파드로 가는 트래픽은 노드 바깥의 router로 나간 뒤, 알맞은 노드로 들어가서 파드에 전달된다.  그러나 이는 수백개의 노드가 각각 수백개의 파드를 가지고 있는 상황에서는 제대로 작동할 수 없다. 라우팅 테이블이 모든것을 담을 수 없다.   weave는 각각의 노드에 agent를 두고, 서로 통신하게 하여 서로의 위치를 확인한다.  agent는 다른 노드의 파드로 가는 트래픽을 가로채서 어디로 보낼지 확인한 뒤 트래픽을 감싼다. 해당 트래픽은 다른 노드로 전달된 뒤에 agent가 다시 가로채서 트래픽을 확인하고 원하는 파드로 전달한다.    IP Address Management - Weave  어떻게 노드의 Virtual Brdige가 IP를 할당하는지에 대해 알아볼 것이다. network plugin에 의존하여 CNI는 IP를 할당한다.  쿠버네티스는 할당한 IP의 관리에는 영향을 주지 않는다.   /etc/cni/net.d/net-script.conf에는 사용하는 CNI에 대한 정보가 적혀있다(IPAM). weaveworks는 10.32.0.0/12로 IP 대역을 파드에 할당하려 하고 각 노드에 10.32.0.1, 10.38.0.1, 10.44.0.0으로 IP 대역을 나누어 노드에 할당한다. customization이 가능하다.  Service Networking  서비스가 생성되었을 때에도 노드의 위치에 상관없이 모든 파드에서 접근이 가능해야한다. 서비스는 특정 노드에 bound되는 것이 아니라 노드 전역에 걸쳐 생성된다. ClusterIP는 클러스터 내부에서만 연결이 된다.  DB처럼 클러스터 내부에서만 통신하려는 목적이면 ClusterIP를 사용하면 된다.   외부로 노출하기 위해서는 NodePort를 사용한다.  IP를 가지는것 뿐만 아니라 노드에서 포트를 할당받기도 한다.   service는 단순히 virtual object로 어딘가에 생성되는 것이 아니다.  쿠버네티스에 serivce를 생성하면 미리 설정된 범위에서 IP 주소를 할당받는다. 할당받은 IP 주소로 forwarding rule이 생성된다. 단순히 IP로 forwarding하지 않고 IP:port로 forwarding한다   userspace, iptables, ipvs 모드로 kube-proxy가 설정될 수 있으며 기본은 iptables이다. service의 ip range는 kube-api-server에서 default로 10.0.0.0/24로 설정되어있다. pod ip range와 service ip range는 겹치면 안된다. kube-proxy의 로그를 보면 어떻게 iptables를 조작했는지 볼 수 있다.  Cluster DNS  쿠버네티스는 설치시 자신만의 DNS 서버를 가진다. 서비스가 생성이 될때, hostname과 IP 주스를 해당 서비스에 맞게 등록한다. 파드 내에서는 해당 domain을 통해 접근할 수 있다. 동일한 네임스페이스에서는 service name만 가지고 접근할 수 있다. 다른 네임스페이스에 있는 경우 \u0026lt;service name\u0026gt;.\u0026lt;namespace\u0026gt;로 접근해야한다. apps namespace 안의 web-service가 있는 경우  hostname: web-service namespace: apps type: svc root: cluster.local   pod에 대해서도 dns에 등록이 된다. 그러나 IP 주소를 10-244-2-5와 같이 dash를 통해 구분할 뿐이다.  type: pod    CoreDNS in Kubernetes  여러 서비스 및 파드가 생성/삭제될 때 domain을 정리하기 위해 중앙의 DNS 서버를 사용한다. 파드 내에서는 /etc/resolv.conf에서 nameserver로 등록한다. 새로운 파드가 생기면 DNS 서버에 엔트리를 추가한다. 1.12 이전에는 kube-dns를 사용했지만 이후로는 CoreDNS를 권장한다. CoreDNS는 kube-system namespace에 파드로 배포되어있다.  Deployment로 두개가 동작중이다.   CoreDNS는 Corefile이라는 파일을 이용한다.  여기에 kubernetes라는 plugin 구역을 보면 된다. root domain이 적혀있다. pods insecure 부분은 파드에 대한 record를 관리할지를 결정한다.  default로 disable이다.   proxy 부분은 dns에 등록되지 않은 것들을 /etc/resolv.conf에서 검색하도록 설정되어있다. 이 파일은 configmap으로 전달된다.   파드에서 DNS 서버에 접근할 떄는 service사용한다.  기본적으로 CoreDNS도 kube-dns 이름으로 사용한다. 이는 파드가 생성될 때 자동으로 IP를 기입한다. (/etc/resolv.conf) kubelet이 설정한다.  option으로 설정되어있음.   /etc/resolv.conf에 search 항목으로 default.svc.cluster.local, svc.cluster.local, cluster.local이 설정되어 있어서 간단하게 이름만 입력해도 근이 가능한 것이다.  그러나 pod는 정의되어있지 않아서 FQDN으로 접근해야한다.      Ingress  service와 ingress의 차이점은 무엇인가? 일반적으로 NodePort로 expose하면 30000 이상의 port(ex: 38080)를 사용하게 된다.  유저가 80포트를 통해 접속하도록(http) 하려면 proxy server를 둬서 트래픽을 80에서 38080으로 전달해야 한다. LoadBalancer를 사용할 때도 이와 비슷하다.   Ingress는 유저가 하나의 external accessble URL을 통해 클러스터내의 서비스에 접속할 수 있게 해준다.  https설정도 할 수 있다. ingress 또한 expose해야한다. NodePort 또는 LoadBalancer. 그러나 서비스가 늘어나도 한번만 수행하면 된다.   ingress controller, ingress resources로 ingress 설정을 한다.  default로 설치되어있지 않음. GCE와 Nginx는 쿠버네티스 프로젝트에서 관리한다.   nginx는 configmap으로 nginx의 설정을 한다.  ServiceAccount를 통해 적절한 권한을 줘야 한다.   ingress resource는 url에 따라 다른 파드로 트래픽을 전달할 수 있다. ingress에 backend로 serviceName과 servicePort를 지정하여 트래픽을 전달할 수 있다.  해당 ingress로 온 트래픽은 모두 serviceName으로 전달된다.   각각의 rule을 지정하여 원하는 서비스에 트래픽을 전달할 수 있다.  host를 따로 지정하지 않으면 모든 트래픽에 대해 매칭이 된다.    "
},
{
	"uri": "http://kimmj.github.io/harbor/what-is-harbor/",
	"title": "Private Docker Registry 오픈소스: Harbor란",
	"tags": ["harbor", "docker-registry"],
	"description": "",
	"content": "Harbor란 Harbor 는 올해(20년) 중순 쯤 CNCF 의 graduated된 오픈소스 프로젝트로 Docker Hub 처럼 이미지를 저장할 수 있는 저장소이다. Docker 는 이미 docker registry 라는 이름으로 개인 이미지 저장소를 컨테이너화 하였었다. Harbor 는 내부적으로 이 docker registry 를 사용하고 있으며, 여기에 RBAC, web page, image scan 같은 편리한 기능을 추가하였다. 외부 서비스를 이용하는 것이 아니라 내부에 이미지 파일을 저장하기 때문에 사내 보안 정책에도 알맞게 사용할 수 있다. 아니면 필요에 따라 S3나 Minio를 사용할 수 있을듯 하다.\n주요 기능 사내에서 사용하며 편리하다고 생각되었던 몇가지 기능들을 소개한다.\n 이미지 복제 (Harbor 외부 \u0026lt;-\u0026gt; Harbor 내부) 이미지 주기적 삭제 (tag retention) Project 별 Disk quota 기능 Project 별 Private/Public 기능 Webhook 기능 (v1.10 이상에서 가능) OCI Support (v2.0 이상에서 가능) docker registry 의 API 사용 가능 (인증 관련해서만 코딩에 주의) Swagger 를 통한 API Docs 제공  개인적인 생각 다른 docker registry 서비스들(예를 들면 Artifactory 라던지, AWS ECR 이라던지..)을 사용해보진 않았지만, 잘 만들어진 프로젝트라고 생각된다. 업데이트도 자주 이루어 지고 있는 중이고 이슈도 등록하면 금방 확인해주는 듯 하다. 특히나 Graduated 된 상태이니 믿고 사용해도 좋을 정도라고 생각된다. 나는 사내에서 용도별로 여러개의 Harbor 를 설치하고 운용중인데, 설치 과정이 매우 쉬워서 새롭게 시작하려는 사람도 테스트를 해보기 용이할 듯 하다.\ndocker registry 를 사용하다보면 당연스럽게도 아쉬운 부분이 GUI에 관한 부분인데 Harbor 는 GUI로 많은 것들을 할 수 있어서 정말 편리하다. 이미지간 복제도 GUI로 할 수 있었고(나의 회사에서는 VPN으로 연결된 구간이 있었는데, Harbor 가 컨테이너 기반이라서 그런지 VPN을 타고 트래픽이 나가지 못했다. 그래서 결국은 그 구간만 스크립트를 짜서 이미지 복제를 하였다), 기본적인 이미지 삭제나 특정 기간을 지난 이미지 삭제 또는 가장 최근에 Pull된 N개의 이미지만 빼고 삭제하기 등 운용하기에 편리한 기능들도 있다.\n내 생각엔 일단, 한 번 써보면 좋을 프로젝트인 듯 하다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-epilogue/",
	"title": "CKA: Certified Kubernetes Administrator 취득 후기",
	"tags": ["CKA", "Certified-Kubernetes-Administrator"],
	"description": "",
	"content": "작년부터 미뤄왔던 CKA: Certified Kubernetes Administrator 취득을 드디어 하게 되었습니다. 이제까지 제가 했던 공부들을 공유하며 이 시험을 보려는 사람들에게 유용한 정보를 주기 위해 이 포스트를 작성합니다.\n저는 다음과 같이 CKA를 취득하였습니다.\n 점수: 97점 (커트라인: 74점) 유효기간: 2020.07 ~ 2023.07 (3년)  CKA는 CNCF를 리딩하는 Linux Foundation에서 주관하는 시험입니다. 가격은 정가 $300으로 저렴하지는 않은 편입니다. 그러나 할인행사를 많이 하므로 급하지 않다면 시간을 가지고 천천히 결제하는게 좋습니다. 결제 후 1년까지 총 두번의 시험을 치룰 수 있습니다. 두번의 기회 안에만 합격하면 CKA를 취득할 수 있습니다.\n시험 환경 시험은 온라인으로 치루게 됩니다. 해당 시험을 위해서 Chrome Extension을 설치해야 합니다. 이외에도 Requirements가 몇가지 더 있으며 이는 시험을 보기 전에 사이트에서 제공하는 툴로 확인이 가능합니다. 시험 종류중에는 중국/일본어로 진행되는 시험도 있지만 아직 한글은 지원하지 않습니다. 그래서 저도 영어로 지원하였고, 이에따라 감독관도 영어권 사람으로 배정되었습니다. 시험을 위해 마이크 및 웹캠이 필요합니다. 웹캠은 주변 환경을 찍어서 보여줘야 하기 때문에 자유롭게 움직일 수 있어야 합니다. 감독관과는 보통 영어로 대화하지 않고 시험 페이지에서 제공하는 메신저를 통해 텍스트로 대화합니다. 시험 스케줄은 KST로 변환하여 선호하는 시간을 선택하면, 가능한 일정을 알려줍니다. 저의 경우 밤에 집중이 잘 되어 저녁 10시에 응시하였습니다. 취소 및 변경은 응시 하루전까지 자유롭게 가능하고, 24시간 이후가 되었을 경우 패널티를 받게 됩니다. 시험 시간은 총 3시간으로 그 시간 안에 24문제를 풀면 됩니다.\n시험의 난이도는 그렇게 어려운 편은 아닙니다. 물론 저의 경우 약 1년간 쿠버네티스 환경을 익혀왔고, 강의도 학습하였지만 객관적으로 보았을 때에도 시험 난이도가 높지는 않습니다. 가격때문에, 떨어질까봐 걱정되어 망설여지신다면 자신있게 응시해도 좋을 것 같습니다. 어차피 일정 변경이 결제 후 1년간 자유롭기 때문에 막상 시험이 다가왔을 때 공부가 좀 더 필요할 것 같다고 생각되면 취소 후 다시 응시하면 됩니다. 게다가 1개의 추가적인 탭을 사용할 수 있는데, 이 탭으로 kubernetes.io 및 github.com/kubernetes에 접속이 가능합니다. 저의 경우 확실하게 하기 위해 모든 문제를 kubernetes.io에서 검색하여 풀었고, 이렇게 풀어도 시간이 1시간가량 남았습니다.\n준비했던 것들 공식 Docs 먼저 공식 Docs를 보시면 됩니다. 가장 기본이 되는 문서로, 필요한 내용은 전부 Docs에 들어가 있습니다. 내용을 외운다기 보다는 어떤 기능이 있는지 이해하는 방식으로 넘어가면 좋을 것 같습니다. 저의 경우 Concepts만 읽어보았으면 Tasks에 있는 내용은 거의 보지 못했습니다.\nkubernetes the hard way 두번째로 kubernetes-the-hard-way입니다. GCP에 쿠버네티스를 kubeadm이 아닌 직접 프로세스를 실행시키는 방식으로 한땀한땀 구성하는게 특징인 프로젝트입니다. 처음에 개념이 잡히지 않은 상태에서는 해도 단순한 복붙작업이 될 가능성이 높습니다. 따라서 어느정도 공부를 하고 난 뒤 심화학습을 한다는 생각으로 해보시면 좋을 것 같습니다.\nUdemy CKA 강의 세번째로 Udemy에서 하는 CKA 강의입니다. 이 강의에서는 Katacoda를 활용하여 직접 실습도 해볼 수 있어서 배웠던 것을 복습하는 데 매우 좋습니다. 또한 강의 말미에는 Mock Exam이 3개 포함되어 있는데 실제 시험과 난이도가 매우 비슷하고 문제도 크게 다르지 않아서 시험을 준비하는데 많은 도움이 되었습니다. 가격도 계속 2만원 선으로 되어있는 것 같으니 그렇게 큰 부담도 되지 않을 것 같습니다. 저의 경우 회사에서 Udemy 서비스를 지원해주어 이를 통해 강의를 들었습니다. 강의 내용이 평소 혼자 공부할 때에는 넘어갈 수 있는 부분들이 많이 포함되어 있어 부족한 부분을 채우는 데 매우 좋았습니다.\n저는 이렇게 3가지만 공부하고 시험을 치뤘습니다. 심화된 내용을 모두 공부하는 것도 좋겠지만 이정도로만 해도 CKA를 취득하는 데에는 무리가 없습니다.\n공부해야 할 것들  kubectl create 명령어 및 kubectl run(파드 실행) 명령어에 익숙해지기 --dry-run -o yaml 옵션을 통해 대강의 뼈대를 구성하고 kubernetes.io를 참조하며 수정하기 journalctl -u kubelet -f로 kubelet의 로그 확인하기 service kubelet status로 kubelet의 상태 확인하기 static pod를 구성하는 방법 알아보기 (kubelet에서 static pod 위치 지정하는 것, 실제로 static pod를 생성해보는 것) etcdctl을 통해 ETCD를 백업하고 복구하기 kubeadm을 통해 클러스터 구성하고, 업그레이드 하기 kubeadm을 통해 새로운 노드를 워커노드로 붙이기 pv, pvc를 통해 파드에 볼륨 추가하기 taint, toleration, cordon, drain, nodeSelector, affinity를 사용하여 파드가 뜨는 위치를 원하는 대로 조정하기  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/10-kubernetes-the-hard-way/",
	"title": "10 Kubernetes the Hard Way",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Design a Kubernetes Cluster  Purpose  Education  Minikube Single node cluster with kubeadm/GCP/AWS   Development \u0026amp; Testing  Multi-node cluster with a Single Master and Multiple workers Setup using kubeadm tool or quick provision on GCP or AWS or AKS   Hosting Production Applications  HA Multi node cluster with multiple master nodes 최대 5000노드 클러스터 내에 최대 약 150000 파드 최대 300000개의 컨테이너 노드당 최대 약 100개의 파드     Cloud or OnPrem  on-prem에는 kubeadm을 사용하자. GKE for GCP Kops for AWS AKS for Azure   Storage  High Performance - SSD 사용 Multiple Concurrent connections - Network based storage Persistent shared volumes for shared access across multiple PODs Node를 disk type에 따라 label을 주입하기 Node selector를 사용하여 특정 disk type을 사용하는 노드로 application 할당하기   Nodes  Virtual or Physical Machines 최소 4개의 노드 Master vs Worker Nodes  Master node도 workload를 호스트할 수 있다.   Linux X86_64 Architecture   Master Nodes  대규모의 Cluster에서는 Master Nodes와 ETCD cluster를 분리해야한다.    Choosing Kubernetes Infrastructure  Windows로 호스팅을 하려면 바이너리 파일이 따로 없기 때문에 VM을 이용해서 리눅스를 띄워야 한다. 간단하게 로컬로 클러스터를 만들고자 한다면 Minikube를 사용할 수 있다.  Single Node   kubeadm으로도 가능하다.  Single/Multi Node 가능   Turnkey Solutions  VM을 Provision 해야함. VM을 Configure 해야함. Script를 통해 Cluster를 배포함. 스스로 VM을 관리해야함. AWS에서의 KOPS   Hosted Solutions  Kubernetes-As-A-Service Provider가 VM을 Provision함. Provider가 Kubernetes를 설치함. Provider가 VM을 관리함. GKE   OpenShift는 on-prem kubernetes platform중 하나이다.  쿠버네티스 위에 설치되어있는 서비스. 쿠버네티스 관리를 위한 추가적인 툴과 GUI를 제공함. CI/CD 파이프라인 제공   Cloud Foundry Container Runtime은 BOSH라는 오픈소스를 통해 HA cluster를 관리함.  Configure High Availability  Master Node가 삭제되면 어떡하나?  일단 계속 동작은 함. 그러다가 파드가 크래시나기 시작하면 문제가 발생. 파드를 재시작해줄 수 없음.   따라서 마스터노드를 여러대 두어야 한다. Signle Point of Failure를 없애기 위함. 마스터노드를 두개 두려면 컴포넌트들 역시 둘 다 배포되어야 한다.  ETCD, API Server, Controller Manager, Scheduler   API Server는 Active, Active 모드로 떠있을 수 있다.  기존에는 API server의 6443 포트로 kubectl 이 접근했었다. 이를 LoadBalancer를 통해서 트래픽을 분기해주는 것이 필요하다. NGINX, HA proxy등이 이용가능하다.   Scheduler, Controller Manager의 경우  클러스터의 상태를 보고 행동을 취하는 것들이다. 계속해서 파드의 상태를 체크한다. 동시에 동작한다면 실제로 생성해야 하는 파드보다 더 많은 파드를 생성해버릴 수 있다. 따라서 Active/Standby Mode로 동작해야한다. leader election process가 필요하다. kube-controller-manager 에서 --leader-elect 를 true 로 설정해야한다. 이들은 kube-controller-manager endpoint 에 lock을 걸로 lease를 한다. 둘다 2초마다 leader가 되고자 한다. 하나가 down되면 두번째 것이 up이 된다.   ETCD는 쿠버네티스 마스터 노드 안에 위치할 수 있고 이는 Stacked Topology라고 한다.  이렇게 구성했을 때에는 하나가 죽었을 때 Redundancy가 제대로 되지 않는다. 다른 방법은 ETCD를 Control plan node와 분리시키는 것이다.   ETCD를 다른 노드에 두는 것을 External ETCD Topology라고 한다.  이는 control plane node가 죽더라도 ETCD cluster와 데이터에 영향을 주지 않아 덜 위험하다. 외부 노드가 필요하기 때문에 서버가 더 필요하고 셋업이 더 어렵다. API Server가 유일하게 ETCD를 바라보고 있고, API Server에 ETCD의 주소를 적을 수 있기 때문에 API Server가 제대로 바라보도록 설정해야 한다.    ETCD in HA  ETCD는 distributed reliable key-value store이다. distributed라는 의미는 동일한 copy를 가진 database를 가지고 있다는 것을 의미한다.  ETCD는 어떤 copy도 동시에 접근이 가능함을 보장한다.   READ는 문제가 없지만 WRITE는 상황이 다르다. ETCD는 하나의 instance만 write를 처리할 수 있다.  내부적으로 Leader/Follower를 정의한다. Follower로가 write 요청을 받으면 이를 Leader로 포워딩하여 WRITE를 처리한다.   Leader Election은 RAFT 알고리즘을 통해서 한다. Quorum 이상의 숫자만큼 write가 되면 정상적으로 쓰기가 되었다고 판단한다.  N/2 + 1 따라서 2개의 인스턴스만 있으면 fault tolerance가 제대로 보장되지 않는다. 짝수로 클러스터를 구성하면 네트워크가 나뉘었을 때 클러스터가 망가질 수 있다. 따라서 홀수가 추천되고 5개가 가장 적절하다.    TLS Bootstrap worker node  kubelet의 client는 kube-apiserver이다. Bootstrap Token을 생성하고 이를 group system:bootstrappers에 넣는다. system:node-bootstraper를 system:bootstrappers 그룹에 할당한다. system:certificates.k8s.io:certificatesigningrequests:nodeclient를 system:bootstrappers 그룹에 넣는다. systemLcertificates.k8s.io:certificatessiningrequests:selfnodeclient를 system:nodes에 넣는다. kube documentation에 있다. 이렇게 하면 워커는 자동으로 TLS certificates를 생성하고 expire 시 renew가 가능하다. kubelet에 --rotate-certificates=true를 넣으면 된다. server certificates는 보안상의 이유로 수동으로 approve해야 한다.  "
},
{
	"uri": "http://kimmj.github.io/jenkins/workspace-list/",
	"title": "Workspace@2를 변경하기 - Workspace List 설정 변경",
	"tags": ["jenkins", "workspace"],
	"description": "",
	"content": "운용하는 노드에 executor가 2개 이상이라면, concurrent build 옵션을 disable했다고 하더라도 zombie process가 있을 경우 workspace@2처럼 @ 캐릭터가 들어간 workspace를 사용할 수 있다.\nworkspace 안에 특정한 파일을 넣고 사용하는 경우라면 @2가 생기면 안될 것이다. 그러나 이는 그렇게 좋은 방법은 아닌것 같으며 이런 경우에는 github 등에 스크립트같은 파일을 옮겨놓고 git pull이나 git 관련 플러그인을 통해 다운로드 한 뒤 사용하는 것이 더 좋은 방법인 것 같다.\n하지만 나의 경우 Perforce를 사용하고 있었느데 p4 sync에서 문제가 발생했다. @라는 문자가 없어야 한다는 것이었다. 사실 저 문자만 없다면, workspace의 폴더 이름이 무엇이든 상관이 없었기 때문에 @대신 다른 캐릭터를 사용하기로 결정했다.\nJAVA_OPTS= '-Dhudson.slaves.WorkspaceList=A'를 추가하면 된다. 내 경우 docker-compose를 통해 jenkins를 설치했으므로 다음과 같이 작성했다.\nenvironments: JAVA_OPTS= \u0026#39;-Dhudson.slaves.WorkspaceList=_\u0026#39; 파일에서 envieonments의 정확한 위치는 때에 따라 다를 수 있으므로 검색을 통해 environments의 정확한 위치를 확인해보고 설정하면 될 것 이다. 위와 같이 설정하면 두번째 workspace는 workspace_2와 같이 생길 것이다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/port-targetport-nodeport-in-kubernetes/",
	"title": "[번역] 쿠버네티스에서의 Port, TargetPort, NodePort",
	"tags": ["kubernetes", "service", "targetport"],
	"description": "",
	"content": "원문: https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ports-targetport-nodeport-service.html\n쿠버네티스의 port declaration 필드에는 여러가지가 있다. 각 type에 대해 빠르게 살펴보고 YAML에서 각각 어떤 의미를 가지고 있는지 알아보도록 하자.\nPod ports list pod.spec.containers[].ports로 정의된 이 배열은 container가 노출하고 있는 포트의 리스트를 나타낸다. 이 리스트를 꼭 작성해야할 필요는 없다. 리스트가 비어있다고 하더라도 container가 포트를 listening하고 있는 한 여전히 네트워크 접속이 가능하다. 이는 단순히 쿠버네티스에게 추가적인 정보를 줄 뿐이다.\n List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \u0026ldquo;0.0.0.0\u0026rdquo; address inside a container will be accessible from the network. Cannot be updated. - Kubernets API Docs\n Service ports list 서비스의 service.spec.ports 리스트는 서비스 포트로 요청받은 것을 파드의 어느 포트로 포워딩할 지 설정하는 것이다. 클러스터 외부에서 노드의 IP 주소와 서비스의 nodePort로 요청이 되면 서비스의 port로 포워딩되고, 파드에서 targetPor로 들어온다.\n nodePort 이는 서비스가 쿠버네티스 클러스터 외부에서 노드의 IP 주소와 이 속성에 정의된 포트로 보일수 있도록 한다. 이 때, 서비스는 type: NodePort로 지정해야 한다 이 필드는 정의되어 있지 않을 경우 쿠버네티스가 자동으로 할당한다.\nport 서비스를 클러스터 안에서 지정된 포트를 통해 내부적으로 노출한다. 즉, 서비스는 이 포트에 대해서 보일 수 있게 되며 이 포트로 보내진 요청은 서비스에 의해 선택된 파드로 전달된다.\ntargetPort 이 포트는 파드로 전달되는 요청이 도달하는 포트이다. 서비스가 동작하기 위해서는 어플리케이션이 이 포트에 대해 네트워크 요청을 listening을 하고 있어야 한다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/services/",
	"title": "Services",
	"tags": ["service", "kubernetes"],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/docker/insecure-registry/",
	"title": "http를 사용하는 docker registry를 위한 insecure registry 설정",
	"tags": ["docker", "insecure-registry", "docker-registry"],
	"description": "",
	"content": "회사같은 곳에서는 보안상의 문제 때문에 Dockerhub에다가 이미지를 올리지 못하는 경우가 많습니다. 이를 위해서 docker에서도 docker registry라는 툴을 제공하는데요, 이는 자신의 local server를 구축하고, dockerhub처럼 이미지를 올릴 수 있는 툴입니다.\n이러한 docker registry는 사용자의 환경에 따라 http를 사용하는 경우가 있습니다. 이 때, docker는 default로 https 통신을 하려 하기 때문에 문제가 발생합니다. 이 경우 다음과 같이 조치를 하면 http 통신을 할 수 있습니다.\n절차 insecure-registry 설정 /etc/docker/daemon.json 파일을 열어 예시처럼 작성합니다. 없을 경우 생성하면 됩니다.\n{ \u0026#34;insecure-registries\u0026#34; : [\u0026#34;docker-registry:5000\u0026#34;] } docker 재시작 # flush changes sudo systemctl daemon-reload # restart docker sudo systemctl restart docker  이제 다시한번 docker pull 명령어를 통해 이미지가 제대로 다운로드 되는지 확인합니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/customize-login-message/",
	"title": "Ubuntu의 Login Message 수정하기",
	"tags": ["ubuntu", "login-message", "motd"],
	"description": "",
	"content": "TLDR   Expand me...    This package seeks to make the /etc/motd (Message of the Day) more dynamic and valuable, by providing a simple, clean framework for defining scripts whose output will regularly be written to /etc/motd.\n Ubuntu에서는 /etc/update-motd.d 안에 있는 파일들을 확인하여 console, ssh 등 어떤 방법으로든 로그인했을 때 메시지를 띄워줍니다. 여기서 파일들을 사전순으로 로딩하게 됩니다.\n따라서 해당 폴더에 적절한 파일들을 생성하게 된다면 로그인 시 출력되는 메시지를 조작할 수 있습니다.\n  적용법 /etc/update-motd.d/로 이동 cd `/etc/update-motd.d` 99-message 파일 생성 99-message 이름으로 파일을 생성합니다.\n#!/bin/sh  printf \u0026#34;hello. this is customized message.\\n\u0026#34; printf \u0026#34;\\n\u0026#34; 그다음 실행파일로 변경해줍니다.\nchmod +x 99-message 다시 세션 로그인하기 hello. this is customized message. Last login: Wed Mar 11 14:19:56 2020 from x.x.x.x wanderlust@wonderland $ "
},
{
	"uri": "http://kimmj.github.io/css/background-img-darken/",
	"title": "background image 어둡게 하기",
	"tags": ["css", "bgimg-darken"],
	"description": "",
	"content": "배경 이미지를 삽입했는데 사진이 너무 밝아 어둡게 필터처리를 넣고 싶은 경우가 있을 수 있습니다. 저의 경우 logo에 제 깃허브 프로필사진을 빼고 노을진 풍경을 넣었는데 사진이 너무 밝아 부자연스러운 느낌이 들었습니다.\n이 때 검색 후 다음과 같이 조치를 하여 어두워지는 효과를 줄 수 있었습니다.\n#sidebar #header-wrapper { background-image: linear-gradient( rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3) ),url(../images/sunset_on_ibiza.jpg); } linear-gradient 속성은 선형으로 gradient를 적용하는 속성인데 이를 활용하여 검정색 필터를 넣었습니다. 필수 파라미터로 두개의 색깔이 필요하고, 이를 각각 검정색(rgb(0, 0, 0))에 투명도를 0.3으로 설정한 rgba(0, 0, 0, 0.3)으로 설정하여 이미지가 전체적으로 어둡게 보이도록 설정한 것입니다.\n여담으로, 파워포인트에서 사진을 어둡게 하여 텍스트를 강조하고자 할 때 많이 이용했던 검정색 반투명 박스를 이용하는 것과 비슷한 방법이라고 느껴지네요.\n"
},
{
	"uri": "http://kimmj.github.io/hugo/google-analytics/",
	"title": "Hugo에 Google Analytics 적용하기",
	"tags": ["hugo", "google-analytics"],
	"description": "",
	"content": "google analytics는 내 블로그를 사용하는 사람들이 얼마나 많은지, 어떤 정보를 보는지 확인할 수 있는 서비스입니다. 무료로 사용할 수 있고, 사용 방법도 어렵지 않기 때문에 search console과 더불어 사용하면 좋은 서비스로 보입니다.\n그래서 저도 제 블로그에 google analytics를 설정할 수 있는지 찾아보던 중 다음과 같은 글을 확인하고, 이를 통해 설정할 수 있었습니다.\nhttps://discourse.gohugo.io/t/implementing-google-analytics-in-hugo/2671/2\n적용 방법 config.toml의 수정 hugo에서는 config 파일을 사용하여 사용자의 설정정보를 보관합니다. 저는 config.toml을 사용하고 있었는데 여기에다가 다음과 같이 추가하면 됩니다.\ngoogleAnalytics = \u0026#34;UA-123-45\u0026#34; 만약 yaml이라면 googleAnalytics: \u0026quot;UA-123-45\u0026quot; 정도로 설정하면 될 것 같습니다.\n여기서 해당 코드를 github에 올릴 때 안전하게 올리고 싶으시다면, git-secret을 참조하시기 바랍니다.\nheader.html에 추가 자신이 사용하는 theme에서 layouts/partials로 이동하면 header.html이라는 파일이 있을 것입니다. 제가 사용중인 Learn 테마에서는 custom-header.html이라는 파일로 사용자가 추가하기를 원하는 내용을 적을 수 있는 파일이 있었습니다. 따라서 저는 그 파일에 다음과 같이 추가해주었습니다.\n{{ template \u0026quot;_internal/google_analytics.html\u0026quot; . }} 또는 async로 이용하길 원한다면 다음과 같이 적으면 됩니다.\n{{ template \u0026quot;_internal/google_analytics.html\u0026quot; . }} 확인 google analytics는 하루에 한번 데이터를 전송합니다. 따라서 하루정도가 지난 후에 google analytics 페이지를 들어가면 통계자료를 열람할 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/git/git-secret/",
	"title": "git-secret을 통한 github 파일 암호화",
	"tags": ["git-secret", "github", "git"],
	"description": "",
	"content": "git을 사용하다 보면 password나 credential같은 정보가 git에 올라가는 경우가 종종 있습니다. aws같은 cloud provider의 crediential을 git에 생각없이 올리고, 이를 다른 해커가 크롤링을 통해 얻어 비트코인을 채굴하는 사례도 있었습니다.\n이처럼 보안이 필요한 파일을 git에 올릴 때, 암호화를 하여 업로드하는 방법이 있습니다.\nhttps://github.com/sobolevn/git-secret\ngit-secret git-secret은 파일에 대한 암화를 지원하기 위해 사용되는 프로그램입니다.\ngit-secret add 명령어를 통해 파일을 암호화하고, git-secret reveal을 통해 복호화합니다. 이 때 gpg를 이용하게 됩니다.\nInstall 사용자 환경에 따라 brew, apt, yum을 통해 설치할 수 있습니다. 또한 make를 통해서 빌드할 수도 있습니다.\n저의 경우 Ubuntu를 사용하고 있고 apt가 있기 때문에 이를 이용하여 설치하였습니다.\napt install git-secret 파일 암호화하기 gpg를 이용하여 키 생성하기 먼저, gpg가 설치되어있는지 확인하고 설치합니다.\n$ gpg -h | grep version License GPLv3+: GNU GPL version 3 or later \u0026lt;https://gnu.org/licenses/gpl.html\u0026gt; # 위와같은 결과가 나오지 않는다면, 설치해야합니다. $ apt install gpg 설치가 되었으면 gpg key를 생성합니다.\n$ gpg --full-generate-key gpg (GnuPG) 2.2.4; Copyright (C) 2017 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) Your selection? 1  RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072) 4096 Requested keysize is 4096 bits Please specify how long the key should be valid. 0 = key does not expire \u0026lt;n\u0026gt; = key expires in n days \u0026lt;n\u0026gt;w = key expires in n weeks \u0026lt;n\u0026gt;m = key expires in n months \u0026lt;n\u0026gt;y = key expires in n years Key is valid for? (0) 0  Key does not expire at all Is this correct? (y/N) y  GnuPG needs to construct a user ID to identify your key. Real name: Ibiza  Email address: my@email.com  Comment: Wanderlust  You selected this USER-ID: \"Ibiza (Wanderlust) \u0026lt;my@email.com\u0026gt;\" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O  We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy.  전부 완료되기까지 꽤 시간이 많이 걸립니다.\ngit-secret을 통한 암호화 본격적으로 시작하기 위해 git-secret을 init합니다.\ngit secret init 그 다음 git-secret에 사용자를 추가합니다.\ngit secret tell my@email.com 이제 secret을 통해 암호화할 파일은 원본이 github에 올라가지 않게 하기 위해 .gitignore에 추가하도록 합니다. 저의 경우 ./config.toml이라는 파일을 암호화하기 위해 다음과 같이 추가하였습니다.\nconfig.toml 그 다음 git-secret이 암호화 하도록 파일을 추가합니다.\ngit secret add config.toml 이 때 다음과 같은 에러가 발생했는데, github issue 중 비슷한 문제가 있어 이를 통해 해결할 수 있었습니다.\n$ git secret add config.toml config.toml is not a file. abort. $ git rm --cached config.toml $ git secret add config.toml 1 items added. 이제 hide를 통해 암호화할 수 있습니다.\ngit secret hide 이후 ls를 통해 확인해보면 .secret이라는 postfix가 들어간 파일을 보실 수 있습니다.\n$ ls config.toml* config.toml config.toml.secret git-secret을 이용한 복호화 reveal을 통해 파일을 복호화할 수 있습니다.\n$ git secret reveal File \u0026#39;/home/wanderlust/Ibiza/config.toml\u0026#39; exists. Overwrite? (y/N) y done. all 1 files are revealed. 이제 .secret 파일을 github에 올리면 안전하게 이용할 수 있습니다.\n다른 컴퓨터에서 접속할 때는 어떻게 해야하나요? gpg 알고리즘은 public key로 암호화하여 private key로 복호화합니다. 즉, 우리는 gpg key를 생성한 host에서 다른 host로 private key를 복사해주어야 합니다.\n따라서 다음과 같이 private key를 복사하고, 이를 import합니다.\n# gpg key를 생성했던 host wanderlust@wonderland $ gpg --export-secret-keys my@email.com \u0026gt; private-key.asc # key 복사 wanderlust@wonderland $ scp private-key.asc wanderlust@wonderland-laptop:~/ # gpg key를 import할 host wnaderlust@wonderland-laptop $ gpg --import ~/private-key.asc 그 다음 tell 옵션을 사용합니다.\ngit secret tell my@email.com 마지막으로 reveal 옵션을 통해 복호화합니다.\n$ git secret reveal done. all 1 files are revealed. "
},
{
	"uri": "http://kimmj.github.io/harbor/install/",
	"title": "Harbor 설치",
	"tags": ["harbor", "docker-registry"],
	"description": "",
	"content": "docker-compose 설치 $ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose # 설치 후 docker-compose 명령어가 실패한다면, symbolic link를 직접 걸어주도록 합니다. $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose $ docker-compose --version docker-compose version 1.24.1, build 1110ad01 harbor installer 다운로드 공식 Github에서 원하는 인스톨러를 다운로드 받습니다. 저는 online installer를 사용할 예정입니다.\n다운로드가 완료되었으면 압축을 해제합니다.\n$ tar xvf harbor-*.tgz harbor/prepare harbor/LICENSE harbor/install.sh harbor/common.sh harbor/harbor.yml harbor.yml 설정 vi로 harbor/harbor.yml을 열고 적당하게 편집합니다.\n여기서는 http로 간단하게 배포하는 설정을 해볼 것입니다.\nhostname: \u0026lt;domain or IP\u0026gt; # 192.168.x.x 그리고 https와 관련된 value를 모두 주석처리해줍니다.\n# https: # https port for harbor, default is 443 # port: 443 # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path proxy 환경이 아니라면 더 손댈 곳은 없습니다. proxy 환경일 경우 하단에 있는 proxy 설정을 /etc/environment 등을 참조하여 미리 적혀있는 부분을 추가하여 작성하시면 됩니다.\ninstall install시 clair, notary, chart-museum을 함께 설치할 수 있습니다.\n여기서 notary는 https 설정이 필요하기 때문에 생략하고 나머지 두개를 설치합니다.\n~/harbor$ ./install.sh --with-clair --with-chartmuseum 확인 이제 hostname에서 설정한 곳으로 접속하여 확인합니다. 설정을 바꾸지 않았다면 80포트로 접속할 수 있기 때문에 address만 입력해주면 접속할 수 있습니다.\n기본 ID/PW는 admin/Harbor12345 입니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/start-tmux-after-reboot/",
	"title": "reboot 후에 tmux를 실행시켜 원하는 작업을 하기",
	"tags": ["tmux", "reboot", "ubuntu"],
	"description": "",
	"content": "tmux는 terminal을 한 창에 여러개 띄울 때 사용하는 프로그램입니다.\n이 프로그램의 특징은 detach 모드로 들어가면, 어디서든 terminal에 접속하여 해당 session에 접속했을 때, 그 화면 그대로를 가져올 수 있다는 것입니다.\n즉, 원격 접속을 통해 서버에 접속했을 때 작업을 돌려놓고 detach모드로 들어가면 나의 session을 꺼도 실제 서버에서는 해당 작업이 계속해서 돌아가고 있다는 것입니다. 퇴근하기 전 시간이 오래걸리는 작업을 돌려놓고 가야할 때 유용하게 사용할 수 있습니다.\n저의 경우는 제 로컬 컴퓨터에서 hugo를 통해 사이트를 생성하여 블로그를 편집할 때마다 즉시 그 결과를 보고 있습니다. 이 떄 사용되는 명령어는 간단하지만, 매번 컴퓨터를 켤 때마다 이를 수행해주어야 한다는 것은 여간 귀찮은 일이 아닙니다. 따라서 다음과 같은 프로세스로 tmux를 생성하여 제가 하는 작업을 수행하도록 설정할 것입니다.\noverview  tmux session을 이름을 지정하여 생성하고, daemon으로 돌린다. 해당 session에 직접 접속하지 않고 명령어를 전달한다. 1, 2의 작업을 스크립트로 만들고 재부팅 시 실행하도록 한다.  절차 sh script 작성 다음과 같은 스크립트를 작성합니다. (tmux-on-reboot.sh)\n#!/bin/zsh  SESSIONNAME=\u0026#34;script\u0026#34; tmux has-session -t $SESSIONNAME 2\u0026gt; /dev/null if [ $? != 0 ] then tmux new-session -s $SESSIONNAME -n script -d \u0026#34;bin/zsh\u0026#34; tmux send-keys -t $SESSIONNAME \u0026#34;cd /home/wanderlust/Ibiza\u0026#34; C-m tmux send-keys -t $SESSIONNAME \u0026#34;hugo server --bind 0.0.0.0 --port 8000 --disableFastRender\u0026#34; C-m fi 여기서 C-m은 enter 명령을 주기 위함입니다.\n그 다음 해당 파일에 실행권한을 줍니다.\nchmod +x tmux-on-reboot.sh 재부팅 시 실행하도록 설정 이를 위해서는 crontab을 사용할 것입니다.\ncrontab을 사용하는 방법은 crontab -e를 통해 root 권한으로 명령어를 실행하는 방법과 /etc/crontab을 수정하여 원하는 유저를 부여하는 방법이 있습니다. 여기서는 제가 사용할 계정을 가지고 생성하는 것을 해보도록 하겠습니다.\nvi /etc/crontab vim이 열리면 가장 아랫줄에 다음과 같이 추가합니다.\n@reboot wanderlust /home/wanderlust/scripts/tmux-on-reboot.sh 저장 후 재부팅하여 실행되는지 확인합니다.\nReference  https://superuser.com/a/440082?  "
},
{
	"uri": "http://kimmj.github.io/docker/use-docker-without-sudo/",
	"title": "Docker를 sudo없이 실행하기",
	"tags": ["docker", "sudo"],
	"description": "",
	"content": "docker 명령어는 docker group으로 실행됩니다. 그러나 저희가 기존에 사용하던 일반 user는 해당 group에 속하지 않기 때문에 docker 명령어를 쳤을 때 permission에 관한 에러가 발생하게 됩니다.\n이 때 다음과 같이 조치를 하면 sudo 없이 user가 docker 명령어를 사용할 수 있게 됩니다.\nsudo usermod -aG docker $USER session을 다시 열고 docker ps 명령어를 입력하여 에러가 발생하는지 확인합니다.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES "
},
{
	"uri": "http://kimmj.github.io/ubuntu/oh-my-zsh-home-end-key/",
	"title": "oh-my-zsh에서 home key와 end key가 안될 때 해결방법",
	"tags": ["oh-my-zsh", "zsh"],
	"description": "",
	"content": "oh-my-zsh을 설치하고 원격접속이나 로컬환경에서 터미널에 접속했을 때 home key와 end key가 먹히지 않는 경우가 있습니다.\n이런 경우에 사용하는 terminal에서 home key와 end key를 눌러 실제 어떤 값이 전달되는지 확인한 후, 이를 beginning-of-line, end-of-line으로 설정하면 해결할 수 있습니다.\n해결법  home key가 되지 않는 terminal에 접속합니다. Control+V를 누릅니다. 문제가 되는 home key를 누릅니다. terminal에 뜬 문자를 기록합니다. ~/.zshrc에 다음과 같이 추가합니다. 여기서 case에 관한 부분은 상황에 따라 넣지 않거나 변경해야 합니다. case $TERM in (xterm*) bindkey \u0026#39;^[[H\u0026#39; beginning-of-line bindkey \u0026#39;^[[F\u0026#39; end-of-line esac  source ~/.zshrc를 하거나 새로운 session을 열어서 확인합니다.  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/base64-encode-decode/",
	"title": "Ubuntu에서 Base64로 인코딩, 디코딩하기",
	"tags": ["ubuntu", "base64"],
	"description": "",
	"content": "Encode echo로 입력하기 $ echo \u0026#34;password\u0026#34; | base64 cGFzc3dvcmQK Control+D를 누를때까지 입력하기 $ base64 admin password ^D # Control+D # result YWRtaW4KcGFzc3dvcmQK Decode echo로 입력하기 $ echo \u0026#34;cGFzc3dvcmQK\u0026#34; | base64 --decode password Control+D를 누를때까지 입력하기 $ base64 --decode YWRtaW4KcGFzc3dvcmQK ^D # Control+D # result admin password "
},
{
	"uri": "http://kimmj.github.io/ubuntu/file-edit-without-editor/",
	"title": "Editor(vi)가 없을 때 파일 수정하기",
	"tags": ["ubuntu", "file", "editor"],
	"description": "",
	"content": "echo로 파일 내용을 입력하는 방법 \u0026gt;로 파일 덮어쓰기 $ cat file asdfasdfasdf $ echo \u0026#34;asdf\u0026#34; \u0026gt; file $ cat file asdf \u0026gt;\u0026gt;로 파일에 이어쓰기 $ cat file asdf $ echo \u0026#34;asdf\u0026#34; \u0026gt;\u0026gt; file $ cat file asdf asdf cat으로 파일 입력하는 방법 \u0026gt;로 파일 덮어쓰기 $ cat file asdf $ cat \u0026gt; file aaaa bbbb ^D # Command+D $ cat file aaaa bbbb \u0026gt;\u0026gt;로 파일에 이어쓰기 $ cat file asdf $ cat \u0026gt;\u0026gt; file aaaa bbbb ^D # Command+D $ cat file asdf aaaa bbbb \u0026laquo;EOF로 EOF을 입력하면 입력 완료하기 $ cat file asdf $ cat \u0026lt;\u0026lt;EOF \u0026gt; file aaaa bbbb EOF $ cat file asdf aaaa bbbb "
},
{
	"uri": "http://kimmj.github.io/kubernetes/stern/",
	"title": "Stern을 이용하여 여러 pod의 log를 한번에 확인하기",
	"tags": ["kubernetes", "stern", "log"],
	"description": "",
	"content": "Kubernetes에서의 trouble shooting kubernetes 환경에서 어떤 문제가 발생하면 다음과 같은 flow로 확인을 해보면 됩니다.\n kubectl get pods -o yaml로 yaml을 확인하기 kubectl describe pods로 pod에 대한 설명 확인하기 kubectl describe deployments(statefulset, daemonset)으로 확인하기 kubectl logs로 로그 확인하기  보통 kubernetes 리소스의 부족과 같은 kubernetes단의 문제는 1~3을 확인하면 전부 문제점을 찾을 수 있습니다. 그러나 어플리케이션의 직접적인 원인을 알아보기 위해서는 log를 확인해야 합니다.\n하지만 kubectl의 logs에는 한가지 한계점이 있는데, 바로 단일 container에 대해서만 log 확인이 가능하다는 점입니다.\nstern은 이러한 문제를 해결하는 tool입니다.\n설치 방법 공식 Github: https://github.com/wercker/stern\nRequirements  golang govendor 기타 build에 필요한 dependency는 govendor로 관리  절차 golang 설치하기 go 언어는 직접 github에서 가져와 build할 수도 있지만, 간단하게 Ubuntu라면 apt를, macOS라면 brew를 이용하여 설치할 수 있습니다.\n# Ubuntu sudo apt install golang # macOS brew install go govendor 설치하기 govendor는 external package를 import하는데 사용하는 툴입니다. 이를 통해 dependency를 쉽게 import할 수 있습니다.\n간단하게 go 명령어만으로 설치할 수 있습니다.\n# install $ go get -u github.com/kardianos/govendor # test $ govendor --version v1.0.9 만약 govendor 명령어가 되지 않는다면, 보통은 go의 binary 폴더가 PATH에 추가되지 않은 것입니다.\ngo env를 통해 GOPATH, GOBIN을 확인합니다. GOBIN이 비어있을 경우 $GOPATH/bin이 GOBIN입니다.\n정상적으로 binary 파일이 있다면 PATH에 추가합니다. 저는 GOPATH가 /root/go였으므로 다음과 같이 입력하였습니다. terminal에 입력하는 것은 임시조치이므로 영구적으로 기록하려면 /etc/profile 또는 ~/.bashrc같은 파일에 저장합니다.\nexport PATH=$PATH:/root/go/bin stern 설치하기 env | grep GOPATH 명령어를 통해 GOPATH가 제대로 적용이 되는지 먼저 확인합니다. 제대로 안되어있을 경우 export GOPATH=/root/go처럼 적용해줍니다. 그 다음 다음의 명령어를 입력합니다.\nmkdir -p $GOPATH/src/github.com/wercker cd $GOPATH/src/github.com/wercker git clone https://github.com/wercker/stern.git \u0026amp;\u0026amp; cd stern govendor sync go install 확인 $ stern -v stern version 1.11.0 사용법 정확한 사용법은 이곳을 확인하시면 좋습니다.\n간단하게 설명하면, stern은 regEx로 매칭되는 pod나 container의 log를 tail -f처럼 보여주는 것입니다. 또한 그 결과는 색깔로 구분지어 보기 쉽습니다.\nstern -n kube-system kube-proxy-* "
},
{
	"uri": "http://kimmj.github.io/git/gitignore/",
	"title": "Gitignore 설정",
	"tags": ["git", "gitignore"],
	"description": "",
	"content": "참조: https://git-scm.com/docs/gitignore\ngitignore은 git에서 어떤 파일을 무시할지 설정하는 파일입니다. 이미 tracked된 것들에는 영향을 주지 않습니다.\ngitignore 참조 순서 syntax blank line 파일과 매칭되지 않습니다. 따라서 가독성을 위해 사용할 수 있습니다.\n# #은 comment로 처리됩니다.\nTraling space \\로 감싸졌다고 하더라도 무시됩니다.\n! !는 not과 같습니다. 파일 이름 맨 앞에 !가 있고, 이를 ignore할 때 사용하려면 \\를 통해 escape 해줘야 합니다.\n/ /는 디렉토리를 구분합니다. 처음, 중간, 끝 어느 위치에도 올 수 있습니다.\n끝에 /를 사용하면 매칭되는 폴더에만 적용이 됩니다. 사용하지 않으면 매칭되는 폴더, 파일 모두 적용됩니다.\n* /를 제외한 모든 길이의 문자와 매칭됩니다.\n? /를 제외한 한 문자와 매칭됩니다.\nrange [a-zA-Z]처럼 range를 설정할 수 있습니다.\n** **는 /를 포함한 모든 길이의 문자와 매칭됩니다.\n**로 시작하고 /를 적으면 모든 디렉토리에서 검색합니다. 예를 들어 **/foo는 모든 디렉토리에 있는 foo와 매칭됩니다. (현재 디렉토리인 .도 포함됩니다. ..는 포함되지 않습니다.)\n**로 끝나는 경우 모든 내부 파일, 폴더와 매칭됩니다. 예를 들어 abc/**는 abc 폴더 내부의 모든 파일, 폴더와 매칭됩니다.\n**를 중간에 적은경우 모든 sub directory(0개 이상)와 매칭됩니다. 예를 들어 a/**/b는 a/b, a/x/b, a/x/y/b 모두 매칭됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/check-listen-port/",
	"title": "열려있는 포트 확인하기",
	"tags": ["linux", "port", "network"],
	"description": "",
	"content": "열려있는 포트 확인하기 # 방법 1 lsof -i -nP | grep LISTEN | awk \u0026#39;{print $(NF-1)\u0026#34; \u0026#34; $1}\u0026#39; | sort -u # 방법 2 netstat -tnlp 열려있는 포트 확인하기 + 관련된 프로세스 이름 확인하기 netstat -tnlp | grep -v 127.0.0.1 | sed \u0026#39;s/:::/0 /g\u0026#39; | sed \u0026#39;s/[:\\/]/ /g\u0026#39; | awk \u0026#39;{print $5\u0026#34;\\t\u0026#34;$10}\u0026#39; | sort -ug "
},
{
	"uri": "http://kimmj.github.io/ubuntu/using-watch-with-pipes/",
	"title": "pipe를 사용한 명령어를 watch로 확인하기",
	"tags": ["watch", "pipe"],
	"description": "",
	"content": "pipe(|)는 grep과 다른 기타 명령어들과 함께 사용하면 좀 더 다양한 작업을 할 수 있습니다.\nwatch는 특정 명령어를 주기적으로 입력하여 결과 메시지를 확인합니다. 즉, 무엇인가를 모니터링할 때 주로 사용하곤 합니다.\n바로 본론으로 들어가서 pipe를 사용한 명령어를 watch로 확인하는 방법은 다음과 같습니다.\nwatch \u0026#39;\u0026lt;command\u0026gt;\u0026#39; 위와같이 quote로 감싸주세요.\nls -al을 가지고 확인해 보도록 하겠습니다.\n$ ls -al | grep config -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml $ watch ls -al | grep config # quote를 사용하지 않은 것 ^C # 결과 출력되지 않음 $ watch \u0026#34;ls -al | grep config\u0026#34; Every 2.0s: ls -al | grep config -rw-rw-r-- 1 wanderlust wanderlust 2960 1월 21 23:40 config.toml $ watch \u0026#34;ll | grep config\u0026#34; Every 2.0s: ll | grep config sh: 1: ll: not found 특히 마지막을 보시면 alias된 명령어는 인식하지 못하는 것을 알 수 있습니다.\n다른 페이지에서 했던 alias watch=\u0026quot;watch \u0026quot;를 적용했다고 하더라도, alias된 것을 quote로 감쌌을 때에는 인식하지 못하는 것을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/use-alias-in-watch/",
	"title": "watch를 사용할 때 alias 이용하기",
	"tags": ["watch", "ubuntu", "alias"],
	"description": "",
	"content": "watch는 정해진 시간동안 뒤에 적은 명령어를 실행해주는 프로그램입니다. 가령 kubernetes를 다룰 때 watch kubectl get pods -n kube-system을 통해 kube-system 네임스페이스에 있는 파드들을 지속적으로 모니터링 할 수 있습니다.\n그러나 watch는 alias된 명령어를 인식하지 못합니다.\n$ ll total 44K drwxrwxr-x 2 wanderlust wanderlust 4.0K 1월 7 20:38 archetypes -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml drwxrwxr-x 16 wanderlust wanderlust 4.0K 2월 22 23:08 content $ watch ll Every 2.0s: ll sh: 1: ll: not found 이 때 해결할 수 있는 가장 편한 방법은 watch 자체를 alias 시켜버리는 것입니다.\n저는 zsh을 사용하고 있으므로 ~/.zshrc에 다음과 같이 추가하였습니다.\nalias watch=\u0026#39;watch \u0026#39; 그 다음 설정파일을 다시 불러오거나 새로운 session을 생성합니다.\n# 설정파일 다시 불러오기 source ~/.zshrc 다시 watch를 하여 확인해 봅니다.\n$ watch ll Every 2.0s: ls --color=tty -lh total 44K drwxrwxr-x 2 wanderlust wanderlust 4.0K 1월 7 20:38 archetypes -rw-rw-r-- 1 wanderlust wanderlust 2.9K 1월 21 23:40 config.toml drwxrwxr-x 16 wanderlust wanderlust 4.0K 2월 22 23:08 content 정상적으로 alias된 명령어를 인식하게 되었습니다.\n"
},
{
	"uri": "http://kimmj.github.io/python/python-beautiful-cli/",
	"title": "[번역]Python을 통해 이쁜 CLI 만들기",
	"tags": ["python"],
	"description": "",
	"content": "링크 : https://codeburst.io/building-beautiful-command-line-interfaces-with-python-26c7e1bb54df\ncommand line application을 만드는 것을 다루기 전에 빠르게 Command Line에 대해서 알아보자.\ncommand line 프로그램은 컴퓨터 프로그램이 생성되었을 때부터 우리와 함께 해왔고, 명령어들로 구성되어있다. commnad line 프로그램은 command line에서 또는 shell에서 동작하는 프로그램이다.\ncommand line interface는 user interface이지만 마우스를 사용하는 것이 아닌 terminal, shell, console에서 명령어를 입력하여 사용하는 것이다. console은 이미지나 GUI가 하나도 없이 전체 모니터 스크린이 텍스트로만 이루어진 것을 의미한다.\n위키피디아에 의하면\n CLI는 주로 1960년대 중만에 컴퓨터 terminal에서의 대부분의 컴퓨터 시스템과의 상호작용을 의미하고 1970년대와 1980년대를 거쳐 OpenVMS, MS-DOS를 포함한 개인용 컴퓨터와 Unix system, CP/M과 Apple DOS에서 사용되어 왔다. 인터페이스는 보통 명령어를 텍스트로 받고 이 명령어를 통해 적절한 system function을 동작시키게 하는 command line shell에서 동작한다.\n 왜 Python인가? Python은 유연성과 현존하는 프로그램들과도 잘 작동하기 때문에 보통 glue code language라고 여겨진다. 대부분의 Python 코드는 script와 command-line interface(CLI)로 작성된다.\n이런 command-line interface와 tool은 거의 대부분 원하는 것들을 자동화할 수 있기 때문에 특히 강력하다.\n우리는 예쁘고 상호작용을 하는 인터페이스, UI, UX가 매우 중요한 시대를 살고 있다. 우리는 이런 것들을 Command Line에 추가하고 사람들이 이를 받아들이고 Heroku같은 유명한 회사는 이를 공식적으로 사용할 것이다.\narguments와 option을 파싱하는 것에서부터 output에 색깔을 주고, progress bar를 추가하고 email을 전송하는 것과 같은 엄청난 CLI \u0026ldquo;frameworks\u0026quot;를 flagging하기까지 command line app을 build하는데 도움을 주는 많은 Python library와 module이 있다.\n이런 module과 함께 우리는 Heroku나 Vue-init이나 NPM-init같은 Node programe처럼 예쁘고 상호작용을 하는 command line interface를 만들 수 있다.\n예쁜 vue init CLI를 쉽게 만드려면 Inquirer.js를 Python에 이식하는 Python-inquirer를 사용한는 것을 권장한다.\n불행히도 Python-inquirer는 blessings(Unix같은 시스템에서만 사용가능한 _curses와 fcntl module을 import하는 python package)를 사용하기 때문에 Windows에서 작동하지 않는다. 어떤 대단한 개발자들은 _curses를 Windows에 이식할 수도 있긴 할것이다. Windows에서 fcntl의 대체제는 win32api이다.\n하지만 구글링을 통해 나는 이를 고치게 되었고 이를 PyInquirer라고 불르게 되었다. 이는 python-inquirer의 대체제이고 더 좋은 점은 이것은 Windows를 포함한 모든 플랫폼에서도 사용이 가능하다는 것이다.\nBasics in Commnad Line Interface with Python 이제 간단하게 command line interface를 보고 Python으로 하나를 만들어보자.\ncommand-line interface(CLI)는 실행파일의 이름으로 보통 시작한다. 그냥 console에서 이름을 입력하면 pip처럼 스크립트의 main entry point에 접근하게 된다.\nscript가 어떻게 개발되었는지에 따라 우리가 전달해 주어야 할 parameter들이 있고 이들은 이런 종류가 있다.\n Arguments: 스크립트에 전달되어야 하는 required parameter이다. 이를 작성하지 않으면 CLI는 error를 발생시킬 것이다. 예를 들어 django는 여기서 arguments이다. pip install django  Options: 이름에서 알 수 있듯이 optional parameter이다. 보통은 pip install django --cache-dir ./my-cache-dir처럼 name과 value의 쌍으로 사용한다. --cache-dir은 option parameter이고 value ./my-cache-dir은 cache directory로 사용되는 것이다. Flags: script에게 특정 행동을 disable할지 enable할지 알려주는 특수한 option parameter이다. 대부분은 --help같은 것들이다.  Heroku Toolbelt같은 복잡한 CLI를 통해 우리는 main entry point 아래의 몇몇 command들에 접근할 수 있게 된다. 이를 보통 commands와 sub-commands라고 한다.\n다른 python package들로 똑똑하고 예쁜 CLI를 어떻게 빌드하는지 보자.\nArgparse argparse는 command line program을 생성하는 default python module이다. 간단한 CLI를 만드는 데에 필요한 모든 기능을 제공한다.\nimport argparse parser = argparse.ArgumentParser(description=\u0026#39;Add som integers.\u0026#39;) parser.add_argument(\u0026#39;integers\u0026#39;, metavar=\u0026#39;N\u0026#39;, type=int, nargs=\u0026#39;+\u0026#39;, helm \u0026#39;integer list\u0026#39;) parser.add_argument(\u0026#39;--sum\u0026#39;, action=\u0026#39;store_const\u0026#39;, const=sum, default=max, help=\u0026#39;sum the integers (default: find the max)\u0026#39;) args = parser.parse_args() print(args.sum(args.integers)) argparse는 간단한 추가적인 동작을 한다. argparse.ArgumentParser는 프로그램에 description을 추가할 수 있도록 하는 반면 parser.add_argument는 command를 추가할 수 있도록 한다. parser.parse_args()는 주어진 arguments를 리턴하고 보통 이것들은 name-value 쌍으로 제공된다.\n예를 들어 args.integers를 통해 integers arguments에 접근할 수 있다. 위의 script에서 --sum은 optional argument이지만 N은 positional argument이다.\nClick click으로 우리는 CLI를 argparse보다는 쉽게 만들 수 있다. click은 argparse가 해결하는 문제와 동일한 것을 해결해준다. 하지만 약간은 다른 접근방식을 사용한다. 이는 decorators concept을 사용한다. 이는 command가 function이 되도록 하며 이를 decorators로 감쌀 수 있게 한다.\n# cli.py import click @click.command() def main(): click.echo(\u0026#34;This is a CLI built with Click\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 아래와 같이 argument와 option을 추가할 수 있다.\n# cli.py import click @click.command() @click.argument(\u0026#39;name) @click.option(\u0026#39;--greeting\u0026#39;, \u0026#39;-g\u0026#39;) def main(name, greeting): click.echo(\u0026#34;(), ()\u0026#34;.format(greeting, name)) if __main__ == \u0026#34;__main__\u0026#34;: main() 위의 script를 실행해보면 다음과 같을 것이다.\n$ python cli.py --greeting \u0026lt;greeting\u0026gt; Oyetoke Hey. OyeTOKI 모든걸 모아 Google Books의 query books에 간단한 CLI를 만들었다.\n click에 대한 자세한 정보는 official documentation에서 확인할 수 있다.\nDocopt docopt는 POSIC-style이나 Markdown 사용법을 파싱하여 쉽게 command line interface를 생성하는 lightweight python package이다. docopt는 수년간 사용해온 command line interface를 설명하는 정형화된 helm message와 man page에 대한 convention을 사용한다. docopt에서의 interface description은 helm message와 비슷하지만 정형화되어있다.\ndocopt는 파일의 젤 위에 required docstring이 어떻게 형식을 갖추는지가 중요하다. tool의 이름 다음에 올 docstring에서의 맨 위의 element는 Usage이여야 하고 이는 command가 어떻게 호출되고자 하는지에 대한 리스트들을 나열해야 한다.\ndocstring에서 두번째 element는 Options여야 하고 이는 Usage에서 나타난 option과 arguments에 대한 정보를 제공해야 한다. docstring의 내용들은 help text의 내용이 될 것이다.\n PyInquirer PyInquirer는 interactive command line user interface이다. 우리가 위에서 보았던 package들은 우리가 원하는 \u0026ldquo;예쁜 interface\u0026quot;를 제공하지 않는다. 따라서 어떻게 PyInquirer를 사용하는지 알아보도록 하자.\nInquirer.js처럼 PyInquirer는 두가지 간단한 단계로 구성되어 있다.\n 질문 리스트를 정의하고 이를 prompt에 전달한다. prompt는 답변 리스트를 리턴한다.  from __future__ import print_function, unicode_literals from PyInquirer import prompt from pprint import pprint questions = [ { \u0026#39;type\u0026#39;: \u0026#39;input\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;first_name\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;What\\\u0026#39;s your first name\u0026#39;, } ] answers = prompt(questions) pprint (answers) 상호작용하는 예시이다.\n 결과는 다음과 같다.\n이 스크립트의 몇몇 부분을 보도록 하자.\nstyle = style_from_dict({ Token.Separator: \u0026#39;#cc5454\u0026#39;, Token.QuestionMark: \u0026#39;#673ab7 bold\u0026#39;, Token.Selected: \u0026#39;#cc5454\u0026#39;, # default Token.Pointer: \u0026#39;#673ab7 bold\u0026#39;, Token.Instruction: \u0026#39;\u0026#39;, # default Token.Answer: \u0026#39;#f44336 bold\u0026#39;, Token.Question: \u0026#39;\u0026#39;, }) style_from_dict는 우리의 interface에 적용하고자 하는 custom style을 정의할 때 사용한다. Token은 component와 같은 것이고 이 아래에는 다른 component들도 있다.\n이전 예시에서 questions list를 prompt로 전달하여 처리하는 것을 보았다.\n이 방식으로 우리가 생성할 수 있는 interactive CLI에는 이런 예시가 있다.\n 결과\nPyFiglet pyfiglet은 ASCII text를 arts fonts로 변환해주는 python module이다. pyfiglet은 모든 FIGlet(http://www.figlet.org/)들을 pure python에 이식한 것이다.\nfrom pyfiglet import Figlet f = Figlet(font=\u0026#39;slant\u0026#39;) print f.rederText(\u0026#39;text to render\u0026#39;) 결과\nClint clint는 CLI를 만드는데 필요한 모든 것들을 통합한 것이다. color도 지원하고 뛰어난 nest-able indentation context manager도 지원하며 custom email-style quotes, auto-expanding column이 되는 column printer도 지원한다.\n EmailCLI 모든걸 통합하여 나는 SendGrid를 통해 메일을 전송하는 간단한 cli를 작성하였다. 아래의 스크립트를 사용하려면 SendGrid에서 API Key를 받아야 한다.\nInstallation pip install sendgrid click PyInquirer pyfiglet pyconfigstore colorama termcolor six   읽으면 좋은 것: https://www.davidfischer.name/2017/01/python-command-line-apps/  "
},
{
	"uri": "http://kimmj.github.io/docker/connect-container-to-container/",
	"title": "[docker-compose] container에서 다른 container로 접속하기",
	"tags": ["docker", "docker-compose", "network", "bridge", "container"],
	"description": "",
	"content": "배경 docker-compose에서는 network bridge를 설정합니다. 이 bridge로 내부 통신을 하게 되죠. 여기서 port-forward를 통해 외부로 서비스를 expose하게 되면 host의 IP와 port의 조합으로 접속할 수 있습니다.\n그런데 저는 네트워크 설정의 문제인지, 하나의 container에서 host IP로 접속이 불가능했습니다. 그러면서도 저는 어떻게든 다른 docker-compose의 서비스로 네트워킹이 됐어야 했습니다. 정확히 말하자면 harbor라는 서비스(docker registry)에서 jenkins로 webhook을 날려야 하는 상황이었죠.\n먼저 시도했던 것은 jenkins의 ip를 docker inspect jenkins_jenkins_1을 통해 알아내고, 이를 통해 webhook을 전송하는 것이었습니다. 그러나 실패했죠.\n다음으로 생각해본 것은, 그렇다면 jenkins를 harbor의 bridge로 연결해보자는 것이었습니다.\n따라서 다음과 같은 조치를 취해주었습니다.\ncontainer에서 다른 container로 접속하기 이미 동작중인 container에 bridge 연결 docker network connect harbor_harbor jenkins_jenkins_1 # docker network connect \u0026lt;bridge\u0026gt; \u0026lt;container\u0026gt; 연결된 bridge에서의 ip 확인 $ docker inspect jenkins_jenkins_1 \u0026#34;harbor_harbor\u0026#34;: { \u0026#34;IPAMConfig\u0026#34;: {}, \u0026#34;Links\u0026#34;: null, \u0026#34;Aliases\u0026#34;: [ \u0026#34;---\u0026#34; ], \u0026#34;NetworkID\u0026#34;: \u0026#34;---\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;---\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.24.0.1\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.24.0.11\u0026#34;, # \u0026lt;--------IP 연결 확인 harbor-core $ curl 172.24.0.11:8080 이 때, 연결은 host가 port-forward한 port가 아닌, container가 expose하고 있는 port를 입력해주어야 합니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-without-password/",
	"title": "password 없이 ssh 접속하기",
	"tags": ["ssh", "passwordless"],
	"description": "",
	"content": "자주 접속하는 서버에 패스워드를 항상 입력하는 것은 귀찮은 일이 될 것입니다.\n여기에서는 ssh key를 생성하고, 이를 이용하여 인증을 해 password를 입력하지 않는 방법을 알아볼 것입니다.\nssh-keygen을 통한 ssh key 생성 ssh 접속을 할 때 password를 입력했던 것처럼, 항상 ssh 접속을 위해서는 인증을 위한 key가 필요합니다.\n인증에 사용할 키를 ssh-keygen으로 생성하는 방법은 다음과 같습니다.\nssh-keygen -t rsa -b 4096 -t는 rsa 알고리즘을 통해 key를 생성하겠다는 의미이며, -b는 key의 사이즈를 정해주는 것입니다.\n다른 알고리즘들과 다른 옵션들은 https://www.ssh.com/ssh/keygen에서 더 확인할 수 있습니다.\n이제 ssh 인증을 위한 public key를 생성하였습니다.\nssh-copy-id를 통한 public key 복사 위에서 생성한 public key를 접속하고자 하는 서버에 복사를 하면, 서버에 접속할 때 서버는 해당 파일을 참조하여 인증을 시도할 것입니다.\nssh-copy-id user@server 이 때 최초 1회만 패스워드를 입력하면 됩니다. 그 뒤 다시한번 로그인을 시도해보면 패스워드 없이 로그인에 성공할 것입니다.\n  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-tunneling/",
	"title": "SSH Tunneling 사용법",
	"tags": ["ubuntu", "tunneling"],
	"description": "",
	"content": "-D 옵션으로 socks proxy 사용하기 A라는 서버에서 B라는 서버에 있는 서비스를 보려고 합니다. 이 때, 해당 웹 어플리케이션은 B에서만 연결된 특정 IP로 통신을 하고 있고, 이 때문에 A에서 어플케이션이 제대로 동작하지 않는 상황입니다.\n이 때 사용할 수 있는 것이 -D 옵션입니다.\n예시\nssh -D 12345 user@server.com 해당 세션이 꺼져있지 않은 상태에서 A 서버에서 웹 브라우저가 localhost:12345를 프록시로 사용하도록 하면 해당 웹 어플리케이션이 제대로 동작합니다.\n만약 windows라면 다음과 같이 진행하면 socks proxy를 사용하도록 할 수 있습니다. CMD를 열고 다음과 같이 입력하면 새로운 창으로 chrome이 뜰 것입니다.\n\u0026#34;C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\u0026#34; --user-data-dir=\u0026#34;%USERPROFILE%\\proxy-profile\u0026#34; --proxy-server=\u0026#34;socks5://localhost:12345\u0026#34; 해당 창에서 어플리케이션을 실행하면 실제 B 서버의 desktop에서 동작하는 것과 같은 효과를 볼 수 있습니다.\n-L 옵션으로 특정 포트로 접속하기 A에서 B라는 서버의 어플리케이션을 사용하려 합니다. 이 어플리케이션은 특정 포트를 사용하고 있습니다. 쉬운 예시를 위해 jenkins를 B에서 구동한다고 하고 포트를 8080이라고 하겠습니다.\n이 상황에서 A에서는 server-b.com:8080에 접속할 수 없는 상황입니다.\n이 경우 -L 옵션을 사용하면 됩니다.\nssh -L 1234:localhost:8080 user@server-b.com 해당 설정을 읽어보자면, A서버의 localhost:1234로 들어오는 요청들을 user@server-b.com으로 접속한 뒤 localhost:8080으로 보내라는 의미입니다.\n다음과 같은 설정도 있을 수 있습니다.\nssh -L 12345:server-c.com:8080 user@server-b.com 이 설정은 A에서 C로 직접 통신이 안되지만, B에서 C로 통신이 되는 상황입니다.\n이 때, 위의 설정은 localhost:12345로 들어오는 요청들을 user@server-b.com으로 접속한 뒤 server-c.com:8080으로 보내라는 의미입니다.\n-R 옵션을 이용하여 Reverse Proxy 사용하기 A에서 B로 ssh 접속을 하고 있습니다. 이 때, B에서 A로는 접속이 방화벽으로 막혀있는 상황입니다.\n이런 상황에서 -R 옵션을 주면 B에서도 A로 접속할 수 있습니다.\nssh -R 12345:localhost:22 user@server-b.com 해당 설정은 A서버에서 접속하는 localhost의 22 포트를 B서버의 12345 포트와 연결하라는 의미입니다.\n그 다음 해당 세션이 유지된 상태에서 B에서 다음과 같이 접속합니다.\nssh localhost -p 12345 그러면 B서버에서 A서버로 접속이 가능한 것을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/ssh-with-jump/",
	"title": "Gateway를 이용하여 SSH 접속하기",
	"tags": ["ssh", "linux"],
	"description": "",
	"content": "ssh cli 이용하는 방법 -J 옵션을 이용한다.\nssh user@server -J user2@server2 두개 이상의 경우 ,로 구분한다.\n예: user2@server2로 접속 후 user3@server3로 접속한 뒤 user@server로 접속해야 할 경우\nssh user@server -J user2@server2,user3@server3 이 상황에서 ssh-copy-id를 이용해 패스워드를 입력하지 않고 이동하려면\nlocaluser@localhost $ ssh-copy-id user2@server2 localuser@localhost $ ssh user2@server2 user2@server2 $ ssh-copy-id user3@server3 user2@server2 $ ssh user3@server3 user3@server3 $ ssh-copy-id user@server 이후 ssh를 통해 진입하면 패스워드 없이 접속 가능.\n만약 port가 필요한 경우 server:port 형태로 입력\nssh user@server:port -J user2@server2:port2,user3@server3:port3 ssh config 파일 이용하는 방법 Host server HostName remote-server User user ProxyJump gateway2 Host server2 HostName gateway1 User user2 Host server3 HostName gateway2 User user3 ProxyJump gateway1 "
},
{
	"uri": "http://kimmj.github.io/iac/translate-what-is-infrastructure-as-a-code/",
	"title": "[번역] What Is Infrastructure as a Code? How It Works, Best Practices, Tutorials",
	"tags": ["IaC", "infrastructure-as-code"],
	"description": "",
	"content": "link: https://stackify.com/what-is-infrastructure-as-code-how-it-works-best-practices-tutorials/\n과거에 IT infrastructure를 관리하는 것은 힘든 일이었다. 시스템 관리자는 수동으로 관리하고 어플리케이션을 구동시키기 위해 모든 하드웨어와 소프트웨어를 설정해야 했다.\n하지만 최근 몇년간 급격하게 상황들이 바뀌었다. cloud computing같은 트렌드가 디자인, 개발, IT infrastructure의 유지를 하는 방법을 혁명화하고 발전시켰다.\n이러한 트렌드의 핵심 요소는 infrastructure as code이다. 여기에 대해 이야기 해보도록 하겠다.\nDefining Infrastructure as Code infrastructure를 코드로 정의하는 것부터 시작해보도록 하자. 이것이 무엇을 의미하는지, 어떤 문제들을 해결하는지 배우게 될 것이다.\nWikipedia에서 IaC는 다음과 같이 정의되어 있다.\n Infrastructure as code is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.\n  Infrastructure as code는 물리적인 하드웨어 설정이나 상호작용을 하는 설정 툴을 이용하는 것이 아닌, 기계가 읽을 수 있는 파일로 정의하여 computer data center들을 관리하고 프로비저닝하는 프로세스이다.\n 이 정의는 나쁘지 않지만 약간 너무 장황하게 설명했다. 더 간단하게 해보자.\n Infrastructure as code (IaC)는 IT infrastructure를 설정 파일로 관리한다는 의미이다.\n 다음 질문은 이렇게 될 것이다. \u0026ldquo;왜 그걸 쓰고싶어 하나요?\u0026rdquo;\nWhat Problem Does IaC Solve? \u0026ldquo;what\u0026quot;에 대해 생각하기 전에 \u0026ldquo;why\u0026quot;에 먼저 집중해보자. 왜 IaC가 필요한가? 이것이 어떤 문제를 풀어주는가?\nThe Pain of Managing IT Infrastructure 역사적으로 IT infrastructure를 관리하는 것은 수동 프로세스였다. 사람들은 물리적으로 서버를 위치시키고 이를 설정했다. 머신이 OS와 어플리케이션이 원하는 설정으로 되었을 때에, 어플리케이션을 배포할 수 있게 된다. 당연하게도 수동 프로세스는 자주 문제를 일으킨다.\n첫번째 큰 문제는 비용이다. 네트워크 엔지니어에서부터 하드웨어 관리 기술자까지 많은 전문가를 고용해서 프로세스의 각 단계를 수행시키도록 해야한다. 이런 모든 사람에게 돈을 지불해야 하면서 또한 관리되어야 한다. 이는 관리 오버헤드를 야기하고 기업 내의 소통의 복잡성을 증대시킨다. 결과적으로? 돈이 사라진다. 또한 우린 아직 비용을 더 많이 늘리는 데이터센터를 관리하는 것과 빌딩에 대한 이야기를 하지 않았다.\n그 다음 문제는 확장성과 가용성이다. 하지만 결국 모든 것은 속도 문제다. 수동으로 설정하는 것은 너무 느리고, 어플리케이션의 접속은 스파이크를 치고 있는 동안 시스템 관리자는 부하를 관리하기 위해 필사적으로 서버를 세팅할 것이다. 이는 가용성에도 무조건 영향을 미친다. 기업이 백업 서버나 심지어 데이터 센터가 없다면 어플리케이션은 장기간동안 이용불가능해질 것이다.\n마지막으로 가장 중요한 문제는 inconsistency이다. 몇몇 사람들이 수동으로 설정을 배포한다면, 균열은 생길수밖에 없다.\nCloud Computing: A Cure? Cloud computing은 방금 읽었던 고통들을 완화시켜준다. 이는 데이터센터를 구축하고 유지보수하는 것과 그 비용으로부터 자유롭게 해준다.\n그러나 Cloud computing은 만병통치약과는 거리가 멀다. 이것이 infrastructure를 빠르게 셋업하는데에는 도움을 주겠지만 (그래서 고가용성이나 확장성의 문제는 해결해 준다) 이는 inconsistency 이슈를 해결하지는 못한다. 설정을 수행하는 사람이 한명보다 많다면, 불균형이 생길 수 있다.\nInfrastructure as Code: The Missing Piece of the Puzzle IaC 정의를 다시 한번 봐보자.\n Infrastructure as code (IaC)는 IT infrastructure를 설정 파일로 관리한다는 의미이다.\n 정의에서 가져온 핵심은 바로 다음과 같다. IaC 이전에 IT 사람들은 infrastructure에 대해 수동으로 설정을 바꾸어야 했다. 아마 스크립트를 쓰거나 몇몇 작업은 자동화를 시켰겠지만, 그냥 그 정도였다. IaC를 통해 infrastructure의 설정은 코드 파일의 형태를 띄게 되었다. 이는 단순한 텍스트이지만 수정하고 복사하고 분배하는 것이 쉽다. 당신은 다른 소스코드 파일들처럼 source control로 이를 관리할 수 있고 또 그래야 한다.\nInfrastructure as Code Benefits 이제까지 수동으로 infrastructure를 관리하는 것의 문제점들을 알아 보았다. 우리는 여기서 어떻게 cloud computing이 이런 몇가지 문제점을 해결해 주는지, 또 어떤것들은 해결해주지 않는지 알아보았다. 그리고 우리는 IaC가 퍼즐의 마지막 조각이라고 말하며 이 논의를 마쳤다.\n이제 우리는 IaC solution을 사용할 때의 이점에 대해서 알아볼 것이다.\nSpeed IaC의 가장 중요한 이점은 스피드이다. Infrastructure as Code는 script를 실행시켜 빠르게 완전한 infrastructure를 셋업할 수 있게 해준다. 이를 개발환경과 production 환경부터 staging, QA 등등까지 모든 환경에 대해서 할 수 있다. IaC는 전체 소프트웨어 개발 라이프사이클을 효과적으로 만들어준다.\nConsistency 수동 프로세스는 실수를 불어일으키고 시간이 걸린다. 사람은 실수할 수 있다. 우리의 기억은 잘못될 수 있다. 소통은 어렵고 우리는 일반적으로 소통을 어려워한다. 여기서 읽었던 것 처럼 수동으로 인프라를 관리하는 것은 얼마나 열심히 하던지 간에 불균형을 일으키게 된다. IaC는 믿을 수 있는 하나의 소스코드로 이 설정 파일을 관리하여 이를 해결해준다. 이런 방식으로 동일한 설정이 어떤 불균형도 없이 계속해서 배포됨을 보장한다.\nAccountability 이는 빠르고 쉬운 것이다. IaC 설정 파일을 소스 코드 파일처럼 버전화 할 수 있기 때문에 설정들의 변경사항을 추적할 수 있다. 누가 이를 했고 언제 했는지 추측할 필요가 없다.\nMore Efficiency During the Whole Software Development Cycle infrastrucutre as code를 적용하면 infrastructure 아키텍쳐를 어려 단계로 배포할 수 있다. 이는 전체 소프트웨어 개발 라이프 사이클을 더욱 효율적으로 해주어 팀의 생산성을 더 끌어올릴 수 있다.\n프로그래머들이 IaC를 사용하여 sandbox 환경을 생성하여 독립된 공간에서 안전하게 개발할 수 있게 할 수 있다. 같은 방식으로 테스트를 돌리기 위해 production 환경을 복사해야 하는 QA 전문가들도 사용할 수 있다. 결과적으로 배포 시에 infrastructure as code는 하나의 단계가 될 것이다.\nLower Cost IaC의 주된 장점 중 하나는 의심할 여지 없이 인프라 관리의 비용이 줄어든다는 것이다. IaC를 통해 cloud computing을 하면 극적으로 비용을 줄일 수 있다. 이는 하드웨어에 많은 시간을 사용하지 않아도 됨을 의미하고 이를 관리할 사람을 고용하지 않아도 되며 저장할 물리적인 장소를 만들거나 대여하지 않아도 됨을 의미한다. 하지만 IaC는 기회비용이라 부르는 다른 미묘한 방식으로 비용을 더 줄여준다.\n보았듯이 똑똑하고, 높은 급료를 받는 전문가가 자동화 할 수 있는 작업을 수행하는 것은 돈 낭비이다. 이제 모든 포커스는 기업에 더 가치있는 일에 맞춰져야 한다. 그리고 자동화 전략을 사용하면 편하다. 이를 사용하여 수동적이고 느리고 에러가 나기 쉬운 작업을 수행하는 것으로부터 엔지니어들을 해방시켜주고 더 중요한 일에 집중할 수 있게 한다.\nHow Does IaC Work? IaC 툴은 어떻게 작동하는지가 굉장히 다양하지만 이를 일반적으로 두가지 종류로 나눠볼 수 있다. 하나는 imperative approach를 따르는 것이고 다른 하나는 declarative approach를 따르는 것이다. 이 위의 카테고리를 프로그래밍 언어 패러다임과 엮을 수 있다면 완벽하다.\nimperative approach는 순서를 제공하는 것이다. 이는 명령어나 지시사항의 순서를 정의하여 인프라가 최종적인 결과를 가지게 하는 것이다.\ndeclarative approach는 완하는 결과를 정의하는 것이다. 명백하게 원하는 결과로 가는 단계들의 순서를 정의하지 않고, 어떻게 최종 결과가 보여야 하는지만 정의한다.\nLearn Some Best Practices 이제 IaC 전략의 모범사례를 확인해 볼 것이다.\n 코드가 하나의 믿을 수 있는 소스로부터 나온다. 명시적으로 모든 인프라 설정을 설정 파일 안에 작성해야 한다. 설정 파일은 인프라 관리 문제에 대해서 단 하나의 관리포인트이다. 모든 설정 파일에 대해 Version Control을 하라. 이는 말할 필요도 없겠지만, 모든 코드는 source control이 되어야 한다. 인프라 스펙에 대한 약간의 문서화만 필요하다. 이 포인트는 첫번째의 논리적인 결과이다. 설정 파일이 단일 소스이므로, 문서화가 필요하지 않다. 외부 문서는 실제 설정과 싱크가 잘 안맞을 수 있지만 설정 파일은 그럴 이유가 없다. 설정을 테스트하고 모니터한다. IaC는 코드로 모든 코드와 같이 테스트될 수 있다. 따라서 가능하면 테스트해라. IaC에 대한 테스트와 모니터링을 하여 production에 어플리케이션을 배포하기 전에 서버에 문제가 있는지 확인할 수 있다.  Resources At Your Disposal 다음은 IaC를 배우기 좋은 유용한 리소스들이다.\n Wikipedia’s definition Edureka’s Chef tutorial Ibexlabs’s.The Top 7 Infrastructure As Code Tools For Automation TechnologyAdvice’s Puppet vs. Chef: Comparing Configuration Management Tools  Infrastructure as Code Saves You Time and Money IaC는 DevOps 움직임의 중요한 부분이다. cloud computing이 많은 수동 IT 고나리의 문제점을 해결하는 데 첫 번째 단계라고 본다면 IaC는 다음의 논리적인 단계가 될 것이다. 이는 cloud computing의 모든 가능성을 열어주고 개발자와 다른 전문가들로부터 수동적이고 에러가 발생하기 쉬운 업무를 없애준다. 또한 소프트웨어 개발 라이프사이클의 효율성을 늘리고 비용을 절감한다. IaC와 함께 Retrace같은 툴을 쓰는것도 좋다. Retrace는 코드 레벨의 Application Performance Manager 솔루션으로 전체 개발 라이프사이클에서 어플리케이션의 퍼포먼스를 관리하고 모니터하게 할 수 있다. 에러 추적이나 로그 관리, 어플리케이션 메트릭과 같은 또한 많은 다른 기능들을 가지고 있다.\n"
},
{
	"uri": "http://kimmj.github.io/cicd/deploy-strategy/",
	"title": "Deploy Strategy",
	"tags": ["deploy", "cicd", "canary", "blue-green", "roll-out"],
	"description": "",
	"content": "Deploy Strategy 실제 시스템을 운용할 때 중요하게 여겨지는 것 중 하나가 downtime을 없애는 것이다. 새로운 업데이트가 있을 때마다 해당 인스턴스가 동작하지 않는다면, 자주 업데이트 하는 것이 어려워질 수 있습니다. 따라서 deploy strategy를 가지고 어떻게 downtime을 줄이는지 알아보도록 하겠습니다.\nCanary canary deploy는 트래픽 비율을 바꾸어가며 배포하는 전략입니다. 이해하기 편하도록 Kubernetes 환경이라고 생각해 보도록 하겠습니다. (또는 LoadBalancer가 있어서 부하를 분산하고 있다고 생각하면 좋을 것 같습니다.) 이 때 업데이트된 버전을 따로 올리고 트래픽을 old:new = 100:0으로 줍니다. 이러면 새로운 버전이 정상적으로 실행할 준비가 될 때까지는 우리의 인스턴스가 정상적으로 동작하고 있을 것입니다.\n준비가 되었다면, old:new = 90:10처럼 약간의 트래픽을 새로운 버전으로 흘려줍니다. 이 어플리케이션이 만약 웹사이트라면, 전체 유저 중 10%의 사람만이 새로운 버전의 웹사이트를 보게 될 것입니다.\n이 때 각종 통계라던지 테스트를 통해 새로운 버전에 문제가 없는지 확인합니다. 혹시나 문제가 발생하더라도, 10%의 사람만이 문제를 경험하게 될 것입니다. 문제점이 발견되면 다시 old:new = 100:0으로 트래픽을 돌려버리면 이전의 잘 돌아가던 상태로 복구할 수 있습니다.\n이런식으로 새로운 버전의 트래픽을 능동적으로 조금씩 늘려가며 여러 지표를 확인하고 정상적이라고 판단되면 old:new = 0:100으로 변경합니다. 그 다음 이전 버전을 삭제하면 이제 완전히 새로운 버전으로 업데이트 된 것입니다.\n이 방식은 특정 부분을 운용중에 긴급 패치하는 경우 사용할 수 있는 전략입니다. 장애가 발생하더라도 큰 위험 부담이 없기 때문이죠.\n그러나 이를 위해서는 새로운 버전이 얼마나 이전 버전과 호환이 잘 되는지가 중요합니다. 만약 이전 버전과는 너무나도 다른 새로운 버전이 있다면 전체 어플리케이션은 정상적으로 동작하지 않을 수 있습니다.\n             Blue-Green blue-green deploy는 두개의 버전을 동시에 올려놓고, 트래픽을 한번에 바꾸는 전략입니다.\n하나의 인스턴스에 대해 버전을 두가지 올립니다. 이 때, 트래픽의 비율은 old:new = 100:0으로 항상 이전 버전으로만 흐르도록 합니다. 그러다가 어느 순간 old:new = 0:100으로 트래픽을 아예 바꾸어버립니다. 그러면 유저는 항상 새로운 버전만 경험하게 될 것입니다.\n이렇게 새로운 버전을 운용하며 문제점을 파악합니다. canary와는 다르게 문제점이 있을 경우 해당 인스턴스가 아예 중단되어 downtime이 생깁니다. 이럴 때에는 다시 원래대로 트래픽을 old:new = 100:0으로 바꾸면 이전 버전으로 빠르게 roll back이 가능합니다.\n또한 canary에서는 긴급 패치하는 경우, 하나의 인스턴스에 대해서만 행해진다고 하였습니다. 그러나 blue-green의 경우 전체 패키지 또는 어플리케이션에 대해서도 사용할 수 있는 전략입니다. 물론 장비가 2배로 들겠지만, 트래픽만 바꾸면 되니 빠른 roll back이 가능하기 때문에 그 비용을 감수할 수 있습니다.\n당연하게도 하나의 인스턴스만 업데이트하는 경우에도 사용할 수 있습니다. 이 경우에는 다른 인스턴스들과 잘 호환이 되어야 합니다. 반면 전체 어플리케이션에 대해 할 경우 함께 설치되는 인스턴스들끼리의 호환성만 확인하면 됩니다. dependency가 복잡할 경우, 어플리케이션이 빠르게 변화하는 경우 사용해볼 수 있을 것입니다.\n          roll out roll out은 하나의 인스턴스에 대해 Pod 또는 VM이 여러개 떠있다고 가정합니다.\n이 때, 하나씩 순차적으로 새로운 버전으로 변경합니다. downtime을 줄이고 싶다면 업데이트하는 동안에는 트래픽을 흐르지 않도록 하면 될 것입니다.\n이런식으로 순차적으로 새로운 버전으로 변경하여 최종적으로는 모든 노드에 대해 업데이트가 완료될 것입니다.\ncanary와 마찬가지로 다른 버전의 인스턴스들과 잘 동작해야함을 보장해야 합니다. 그래야 두 버전이 동시에 사용되고 있다고 하더라도 정상적으로 어플리케이션이 동작하게 될 것입니다.\n          "
},
{
	"uri": "http://kimmj.github.io/kubernetes/concepts/pods/",
	"title": "Pods",
	"tags": ["kubernetes", "pod"],
	"description": "",
	"content": "Pod Overview Pod의 이해 Pod는 Kubernetes에서 가장 작은 배포 오브젝트이며 쿠버네티스에서 관리하는 최소 관리 단위입니다. Pod는 cluster 안에서 실행중인 어떤 프로세스를 의미합니다. application container, 스토리지 리소스, 유일한 network ip, container가 어떻게 실행할지를 캡슐화한 것입니다.\n각각의 Pod는 주어진 application에서 단일 인스턴스를 수행합니다. 즉, 한가지 역할을 맡고 있다고 생각하시면 됩니다.. 따라서 application을 수직확장하고 싶다면 각 인스턴스에 대해 여러 Pod를 생성하면 된다. 그러면 동일한 역할을 하는 Pod가 늘어나니, 병렬적으로 처리가 가능할 것입니다.\nPod는 서비스 중에서 서로 연관성이 높은 프로세스를 지원하기 위해 디자인되었습니다. container는 리소스와 의존성들을 공유하고, 서로 통신하며, 언제/어떻게 종료하는지에 대해 서로 조정합니다. Pod 내의 container는 Networking과 Storage를 공유할 수 있습니다.\nPod 안에는 둘 이상의 container가 있을 수 있습니다. 이 때 Pod 내에 여러 container를 두는 것은 container가 정말 강하게 결합될 때 입니다. 예를 들어, 하나의 container는 web server로 shared volume에서 파일을 가져와 호스팅하는 역할을 하고, 나머지 하나의 side-car container는 외부에서 file을 pulling하여 shared volume에 올리는 역할을 하는 것입니다. 이처럼 둘간의 결합성이 큰 경우(여기서는 shared volume일 것입니다) 동일한 Pod에 위치할 수 있습니다.\ninit container는 실제 app container가 시작하기 전에 먼저 작업을 하는 container입니다.\nNetworking 각 Pod 단위로 네트워크 IP 주소를 할당받습니다. 그 내부의 container끼리는 localhost로 통신하게 되고 외부와의 통신을 위해선 공유중인 네트워크 리소스를 어떻게 분배할 지 합의 및 조정해야합니다. 예를 들어 Pod 단위로 생각해보면 하나의 IP를 가지게 되고 expose할 port들을 가지게 될 것입니다. IP는 상관 없지만 port의 경우 특정 container로 binding이 되어야 하기 때문에 각 container는 동일한 port를 expose할 수 없습니다. Pod 관점에서 보면 어느 container로 전달해주어야 하는지 모르기 때문이죠.\nStorage Pod는 공유하는 저장공간을 volumes로 지정합니다. 이렇게 하면 하나의 container만 재시동되는 경우에도 데이터를 보존할 수 있습니다.\nPod로 작업하기 Pod를 사용할 때에는 kubectl create pods처럼 controller 없이 Pod를 생성하는 것은 좋은 생각이 아닙니다. 이렇게 할 경우 Kubernetes의 장점들을 충분히 활용할 수 없습니다. 특히 self-healing을 하지 못하기 때문에 Pod가 떠있던 노드에 장애가 발생하면 Pod는 영영 복구되지 않을 수 있습니다. 반면 controller로 관리할 경우 self-healing을 지원하여 노드에 장애가 발생해도 다른 노드에 해당 Pod를 띄워서 계속하여 서비스를 할 수 있습니다.\nPod의 재시작과 container의 재시작을 혼동하면 안됩니다. Pod는 그 자체만으로 동작하지 않습니다. 오히려 Pod는 삭제되기 전까지 계속 유지가 되는, container가 동작하는 환경이라고 보면 됩니다. Pod를 VM을 사용하는 상황과 비유를 해보자면 Pod는 VM에서 하나의 VM에 실행하는 application들처럼 하나의 논리적 호스트에서 container들을 실행하는 개념입니다.\nPod 내의 container들은 IP주소와 port space를 공유합니다. 그리고 서로 localhost로 통신할 수 있습니다. 반면에 다른 Pod에 있는 container와는 Pod에 할당된 IP를 사용하여 통신할 수 있습니다.\nPod는 container처럼 임시적인 자원이기 때문에 삭제시 reschedule이 아닌 새로운 동일한 spec의 Pod를 새로운 UID(Unique ID)로 생성합니다. 즉, Pod가 삭제되면 그 안에 있는 내용들을 잃어버리게 됩니다. VM에 빗대자면 VM을 생성하는 template을 가지고 새로운 VM을 생성하며, 삭제할 경우 해당 VM을 완전히 삭제한다는 개념입니다. 이 때 volume도 Pod가 삭제되면 삭제됩니다.\nPod Lifecycle Pod의 status 필드는 PodStatus의 phase 필드입니다. Pod의 phase는 간단하게 Pod가 위치한 lifecycle의 상위 개념에서의 요약정보입니다.\nphase 의 value들에는 다음과 같은 것들이 있습니다.\n Pending : Pod가 kubernetes 시스템에 의해 받아들여졌지만 하나 이상의 container image가 생성되지 않은 상태. Running : Pod가 node에 바운드되고 모든 container들이 생성됨. 최소한 하나의 container가 실행 중이거나 시작 또는 재시작 중. Succeeded : 모든 container가 성공적으로 종료되었고, 재시작되지 않음. Failed : 모든 container가 정료되었고, 최소 하나의 container가 failure 상태. Unknown : 어떤 이유로 인해 Pod의 state를 얻어낼 수 없음. 보통 Pod의 호스트와 통신이 안되는 문제.  Container probe kubelet이 container을 진단할 때 사용하는 것입니다. container에서 구현된 Handler를 호출하여 이러한 진단을 수행합니다.\n크게 3가지 probe가 있습니다.\n livenessProbe: container가 실행 중인지 나타냄. liveness probe가 실패하면 kubelet은 container를 죽이고, 이 container는 restart policy를 실행한다. redinessProbe: container가 service requests를 받을 준비가 되었는지 나타냄. readiness probe가 실패할 경우 endpoint controller는 Pod의 IP 주소를 Pod와 매칭되는 Service들의 endpoints에서 삭제한다. initial delay 이전의 default 값은 Failure이다. startupProbe: container 내부의 application이 시작되었는지를 나타냄. 다른 probe들은 startup probe가 성공할 때까지 비활성화 상태.  livenessProbe를 사용해야하는 상황 livenessProbe는 container가 제대로 동작하지 않은 경우 제대로 동작할 수 있을 때까지 재시동하는 목적으로 사용합니다. 따라서 container가 제대로 동작하지 않을 때 실행하는 프로세스가 이미 있다면 굳이 사용하지 않아도 됩니다. 원래 목적대로 재시동을 하고 싶다면 livenessProbe를 지정하고 restartPolicy를 설정합니다.\nreadinessProbe를 사용해야하는 상황 Pod에 request를 보내면 트래픽은 그 내부의 container로 전달됩니다. 이 때, 해당 container가 제대로 동작을 하지 않는다면, 파드에 request를 보냈을 때 비정상적인 응답을 할 것입니다.\n따라서 Pod가 제대로 응답을 보내줄 수 있는 상황에만 해당 Pod로 트래픽을 전달하고 싶다면, readinessProbe를 사용합니다. Kubernetes는 readinessProbe가 실패하면 Service와 연결된 Endpoint에서 해당 Pod를 삭제합니다. 그러면 Service로 흐른 트래픽은 redinessProbe가 실패한 Pod로 흐르지 않게 됩니다.\n단순히 삭제시 트래픽이 안흐르도록 하고 싶다면 굳이 할 필요는 없습니다. 알아서 삭제시 Service와 연결된 Pod의 Endpoint를 삭제하기 때문입니다.\nstartupProbe를 사용해야하는 상황 startupProbe는 위의 두 probe들과는 약간 다른 성격을 가졌습니다. livenessProbe와 함께 사용이 되는데요, container가 initialDelaySeconds + failureThreshold × periodSeconds만큼의 시간이 지난 후에 정상동작을 할 경우(container가 작업을 시작하기까지 시간이 오래 걸리는 경우) livenessProbe에 의해 fail이 발생하고, 재시작 되는것을 막아 deadlock 상태를 방지해줍니다.\nRestart policy PodSpec에서 restartPolicy 필드에는 Always, OnFailure, Never를 사용할 수 있습니다. 그 중 default는 Always입니다.\nrestartPolicy는 Pod 내의 모든 container에 적용됩니다. 또한 restartPolicy는 exponential back-off delay로 재시작됩니다. 즉, 10초, 20초, 40초로 계속해서 일정수준까지 delay가 늘어납니다. 성공적으로 실행되고 나서 10분이 지나면 해당 delay는 초기화됩니다.\nPod lifetime 일반적으로 Pod는 사람 또는 컨트롤러가 명백하게 이를 지우지 않는 이상 유지됩니다. control plane은 Pod의 총 개수가 지정된 threshold를 초과하면(node마다 정해져 있습니다) 종료된 Pod들(Succeeded 또는 Failed)을 삭제합니다.\n컨트롤러는 3가지 타입이 있습니다.\n Job: batch computations처럼 종료될것으로 예상되는 Pod입니다. ReplicationController, ReplicaSet, Deployment: 종료되지 않을 것으로 예상되는 Pod입니다. DaemonSet: 머신마다 하나씩 동작해야하는 Pod입니다.  Init Container Init container는 app image에서 사용할 수 없거나 사용하지 않는 setup script나 utility들을 포함할 수 있습니다. 실제 app container가 시작되기 전에 먼저 필요한 작업들을 수행하는데 사용됩니다.\nPod Specification에서 containers 배열과 같은 개위로 작성하면 Init container를 사용할 수 있습니다.\nInit container는 completion이 되기 위해 실행됩니다. 따라서 complete상태가 되면 재시작되지 않습니다. completion을 위해 실행되므로 당연하게도 readinessProbe는 사용할 수 없습니다.\ninit container는 여러개를 정의했을 경우 kubelet은 이를 순서대로 실행됩니다. 그리고 각 init container는 다음 init container가 실행되기 전에 반드시 성공적으로 종료되어야 합니다.\ninit container가 실패하면 성공할때까지 재시작합니다. 하지만 Pod의 restartPolicy가 Never이면 init container도 재시작하지 않습니다.\ninit container는 app container가 사용할 수 있는 대부분의 필드를 그대로 사용할 수 있습니다. 일반적인 container와의 차이점은 resource에 대해 다르게 관리된다는 것입니다. 자세한 내용은 공식 홈페이지의 docs를 확인하시기 바랍니다.\nInit container 사용하기  Init container는 app image에는 없는 utility나 custom code를 포함할 수 있습니다. Init container는 동일한 Pod 내에 있는 app container와는 다른 filesystem view를 가질 수 있습니다. 따라서 app container는 접근할 수 없는 Secret을 가지고 동작할 수 있습니다. Init container가 성공할 때까지 Pod의 app container들은 생성되지 않습니다. App container보다 안전하게 utility, custom code를 실행시킬 수 있습니다. 따라서 보안 취약점을 줄일 수 있습니다.  메인 app container를 실행할 때 필요한 configuration file에 필요한 value들을 주입할 때 init container를 사용할 수 있습니다.\nDetailed behavior Pod가 시작되는 동안 network와 volume들이 초기화 된 후 init container가 순서대로 실행되게 됩니다. 각 container는 반드시 다음 container가 실행되기 전까지 성공적으로 종료되어야 합니다.\nInit container에 대한 spec 변경은 container image에 대한것만 가능하다. 그리고 Init container는 idempotent1가 성립해야합니다.\nInit container가 실패 시 계속해서 재시작 되는 것을 막으려면 Pod에 activeDeadlineSeconds와 Container에 livenessProbe를 설정하면 막을 수 있습니다.\nResource 다루는 법  모든 init container에 대해 가장 높은 resource request나 limit은 effective init request/limit이라고 정의합니다. Pod의 effective request/limit은 다음보다 커야합니다.  모든 app container의 resource에 대한 request/limit의 합 resource에 대한 effective init request/lmit   effective request/limits를 기준으로 스케쥴링합니다. 즉, init container의 resource는 Pod의 life 동안 사용되지 않음을 의미합니다. Pod의 effective QoS tier에서 QoS tier는 init container와 app container의 QoS tier와 같습니다.  init container가 재시작되는 경우  user가 pod specification을 업데이트 하여 init container의 이미지가 변경되었을 경우입니다. App container image의 변화는 app container만 재시작시킵니다. Pod infrastructure container가 재시작 되었을 때 Init container가 실행됩니다. restartPolicy가 Always로 설정이 되어있는 상태에서 Pod가 재시작 되었을 때 init container가 이전 완료 상태를 저장한 것이 만료되거나 없을 경우 재시작될 수 있습니다.  Disruptions Pod는 원래 누군가가(사람 또는 컨트롤러) 지우지 않는다면, 또는 피할 수 없는 하드웨어, 소프트웨어적인 에러가 아니라면 삭제되지 않습니다.\n여기서 unavoidable인 경우를 involuntary disruptions라고 부릅니다. involuntary disruption에는 다음과 같은 것들이 있습니다.\n hardware failure cluster administrator가 실수로 VM을 삭제 cloud provider나 hypervisor의 장애로 VM이 삭제됨 kernel panic cluster network partition에 의해 node가 cluster에서 사라짐 노드가 out-of-resource여서 pod의 eviction이 실행됨  voluntary disruption은 application이나 cluster administrator에 의해 시작된 동작들입니다. voluntary disruption에는 다음과 같은 것들이 있습니다.\n 해당 Pod를 관리하고 있던 delployment나 다른 controller의 삭제 deployment의 Pod template update가 재시작을 유발함 직접적으로 Pod를 삭제  Cluster Administrator는 다음이 disruption을 유발할 수 있습니다.\n Upgrade를 위한 Draining Node cluster를 scale down 하기 위해 Draining Node 특정 노드에 띄워야 하는 요구사항 때문에 기존에 있던 해당 노드에서 Pod를 제거  Dealing with Disruptions  Pod에게 충분한 양의 resource 할당하기 고가용성을 원할경우 application을 복제하기 application을 rack또는 zone에 분배하기  How Disruption Budgets Work PodDisruptionBudget(PDB)를 각 application에 설정할 수 있습니다. 이는 voluntary disruption 상황에서 동시에 down될 수 있는 pod의 갯수를 제한합니다.\nPodDisruptionBudget(PDB)를 사용하려면 Cluster Manager는 Eviction API를 통해서 Pod를 삭제해야합니다. 즉, 직접 Pod나 Deployment를 삭제하게 되면 PDB를 사용하지 못하게 됩니다. Eviction API를 사용하는 예시에는 kubectl drain가 있습니다.\nPodDisruptionBudget(PDB)는 involuntary disruption 상황에서는 작동하지 않습니다. 하지만 몇개가 종료되는지는 기록하여 budget에 추가합니다.\nRolloing upgrade 때문에 Pod가 삭제되거나 사용 불가능한 상태일 때에도 PodDisruptionBudget(PDB)는 이를 카운트하지만 PDB때문에 제한되지는 않습니다. application의 업데이트 동안 발생한 장애처리는 controller spec에서 정의내린대로 실행합니다.\n  멱등법칙. 여러번 실행하더라도 동일한 결과를 냄. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "http://kimmj.github.io/css/greater-than-sign/",
	"title": "Greater Than Sign",
	"tags": ["css"],
	"description": "",
	"content": "\u0026gt;의 의미 \u0026gt;는 child-combinator 입니다.\n다음의 예시를 통해 정확히 어떤 역할을 하는지 알아보도록 하겠습니다.\n\u0026lt;div\u0026gt; \u0026lt;p class=\u0026#34;some_class\u0026#34;\u0026gt;Some text here\u0026lt;/p\u0026gt; \u0026lt;!--Selected [1] --\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;p class=\u0026#34;some_class\u0026#34;\u0026gt;More text here\u0026lt;/p\u0026gt; \u0026lt;!--Not selected [2] --\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;/div\u0026gt; 위와 같은 예시에서 div \u0026gt; p.some_class는 div 바로 밑에 있는 p.some_class만을 선택합니다. \u0026lt;blockquote\u0026gt;로 감싸진 p.some_class는 선택되지 않습니다.\n이와는 다르게 space만 사용하는 descendant combinator는 두개의 p.some_class 모두를 선택합니다.\n"
},
{
	"uri": "http://kimmj.github.io/hugo/insert-comment/",
	"title": "Hugo에 Comment 추가하기 (Utterance)",
	"tags": ["hugo", "utterance"],
	"description": "",
	"content": "댓글 서비스 선택 블로그를 운영하는데 관심을 가지기 시작하면서, 기본적으로 jekyll이나 hugo에는 댓글 기능이 없다는 것을 알게 되었습니다. static site를 만드는데 사실 댓글을 지원한다는게 이상한 상황이긴 하지요. 그래도 서드파티의 지원을 받으면 댓글 기능이 가능해집니다. 여러 블로그들을 탐방하며 git page 기능을 사용하는 블로그들에도 댓글이 있는것을 항상 봐왔으니까요.\n따라서 댓글을 어떻게 사용하는지 검색해보게 되었습니다. 대표적인 것이 Disqus 입니다. 실제로 많은 사이트들이 Disqus를 기반으로 댓글 기능을 사용합니다.\n저는 이 hugo 기반 블로그를 만드는 데 큰 도움을 준 https://ryan-han.com/post/etc/creating_static_blog/를 보고 Utterance에 대해 접하게 되었으며 개발자에게는 너무나도 친숙한 깃허브 기반이라는 점이 끌려서 이 서비스를 선택하게 되었습니다.\n적용 방법 적용하는 방법은 너무나도 쉽습니다.\n미리 댓글을 위한 레파지토리를 생각해둡니다(기존에 있는 레파지토리 혹은 댓글만을 위한 레파지토리). 저는 사이트를 렌더링해주는 kimmj.github.io 레파지토리로 선택하였습니다. https://utteranc.es/에 접속합니다. 중간쯤에 보이는 utterances app 하이퍼링크를 클릭합니다. 해당 앱을 적절한 레파지토리에 설치합니다. 설치한 레파지토리를 1번에서 접속한 사이트에 양식에 맞게 기입합니다. Blog Post ↔️ Issue Mapping 섹션에서 적절한 것을 선택합니다. 저는 default 옵션을 사용했습니다. Issue Label은 댓글로 생성된 이슈에 label을 달지, 단다면 어떤 label을 달지 선택하는 것입니다. 저는 comment로 작성했습니다. Theme을 선택합니다. 저는 default 옵션을 사용했습니다. 하단에 생성된 코드를 복사합니다. hugo에서 댓글이 보이게 될 위치에 8번에서 복사한 코드를 붙여넣습니다. 저의 경우 custom-comment.html이라는 파일에 붙여넣기 했습니다. 사이트에 제대로 뜨는지 확인합니다.  후기 이렇게 hugo에 댓글 기능을 추가하였습니다. 생각보다 너무나도 간단했네요. 이 댓글 기능을 하자고 생각하고 나서 적용까지 5분정도 걸렸던 것 같습니다. 댓글도 너무나 친숙한 레이아웃이라 정감이 가는 것 같네요.\nReference  https://github.com/Integerous/TIL/blob/master/ETC/Hugo%2BGithub_Page.md https://utteranc.es/  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/tools/tmux/",
	"title": "Tmux",
	"tags": ["tmux", "ubuntu"],
	"description": "",
	"content": "tmux란? tmux는 하나의 화면에서 여러개의 터미널을 키고싶을 때 사용하는 프로그램으로, ubuntu를 설치하면 기본적으로 설치되는 프로그램입니다.\n다음과 같은 구조를 가집니다.\ntmux ├── session │ ├── windows │ │ ├── pane │ │ └── pane │ └── windows │ ├── pane │ └── pane └── session ├── windows │ ├── pane │ └── pane └── windows ├── pane └── pane session 사용법 먼저 가장 큰 단위인 session을 다루는 방법부터 시작해보도록 하겠습니다.\nsession 생성 tmux 위처럼 tmux를 생성할 수 있습니다. 이 경우 tmux session의 이름은 0부터 차례로 증가하는 숫자로 정의됩니다.\n여기에 session의 이름을 사용자가 정의할 수도 있습니다.\ntmux new -s \u0026lt;my-session\u0026gt; 이렇게 이름을 지어놓으면 용도에 따른 session을 구분할 때 좋습니다.\nsession에 접속하기 이번에는 이미 생성된 session에 접속하는 방법입니다.\ntmux a tmux a -t \u0026lt;my-session\u0026gt; tmux attach tmux attach -t \u0026lt;my-session\u0026gt; -t 옵션을 줘서 이름을 지정할 수도 있고, (임의로 생성된 숫자 또한 마찬가지입니다.) 옵션없이 실행할 경우 가장 최근 열린 session으로 접속합니다.\nsession 확인하기 session어떤 것들이 있는지 보려면 ls를 이용하면 됩니다.\ntmux ls 또는 이미 session에 들어간 상태에서, session들의 리스트를 봄과 동시에 미리보기 화면으로 어떤 작업중이었는지도 볼 수 있습니다.\n[prefix] s 여기서 [prefix]는 일반적으로 Ctrl+b를 의미하며, 사용자에 의해 변경될 수 있습니다.\nsession에서 빠져나오기 session을 빠져나오는 방법에는 두가지가 있습니다.\n 완전히 session을 로그아웃하여 session 삭제하기 session이 계속 돌아가는 상태에서 빠져나오기  1번의 경우는 모든 pane에서 log out을 하면 되므로 생략하도록 하겠습니다. 두번째 session이 계속 돌아가는 상태에서 빠져나오는 방법은 다음과 같습니다.\n[prefix] d session 죽이기 tmux 바깥에서 session을 없애려면 들어가서 로그아웃을 통해 끄는 방법도 있을테지만, kill-session이라는 명령어를 통해서도 session을 없앨 수 있습니다.\ntmux kill-session -t \u0026lt;my-session\u0026gt; session 이름 바꾸기 session에 들어가 있는 상태에서 현재 session의 이름을 변경할 수 있습니다.\n[prefix] $ 그러면 상태표시줄에서 session의 이름을 정해줄 수 있습니다.\nwindows 사용법 windows는 session내의 tab과 같은 존재입니다. session은 큰 사용 목적으로 묶어준다면 windows는 그에따라 tab으로 관리할 필요가 있을 때 사용하면 좋습니다. 물론 session과 pane만 가지고 사용해도 되지만, 그보다 더 유연하게 하려한다면 windows도 알아두는 것이 좋습니다.\nwindows의 생성 windows를 관리하고자 한다면, 우선 session에 들어가 있는 상태여야 합니다.\n[prefix] c 위처럼 windows를 생성하고 나면 아래 tmux 상태표시줄에 0:bash- 1:bash*와 같은 형태로 windows가 추가됨을 볼 수 있습니다. 여기서 *는 현재 사용중인 windows를 의미합니다.\nwindows 움직이기 먼저 기본적으로 앞, 뒤로 움직이는 방법입니다.\n[prefix] n # next window [prefix] p # previous window session에서 미리보기를 사용하여 순회했던 것처럼, windows도 list들을 미리보기형식으로 순회할 수 있습니다.\n[prefix] w windows 이름 변경하기 특정 window에 들어가있는 상태에서 해당 window의 이름을 변경할 수 있습니다.\n[prefix] , windows 죽이기 현재 window를 죽이려면 로그아웃을 하는 방법도 있지만, 강제로 죽이는 방법도 존재합니다.\n[prefix] \u0026amp; panes 다루기 pane이란 tmux의 화면을 분할하는 단위입니다. tmux를 사용하는 가장 큰 이유라고 볼 수 있습니다.\npane 분할하기 [prefix] % # vertical split [prefix] \u0026#34; # horizontal split 위와같은 방법으로 pane을 생성할 수 있습니다.\npane 이동하기 먼저 기본적으로 방향키를 이용하여 움직일 수 있습니다.\n[prefix] \u0026lt;방향키\u0026gt; 일정시간이 지나면 pane내에서의 방향키 입력으로 전환되어 pane을 움직일 수 없으므로 빠르게 움직여줍니다.\npane 위치 바꾸기 pane들을 rotate하는 방법입니다.\n[prefix] ctrl+o # 시계방향으로 회전 [prefix] alt+o # 반시계방향으로 회전 또는 하나를 이동할 수도 있습니다.\n[prefix] { # move the current pane left [prefix] } # move the current pane right pane 크게 보기 pane을 여러개 쓰다가 하나만 크게 보고싶은 경우가 있을 수 있습니다.\n[prefix] z 돌아가는 방법 또한 같은 명령어를 통해 할 수 있습니다.\n[prefix] z pane을 새로운 window로 분할하기 특정 텍스트를 마우스로 복사하거나 여러가지 상황에서 새로운 window로 분할하는 것이 편한 경우가 있습니다.\n[prefix] ! 모든 pane에서 동시에 입력하기 tmux를 통해서 여러개의 서버에 ssh 접속을 한 뒤, 동시에 같은 입력을 하게 할 수도 있습니다.\n[prefix] :set synchronize-panes yes (on) [prefix] :set synchronize-panes no (off) Tips tmux에는 기본 내장된 layout이 있습니다. 이를 통해서 pane들을 resize하지 않고 기본 형식에 맞게 쉽게 변경이 가능합니다. 그 중 가장 자주 사용할 수 있는 것은 다음 두가지입니다.\n[prefix] alt+1 # 수직 [prefix] alt+2 # 수평 Reference  https://gist.github.com/MohamedAlaa/2961058 https://gist.github.com/andreyvit/2921703  "
},
{
	"uri": "http://kimmj.github.io/prometheus/federation/",
	"title": "Federation",
	"tags": ["prometheus", "federation"],
	"description": "",
	"content": "What is Federation 영어 의미 그대로는 \u0026ldquo;연합\u0026quot;이라는 뜻입니다. 즉, Prometheus의 Federation은 여러개의 Prometheus에서 Metric을 가져와 계층구조를 만드는 것을 의미합니다.\n위의 그림에서 너무나도 잘 표현이 되어 있습니다. 그림에서 보시면 상위에 있는 Prometheus에서 하위의 Dev, Staging, Production쪽으로 화살표가 간 것을 볼 수 있습니다. 이는 아래에 있는 Prometheus가 http(s)://\u0026lt;url\u0026gt;/federation으로 보여주는 Metric들을 위쪽에 있는 Prometheus에서 scrape하기 때문입니다.\n저의 상황을 설명해드리고 지나가도록 하겠습니다. 저는 Kubernetes Cluster가 Dev(Canary), Staging, Production과 비슷하게 3개 있었습니다. 여기서 Spinnaker를 통해 Dev에 새로운 이미지들을 배포할 것이고, 이에 대한 Metric을 Canary Analysis를 통해 분석하여 Dev로 배포된 이미지가 이전 Staging의 이미지와 어떻게 다른지 등을 점수화하여 Staging 서버에 배포를 할지 말지 결정하도록 해야하는 상황이었습니다.\n이 때, Spinnaker의 설정상 Prometheus를 연동하고 나서 Canary Config를 설정할 때 하나의 Prometheus만 바라보도록 할 수 있었습니다. 따라서 여러대의 Promethus의 Metric을 비교하기 위해서는 여러대의 Prometheus가 가지고 있는 Metric을 상위개념의 Prometheus가 scrape하도록 해야했습니다. 어떻게 해야하는지 검색해본 결과 Federation이라는 기능이 있는 것을 알게 되었습니다.\nHow to configure Prometheus Federation Prerequisites 우선 여러대의 Prometheus가 필요합니다. 그리고 이를 하나로 모아줄 또 다른 Prometheus가 필요합니다.\n저의 경우 docker-compose를 통해 Prometheus를 구동하여 다른 서버의 Prometheus Metric을 가져오도록 설정하였습니다. Install에 관해서는 다른 문서를 참고해 주시기 바랍니다.\nConfiguring federation 다음은 공식 사이트에 나온 federation의 구성입니다.\nscrape_configs: - job_name: \u0026#39;federate\u0026#39; scrape_interval: 15s honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; params: \u0026#39;match[]\u0026#39;: - \u0026#39;{job=\u0026#34;prometheus\u0026#34;}\u0026#39; - \u0026#39;{__name__=~\u0026#34;job:.*\u0026#34;}\u0026#39; static_configs: - targets: - \u0026#39;source-prometheus-1:9090\u0026#39; - \u0026#39;source-prometheus-2:9090\u0026#39; - \u0026#39;source-prometheus-3:9090\u0026#39; job_name job_name은 static_configs[0].targets에 적힌 Prometheus Metric에 어떤 job=\u0026quot;\u0026lt;job_name\u0026gt;\u0026quot;을 줄지 결정하는 것입니다. 이를 통해 저는 Canary, Stage 서버를 구분하였습니다.\nscrape_interval 얼마나 자주 Metric을 긁어올 지 결정하는 것입니다.\nmetrics_path 어떤 path에서 Metric을 가져오는지 설정합니다. 보통의 경우 federation으로 설정하면 됩니다.\nparams 실제로 \u0026lt;PrometheusUrl\u0026gt;/federation으로 접속해보면 아무것도 뜨지 않습니다. /federation은 param로 매칭이 되는 결과만 리턴하며, 없을 경우 아무것도 리턴하지 않습니다. 따라서 이는 필수 필드이고, 원하는 job만 가져오게 하거나 {job=~\u0026quot;.+\u0026quot;}와 같은 방법으로 모든 Metric을 가져오게도 할 수 있습니다.\nstatic_configs 어떤 Prometheus에서 Metric을 가져올지 결정하는 것입니다.\nValidation 위와같이 설정을 한 뒤, Prometheus에 접속하여 Targets에 들어가 봅니다. 리스트에 설정한 job_name들이 떠있고, UP인 상태로 있으면 정상적으로 구성이 된 것입니다.\nReference https://prometheus.io/docs/prometheus/latest/federation/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/canaryanalysis/canary-analysis/",
	"title": "Canary Analysis",
	"tags": ["canary", "canary-update", "spinnaker"],
	"description": "",
	"content": "Spinnaker Canary Analysis Spinnaker에는 Canary Analysis라는 자동 분석 도구가 있습니다. Kayenta라는 micro service를 사용하는데, 이를 통해 자동으로 canary deploy가 괜찮은 버전인지를 확인해 줍니다.\n그러나 이 툴은 Spinnaker에서 사용하기에 여간 어려운 것이 아닙니다. 제일 먼저 봉착하는 난관은 바로 \u0026ldquo;어떻게 Canary Analysis를 활성화 하는가?\u0026ldquo;입니다.\n이곳에 방법이 나와있지만, 사실 저도 엄청 많이 헤멨습니다. 저는 bare-metal 환경에서 Kubernetes cluster를 구축하였었고, aws나 azure, gcp는 사용하지 못하는 상황었습니다. (물론 지금도 집에서 VM으로 로컬에 구성하였지만, cloud platform은 언제나 과금때문에 꺼려지게 됩니다.)\n이런 상황에서 어떻게 Canary Analysis를 활성화했는지부터, 어떻게 이를 통해 Metric을 비교하는지까지 한번 알아보도록 하겠습니다.\nPrerequisites 첫번째로 metric service를 선택해야 합니다. 여러가지가 있을 수 있겠지만, 저는 로컬에서 사용할 수 있는 Prometheus를 사용할 것입니다.\n두번째로 가져온 metric의 결과들을 저장해 놓을 storage service가 필요합니다. 저는 Spinnaker를 구성할 때 minio를 storage service로 구축을 했었으므로, 여기에서도 마찬가지로 minio를 통해 데이터를 저장할 것입니다.\nHow to enable Canary Analysis 제일 먼저 hal command를 통해서 Canary Analysis를 활성화시켜야 합니다.\nhal config canary enable 그다음엔 Prometheus를 canary analysis에 사용되도록 설정할 것입니다.\nhal config canary prometheus enable hal config canary prometheus account add my-prometheus --base-url http://192.168.8.22/9090 이처럼 Prometheus 콘솔창이 보이는 url을 입력하면 됩니다. 저는 docker-compose를 통해서 9090 port로 expose 시켰으므로 위와같이 적어주었습니다.\n그 다음에는 metric provider를 설정합니다. 앞서 말했듯이 여기서는 Prometheus를 사용할 것입니다.\nhal config canary edit --default-metrics-store prometheus hal config canary edit --default-metrics-account my-prometheus 위의 두 설정으로 Prometheus가 default metric store로 선택되었습니다. 이는 물론 나중에 canary configuration에서 원하는 것으로 선택할 수도 있습니다. 또한 default account도 my-prometheus라는 이름으로 선택해 주었습니다.\n이번엔 default storage account를 설정할 것입니다. 공식 docs에서는 minio에 관련된 설정방법이 잘 나와있지 않습니다.\n하지만 hal command를 잘 보시면 어떻게 해야할지 감이 약간은 잡히실 것입니다.\n--api-endpoint에는 minio의 url을 적고, --aws-access-key-id에는 minio의 ID였던 minio를 입력합니다. 그리고 --aws-secret-access-key는 minio의 PW인 minio123을 입력합니다.\nhal config artifact s3 account add my-minio \\  --api-endpoint http://192.168.8.22:9000 \\  --aws-access-key-id minio \\  --aws-secret-access-key minio123 그 뒤에는 위에서 입력했던 my-minio storage account를 Canary Analysis에 사용하도록 설정하면 됩니다.\nhal config canary edit --default-storage-account my-minio Validate 제대로 설정을 마쳤으면 pipeline의 config에서 Feature 탭에 Canary가 추가된 것을 볼 수 있습니다. 이를 활성화하면 본격적으로 Canary Analysis를 사용할 수 있습니다.\nReference https://www.spinnaker.io/guides/user/canary/\nhttps://www.spinnaker.io/guides/user/canary/config/\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/change-hostname/",
	"title": "Hostname 변경하기",
	"tags": ["hostname", "ubuntu"],
	"description": "",
	"content": "hostname을 바꾸는 일은 흔치 않지만 최초 셋업할 때 많이 사용하곤 합니다.\n# hostnamectl set-hostname \u0026lt;host name\u0026gt; hostnamectl set-hostname wonderland 변경 후 터미널을 끄고 재접속을 하면 변경된 사항을 볼 수 있습니다.\nhostname "
},
{
	"uri": "http://kimmj.github.io/hugo/ibiza/font-change/",
	"title": "Font Change",
	"tags": ["font", "hugo"],
	"description": "",
	"content": "Ibiza 프로젝트를 진행하는데 폰트가 마음에 들지 않았습니다. 따라서 저는 폰트를 변경하기로 마음먹었습니다.\n먼저, 폰트 설정을 어디서 하는지 알아낼 필요가 있었습니다.\nfind . | grep font 결과를 보니, theme 폴더 안에 제가 사용하는 hugo-theme-learn 테마에서 static/fonts/ 폴더에 폰트들을 저장해두고 있었습니다. 그렇다면 어느 파일에서 어떤 폰트를 사용한다고 설정할까요?\nhugo-theme-learn폴더로 이동하여 어디에 사용되는지 확인해보았습니다.\ngrep -ri \u0026#34;font\u0026#34; 결과가 길게 나오는데요, 여기서 static/css/theme.css 안에 폰트에 대한 설정을 한 것이 보였습니다. 그 파일을 보니, @font-face라는 설정이 보이네요. 여기서 Work Sans라는 폰트를 불러오고 있었습니다.\n이 폰트를 Noto Sans CJK KR이라는 폰트로 바꾸려고 합니다. 따라서 먼저 폰트를 다운로드 받아야 합니다.\n다운로드 페이지 : https://www.google.com/get/noto/#sans-kore\n여기에서 다운로드 버튼을 눌러 폰트를 다운받습니다.\ncurl -o noto-mono.zip https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKkr-hinted.zip 이를 my-custom-theme 폴더 내의 static/fonts 폴더 안에다가 압축해제할 것입니다.\nmv noto-mono.zip mj-custom-theme/static/fonts unzip noto-mono.zip rm noto-mono.zip README 그러고나서 font-face 설정을 바꾸어 보도록 하겠습니다. 처음에는 이 설정으로 폰트가 정말 바뀌는지 확인해보기 위해 현재 사용중인 폰트의 url 부분을 Noto Mono 폰트로 변경해보았습니다.\n@font-face { font-family: \u0026#39;Work Sans\u0026#39;; font-style: normal; font-weight: 500; src: url(\u0026#34;../fonts/NotoSansMonoCJKkr-Bold.otf?#iefix\u0026#34;) format(\u0026#34;embedded-opentype\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Bold.otf\u0026#34;) format(\u0026#34;woff\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.svg#WorkSans\u0026#34;) format(\u0026#34;svg\u0026#34;), url(\u0026#34;../fonts/Work_Sans_500.ttf\u0026#34;) format(\u0026#34;truetype\u0026#34;); } 하지만 예상과 다르게 변경되지 않았습니다. 확인해보니 이는 font-weight이라는 설정때문이었습니다. body에 대한 font-weight은 300으로 설정이 되어있었고, 따라서 제가 설정한 폰트가 아닌 font-weight: 300인 폰트를 선택하게 된 것입니다.\n다시한번 body쪽의 font-weight을 500으로 바꾸어 실험해보았습니다.\nbody { font-family: \u0026#34;Work Sans\u0026#34;, \u0026#34;Helvetica\u0026#34;, \u0026#34;Tahoma\u0026#34;, \u0026#34;Geneva\u0026#34;, \u0026#34;Arial\u0026#34;, sans-serif; font-weight: 500; line-height: 1.6; font-size: 18px !important; } 그러자 제가 원하는 폰트로 변경이 된 것을 확인하였습니다. 다시 위로 돌아가서 @font-face 설정을 저의 폰트 이름으로 변경하고 제가 원래 하려던 폰트로 변경하였습니다. font-weight: 300으로 다시 돌려놓았고, 새로운 폰트를 font-weight: 300으로 주었습니다.\n@font-face { font-family: \u0026#39;Noto Mono Sans CJK KR \u0026#39;; font-style: normal; font-weight: 300; src: url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.eot?#iefix\u0026#34;) format(\u0026#34;embedded-opentype\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.woff\u0026#34;) format(\u0026#34;woff\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.svg#NotoSansMonoCJKkr\u0026#34;) format(\u0026#34;svg\u0026#34;), url(\u0026#34;../fonts/NotoSansMonoCJKkr-Regular.ttf\u0026#34;) format(\u0026#34;truetype\u0026#34;); } 여기서 보시면 format 속성들이 많은 것을 볼 수 있습니다. 이는 브라우저별로 지원하는, 지원하지 않는 폰트들에 대해서 처리를 해주기 위한 것입니다. 저는 폰트를 다운로드 받았을 때 otf 포멧밖에 없었습니다. 따라서 다른 포멧들로 변경해 줄 필요가 있었습니다.\nfont converter : https://onlinefontconverter.com/\n위 사이트에서 제가 필요한 eot, woff, woff2, svg, ttf 파일들로 변환후 저장했습니다.\n이제 body의 css에서 font-family 맨 앞에 앞서 정의한 폰트를 지정해줍니다.\nbody { font-family: \u0026#34;Noto Sans Mono CJK KR\u0026#34;, \u0026#34;Work Sans\u0026#34;, \u0026#34;Helvetica\u0026#34;, \u0026#34;Tahoma\u0026#34;, \u0026#34;Geneva\u0026#34;, \u0026#34;Arial\u0026#34;, sans-serif; font-weight: 300; line-height: 1.6; font-size: 18px !important; } 확인해보니 제대로 적용이 되었네요.\nbefore font change after font change Reference  https://wit.nts-corp.com/2017/02/13/4258 https://aboooks.tistory.com/153 https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/webfont-optimization?hl=ko  "
},
{
	"uri": "http://kimmj.github.io/ubuntu/network/netplan/",
	"title": "Netplan으로 static IP 할당받기",
	"tags": ["ubuntu-18.04", "netplan", "static-ip"],
	"description": "",
	"content": "유선 static IP 할당 다음과 같이 static-IP-netplan.yaml을 작성합니다.\nnetwork: version: 2 ethernets: enp3s0: dhcp4: no dhcp6: no addresses: [ 192.168.1.26/24 ] gateway4: 192.168.1.1 nameservers: addresses: [ 8.8.8.8, 8.8.4.4 ] 하나씩 살펴보도록 하겠습니다.\n ethernetes: 유선랜 설정입니다. enp3s0 설정을 사용할 랜카드입니다. dhcp4, dhcp6: dynamic으로 IP를 할당받는 dhcp를 disable한 것입니다. addresses: 사용할 IP 및 CIDR입니다. gateway4: IP가 사용하는 gateway입니다. nameservers: dns 주소입니다. 8.8.8.8과 8.8.4.4를 사용합니다.  WIFI static IP 할당 wifis: wlp2s0: dhcp4: no dhcp6: no addresses: [ 192.168.8.26/24 ] gateway4: 192.168.8.30 nameservers: addresses: [ 8.8.8.8, 8.8.4.4 ] access-points: \u0026#34;my-SSID\u0026#34;: password: \u0026#34;my-password\u0026#34; 하나씩 살펴보도록 하겠습니다.\n wifis: wifi에 대한 설정입니다. wlp2s0: 무선 랜카드입니다. access-points: 사용할 wifi에 대한 설정입니다. \u0026quot;my-SSID\u0026quot;: wifi의 SSID 즉, wifi 이름입니다. password: 해당 wifi의 비밀번호를 적으면 됩니다.  만약 ubuntu server를 사용한다면, wifi 관련 패키지는 초기에 설치되지 않습니다. 따라서 wpasupplicant를 설치해주어야 합니다.\nsudo apt install wpasupplicant "
},
{
	"uri": "http://kimmj.github.io/hugo/hugo-with-html/",
	"title": "HUGO로 HTML이 되지 않을 때 가능하게 하는 방법",
	"tags": ["hugo", "html"],
	"description": "",
	"content": "Hugo는 markdown을 기본적으로 사용하지만 html을 이용해서 좀 더 다양하게 커스터마이징이 가능한 장점도 가지고 있습니다.\n하지만 저는 처음에 html 코드를 사용하게 되면 \u0026lt;!-- raw HTML omitted --\u0026gt;와 같은 줄로 대치가 되곤 했습니다. 구글링 결과 이는 Hugo의 버전이 0.60.0으로 되면서부터 기본적으로 disable 시켰기 때문입니다.\n따라서 다음과 같이 조치를 하면 간단하게 해결이 가능합니다.\n[markup.goldmark.renderer] unsafe= true 위와 같은 설정을 config.toml에 추가하기만 하면 됩니다. 추가를 한 뒤 다시 확인해보면 정상적으로 html 코드가 적용된 모습을 볼 수 있습니다.\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/tips/pipeline-expressions/",
	"title": "Pipeline Expressions",
	"tags": ["spinnaker", "pipeline"],
	"description": "",
	"content": "Spinnaker는 배포를 자동화할 때 사용합니다. 그렇기 때문에 자동화를 위해선 다른 곳에서 사용된 값들을 가지고 와야할 필요성이 생기기도 합니다.\n이 문서에서는 그럴 때 사용할 수 있는 pipeline function에 대해 알아보도록 하겠습니다.\npipeline functions pipeline에서 다른 pipeline의 값들 불러오기 Note: Pipeline expression syntax is based on Spring Expression Language (SpEL).\n 위의 Note에도 적었듯이, Spinnaker는 SpEL을 기반으로 Expressions를 사용합니다. SpEL에 대해 이미 잘 알고있다면 너무나도 좋겠지만, 저는 익숙하지가 않았기 때문에 많은 시행착오를 거쳐서 습득을 하게 되었습니다.\n기본적으로 ${ expression }의 형태를 가지게 됩니다.\n여기서 한가지 기억해 두어야 할 것은 nested가 되지 않는다는 것입니다. 즉, ${ expression1 ${expression2} }가 되지 않습니다.\n언제 pipeline expression을 사용하나요 pipeline expression은 Spinnaker UI로는 해결할 수 없는 문제들을 해결하여줍니다. 예를 들어 특정 stage가 성공했는지의 여부에 따라 stage를 실행할지, 말지 결정하는 방법을 제공해 줍니다. 또는 가장 최근에 deploy된 pod를 알아낸다거나, spinnaker를 통한 canary analysis를 할 때 비교할 두가지 대상을 선택하기 위해 사용할 수도 있습니다.\nSpinnaker는 모든 파이프라인을 JSON 형태로도 관리할 수 있기 때문에, UI에는 없는 값들도 입력할 수 있습니다. 이렇게 좀 더 유연한 방법으로 Spinnaker를 이용하고자 한다면 pipeline expression은 꼭 알아두어야 합니다.\n원하는 값을 어떻게 찾나요 pipeline이 구동되고 나면, Details를 누르고 Source를 눌렀을 때 해당 pipeline의 실행결과가 json형태로 출력됩니다. 이를 VS Code나 다른 편집기를 이용하여, json으로 인식하게 한 뒤, 자동 들여쓰기를 하면 보기 좋게 만들어줍니다.\n이를 통해서 어떤 값을 내가 사용할 지 확인하여 pipeline expression을 작성하면 됩니다.\n내가 작성한 pipeline expression은 어떻게 테스트하나요 작성한 pipeline expression을 테스트하기 위해 파이프라인을 구동한다는 것은 끔직한 일입니다. Spinnaker는 이를 테스트하기 위해 API endpoint를 제공합니다. 즉, 파이프라인을 다시 구동시키지 않고도 어떤 결과값이 나오는지 확인할 수 있다는 것을 의미합니다.\n테스트 방법은 간단합니다. 다음과 같이 curl을 통해 endpoint로 테스트하면 됩니다.\nPIPELINE_ID=[your_pipeline_id] curl http://api.my.spinnaker/pipelines/$PIPELINE_ID/evaluateExpression \\  -H \u0026#34;Content-Type: text/plain\u0026#34; \\  --data \u0026#39;${ #stage(\u0026#34;Deploy\u0026#34;).status.toString() }\u0026#39; 여기서 api.my.spinnaker는 Gate의 Service를 보고 포트를 참조하여 작성하면 됩니다. 기본값은 localhost:8084입니다. 이렇게 하면 Deploy라는 stage가 성공했을 때 다음과 같은 결과를 볼 수 있습니다.\n{\u0026#34;result\u0026#34;: \u0026#34;SUCCEEDED\u0026#34;} Spinnaker가 expression을 통해 결과를 만들어내지 못한다면 다음과 같이 에러와 로그가 발생합니다.\n{ \u0026#34;detail\u0026#34;: { \u0026#34;{ #stage(\\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Failed to evaluate [expression] Expression [{ #stage( #root.execution, \\\u0026#34;Deploy\\\u0026#34;).status.toString() ] @0: No ending suffix \u0026#39;}\u0026#39; for expression starting at character 0: { #stage( #root.execution, \\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34;, \u0026#34;exceptionType\u0026#34;:\u0026#34;org.springframework.expression.ParseException\u0026#34;, \u0026#34;level\u0026#34;:\u0026#34;ERROR\u0026#34;, \u0026#34;timestamp\u0026#34;:1531254890849 } ] }, \u0026#34;result\u0026#34;:\u0026#34;${#stage(\\\u0026#34;Deploy\\\u0026#34;).status.toString() \u0026#34; } Reference Spinnaker Docs: https://www.spinnaker.io/guides/user/pipeline/expressions/\n"
},
{
	"uri": "http://kimmj.github.io/spinnaker/tips/",
	"title": "Tips",
	"tags": [],
	"description": "",
	"content": "Spinnaker Tips spinnaker를 운영하며 생기는 팁들을 모아보았습니다.\n Pipeline Expressions     "
},
{
	"uri": "http://kimmj.github.io/ansible/create-vm-with-ansible-libvirt/",
	"title": "Create Vm With Ansible Libvirt",
	"tags": ["ansible", "libvirt"],
	"description": "",
	"content": "Ansible은 어떠한 프로세스를 자동화 할 때 사용할 수 있는 툴입니다. 그리고 libvirt는 linux 환경에서 qemu를 이용하여 VM을 생성할 때 사용하는 python 모듈입니다.\n이 두가지를 합하여 Ansible을 통해 VM을 생성하는 방법에 대해 알아보도록 하겠습니다.\nansible-role-libvirt-vm 참조 Github : https://github.com/stackhpc/ansible-role-libvirt-vm\n위의 Github 프로젝트는 libvirt를 ansible에서 사용할 수 있도록 만든 오픈소스입니다. 이를 이용하여 ansible-playbook을 통해 VM을 생성해 볼 것입니다.\n이를 로컬에 clone 합니다.\ngit clone https://github.com/stackhpc/ansible-role-libvirt-vm 테스트 환경 저는 Ubuntu 18.04.3 Desktop을 사용하고 있습니다. 그리고 설치에 사용될 iso는 제 포스트에서 작성한 적이 있었던 preseed.cfg를 이용한 자동 설치 이미지입니다. 따라서 이미지를 넣고 부팅만 하면 실행할 수 있습니다.\nplay.yaml  저는 이러한 play.yaml 파일을 사용하였습니다.\n여기서 cdrom을 사용하였는데, 이미지는 baked-ubuntu.iso를 사용하였습니다.\n또한 장비들에 대한 설정을 xml로 추가적으로 하고싶어서 xml_file을 설정해 주었습니다.\nxml_file또한 업로드 해두었습니다.\n 네트워크는 설정을 빼놓을 경우 설치중에 확인창이 발생하여 기본적으로 NAT를 사용하도록 하였습니다. 이는 필요에 따라 변경을 해야 합니다. 또한 enable_vnc의 경우 virt-manager를 통해 상황의 경과를 확인하고 싶어서 추가하였습니다.\n위의 파일들을 workspace에 두시면 됩니다.\nTest 이렇게까지 한 뒤 play.yaml이 있는 위치에서 시작합니다.\n그러면 ansible-playbook은 ansible-role-libvirt-vm이라는 role을 해당 위치에서 검색하고, 실행이 될 것입니다.\nansible-playbook play.yaml 실행 중 sudo 권한이 필요하다고 할 수도 있습니다. 이럴 경우 sudo su로 잠시 로그인 후 exit로 빠져나오시면 에러가 발생하지 않습니다.\n확인 virt-manager를 통해 GUI 환경에서 실제로 잘 되고 있는지 확인할 수 있습니다.\nvirt-manager preseed.cfg를 사용한 이미지라면 30초 후 설치 언어가 자동으로 영어로 설정이 되면서 계속해서 설치가 진행될 것입니다.\n마치며 vm을 생성하는 일이 잦다면, 이 또한 굉장히 귀찮은 일이 아닐 수 없습니다. 소규모가 아닌 대규모로의 확장성을 생각한다면 당연히 자동화를 하는 것이 올바른 접근이라고 생각합니다.\nVM 설치 자동화의 방법이 여러가지가 있을 것이고 이 방법 또한 그 여러가지 방법 중 하나입니다.\n더 좋은, 더 편한 방법이 있다면 알려주시면 감사하겠습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/unattended-ubuntu/",
	"title": "추가 입력절차(prompt) 없이 Ubuntu 설치하는 이미지 만들기",
	"tags": ["ubuntu-18.04-server", "preseed.cfg"],
	"description": "",
	"content": "어디에 좋을까 Ubuntu Server를 설치하기 위해서는 많은 추가 입력이 있어야 합니다. 사용자가 어떻게 설치하기를 원하는지 모르기 때문에, 또 다양한 옵션을 사용자가 선택하기 위해서는 어찌보면 당연한 것이겠지요. 하지만 만약 똑같은 설정을 사용할 것인데, 여러대의 서버에 OS를 설치하는 상황이라고 생각해보면 정말 암울합니다. 온전히 시간을 OS 설치에만 투자하자니 이건 간단한 업무로 인해 다른 업무를 보지 못하게 됩니다. 또 다른 업무와 동시에 하자니 다음 입력창이 뜰 때인지 한번씩 확인해 주어야 합니다.\n따라서 어차피 같은 설정을 한다면, 이러한 설정을 미리 해 놓는 방법이 Ubuntu iso 파일 내부에 있을 것이라고 추측했습니다. 분명 누군가가 이런 불편함을 해결했으리라 생각했죠. 다행이 몇번의 구글링을 통해 preseed.cfg라는 파일이 제가 말했던 사용자의 입력을 미리 정해놓는 파일이라는 것을 알 수 있었습니다.\n이 preseed.cfg 파일을 잘만 활용한다면, 서버에 OS를 설치할 때 불필요한 시간 낭비를 줄일 수 있을 것입니다.\n차라리 VM이었다면, 그냥 VM을 복사해서 IP나 MAC, hostname 같은 것들만 변경해도 됐을 수 있습니다. 하지만 preseed.cfg를 이해하게 되면 언제 어디서든 내가 원하는 설정을 해주는 우분투 설치 파일을 만들 수 있을 것입니다.\n사전 준비 먼저, 설정을 넣어줄 Ubuntu 18.04 Server가 필요합니다. 물론 Ubuntu 18.04 Desktop에도 적용이 될 것으로 보입니다. (검색했을 때 대부분이 Desktop 설치 이미지에 관한 내용이었으니까요.)\n여기서 중요한 점은 live라고 적혀있는 이미지가 아니어야 합니다. live가 붙은 것은 인터넷으로 파일들을 다운로드 받게 되고, 그럴 경우 오프라인 설치가 필요한 환경에서는 적합하지도 않고 작업할 때 필요한 파일 또한 없습니다.\n두번째로 중요한 점은 amd64입니다. 처음에 잘못받고 arm64를 다운받았었는데, 내부 파일들의 폴더 명도 다르고 동작방식도 달라 구글링을 통해 amd64 이미지를 따로 받았습니다.\npreseed.cfg 작성 제가 설정했던 preseed.cfg 파일은 다음과 같습니다.\n 설명은 후에 추가하도록 하겠습니다.\niso 파일 생성하기 크게 순서를 정한다면 이렇게 됩니다.\n initrd.gz를 압축해제한 뒤 preseed.cfg 관련 정보를 initrd.gz에 추가 다시 initrd.gz로 압축 md5sum을 통한 checksum 재생성 genisoimage를 통한 부팅용 이미지 생성  그러나 preseed.cfg를 수정할 때마다 이를 반복하는 것은 여간 귀찮은 일이 아닐 수 없습니다. 그래서 저는 이를 bakeIsoImage.sh이라는 간단한 shell 프로그램으로 만들어서 iso파일을 생성하도록 하였습니다.\n 설치 테스트 위의 방식대로 진행을 했다면 baked-ubuntu.iso라는 파일이 생성되었을 것입니다. 이를 virt-manager나 virtual box등을 통해 가상머신을 생성하여 설치 테스트를 합니다.\n설치를 하면서 아무런 입력을 하지 않았다면, 원래의 의도대로 잘 설치가 된 것이라고 볼 수 있겠네요.\n마치며 preseed.cfg라는 엄청나게 유용한 방법이 있음에도 불구하고, 공식적인 가이드 자체가 많이 없는 상황입니다. 어떤 옵션들이 있는지도 잘 모르고, 설명도 자세히 되어있지 않았습니다. 단지 주어진 것이라고는 공식 문서에서 예시로 제공하는 preseed.cfg 파일 하나와, 다른사람들이 작성해 놓은 파일들 뿐이었습니다.\n저 또한 입력없이 설치하는 우분투 설치 이미지를 만들기 위해 고군분투했습니다. 누군가가 이 글을 통해서 환경에 맞는 설정을 해주는 우분투 설치 이미지를 생성하여 자동화를 할 수 있게된다면 정말 좋을 것 같습니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/how-to-edit-boot-parameter-during-install/",
	"title": "Ubuntu 설치 시 Boot Parameter를 수정하기",
	"tags": ["ubuntu", "install", "boot parameter"],
	"description": "",
	"content": "Ubuntu 설치할 때 boot parameter가 필요한 상황이 간혹 발생할 수 있습니다.\n특히 저의 경우, preseed.cfg를 수정하기 위해 인스톨러가 질의하는 것이 preseed.cfg의 어떤것과 대응이 되는지를 보기 위해 DEBCONF_DEBUG=5라는 옵션을 boot parameter로 주어야 했습니다. 이 때 사용할 수 있는 방법을 소개드립니다.\n먼저 평소와 같이 ubuntu를 설치하기 위해 설치 이미지를 삽입합니다. 그 다음에는 언어를 선택하시면, 다음으로 넘어가기 전에 메뉴가 뜹니다.\n이 상태에서 F6을 누르시면 옵션을 선택할 수 있고, 이 때 ESC키를 누르면 boot parameter가 하단에 보일 것입니다. 여기서 원하는 boot parameter를 입력하면 됩니다.\n이 때, 위아래 방향키를 누르게 되면 입력했던 내용이 사라지게 됩니다. 따라서 미리 맨 위 install ubuntu에 커서를 올리고 수정하시기 바랍니다.\ninstall 시에 설정으로 넣어버리기 preseed.cfg로 미리 질문에 대한 답을 다 정할 수 있었듯이, boot parameter 또한 미리 설정할 수 있습니다. 해당 파일은 iso 파일을 압축해제 하였을 때, /isolinux/txt.cfg 파일 내에 있습니다.\ngrep -ri \u0026#39;initrd\u0026#39; . 이렇게 검색해 보았을 때 quiet ---이라고 적힌 것들이 있는데, --- 뒤에가 boot parameter로 쓰이는 것들입니다.\nvim으로 /isolinux/txt.cfg 파일을 열고 원하는 설정을 기입하면 됩니다.\n이렇게 원하는 boot parameter를 적었다면, 다시 md5sum을 통해 체크섬을 만들어주어야 합니다. 이에 대한 내용은 앞선 [포스트]({% post_url 2020-01-05-unattended-ubuntu %})에서도 확인할 수 있으니 bakeIsoImage.sh 스크립트를 참조하여 md5sum을 하고 iso 파일을 만들면 됩니다.\n"
},
{
	"uri": "http://kimmj.github.io/ubuntu/how-to-use-sudo-without-password/",
	"title": "sudo를 password 없이 사용하기",
	"tags": ["sudo", "passwordless", "ubuntu"],
	"description": "",
	"content": "/etc/sudoers는 sudo를 사용할 수 있는 파일입니다. 이 파일을 열어보면 다음과 같은 글이 적혀 있습니다.\n Please consider adding local content in /etc/sudoers.d/ instead of directly modifying this file\n 즉, 직접 이 파일을 수정해서 sudo 권한을 주지 말고, /etc/sudoers.d/ 폴더 내에 파일을 추가하라는 의미입니다.\n이 곳에는 /etc/sudoers와 마찬가지로 계정에 대한 설정을 추가할 수 있습니다. 그리고 /etc/sudoers에서는 \u0026ldquo;NOPASSWD\u0026quot;라는 옵션을 주어 password없이 타 계정의 권한을 가지게 만들 수 있습니다.\n이 두가지를 종합하여 내 linux 계정이 sudo 명령어를 입력할 때, 즉 root 권한을 가지게 될 때 password를 입력하지 않도록 설정할 수 있습니다.\nexport ACCOUNT=$(whoami) echo \u0026#34;$ACCOUNTALL = (root) NOPASSWD:ALL\u0026#34; | sudo tee /etc/sudoers.d/$ACCOUNT sudo chmod 0440 /etc/sudoers.d/$ACCOUNT 이제 sudo 명령어를 쳐도 더 이상 password를 입력하라는 출력이 뜨지 않습니다.\n"
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/12-end-to-end-tests-on-a-kubernetes-cluster/",
	"title": "12 End to End Tests on a Kubernetes Cluster",
	"tags": ["kubernetse", "cka"],
	"description": "",
	"content": "End to End Tests  Test - Manual  kubectl get nodes kubectl get pods -n kube-system이 잘 되는지 service kube-apiserver status service kube-controller-manager status service kube-scheduler status service kubelet status service kube-proxy status kubectl run nginx kubectl expose deployment nginx --port=80 --type=NodePort   Kubernetes에는 test-infra라는 프로젝트가 있다.  약 1000개의 테스트를 해줌.   sonobuoy  End to End Tests - Run and Anlyze  go get으로 소스를 받고 skeleton으로 하여 op-prem을 테스트할 수 있다. 그 다음 테스트 명령어를 입력하여 테스트를 할 수 있고 12시간정도 걸린다.  Smoke test  일일히 입력하여 테스트하는 방법.  "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/13-troubleshooting/",
	"title": "13 Troubleshooting",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Application Failure  Appllication failure의 root cause 검색  application에 대한 map을 작성한 다음 root cause를 찾는다.    "
},
{
	"uri": "http://kimmj.github.io/kubernetes/cka-study/14-other-topics/",
	"title": "14 Other Topics",
	"tags": ["kubernetes", "cka"],
	"description": "",
	"content": "Advanced Kubectl Commands  JSON PATH를 사용하는 이유는 많은 정보들 속에서 특정한 데이터를 필터링하기 위함이다. kubectl command를 사용하면 kube-apiserver와 통신한다.  이는 json 포멧으로 kubectl에게 응답하고, 이를 사람이 읽을 수 있게 보여주는 것이다. detail을 보려면 -o wide를 주면 된다.   JSON PATH를 사용하면 보고싶은 결과만 볼 수 있다. 이를 사용하려면 다음과 같은 절차가 필요하다.  kubectl command로 raw data를 볼 수 있어야 한다. -o json을 통해 json output을 확인할 수 있어야 한다. JSON PATH query를 작성한다. -o=jsonpath=를 통해 query를 전달한다. 이 때 '{ }'로 감싸야 한다.   이 때 여러개의 query를 넣을 수도 있다. 중간에 {\u0026quot;\\n\u0026quot;}을 넣어서 이름과 데이터를 구분할 수 있다. Loop을 사용하여 루프를 돌 수 있다.  {range .items[*]}를 사용하면 된다. 마지막에 {end}를 넣는다. 또한 custom-columns도 사용할 수 있다.  -o=custom-columns=\u0026lt;COMLUMN NAME\u0026gt;:\u0026lt;JSON PATH\u0026gt; -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu     Sort도 사용할 수 있다.  --sort-by=\u0026lt;query\u0026gt;    "
},
{
	"uri": "http://kimmj.github.io/tags/docker-registry/",
	"title": "docker-registry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/harbor/",
	"title": "harbor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/certified-kubernetes-administrator/",
	"title": "Certified-Kubernetes-Administrator",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/cka/",
	"title": "CKA",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/kubernetse/",
	"title": "kubernetse",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/cks/",
	"title": "cks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/jenkins/",
	"title": "jenkins",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/workspace/",
	"title": "workspace",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/service/",
	"title": "service",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/targetport/",
	"title": "targetport",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/docker/",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/insecure-registry/",
	"title": "insecure-registry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/login-message/",
	"title": "login-message",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/motd/",
	"title": "motd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu/",
	"title": "ubuntu",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/bgimg-darken/",
	"title": "bgimg-darken",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/css/",
	"title": "css",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/google-analytics/",
	"title": "google-analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/git/",
	"title": "git",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/git-secret/",
	"title": "git-secret",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/github/",
	"title": "github",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/reboot/",
	"title": "reboot",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/tmux/",
	"title": "tmux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/sudo/",
	"title": "sudo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/oh-my-zsh/",
	"title": "oh-my-zsh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/zsh/",
	"title": "zsh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/base64/",
	"title": "base64",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/editor/",
	"title": "editor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/file/",
	"title": "file",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/log/",
	"title": "log",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/stern/",
	"title": "stern",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/gitignore/",
	"title": "gitignore",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/linux/",
	"title": "linux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/port/",
	"title": "port",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pipe/",
	"title": "pipe",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/watch/",
	"title": "watch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/alias/",
	"title": "alias",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/bridge/",
	"title": "bridge",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/container/",
	"title": "container",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/docker-compose/",
	"title": "docker-compose",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/passwordless/",
	"title": "passwordless",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ssh/",
	"title": "ssh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/tunneling/",
	"title": "tunneling",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/install/",
	"title": "install",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/infrastructure-as-code/",
	"title": "infrastructure-as-code",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/blue-green/",
	"title": "blue-green",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/canary/",
	"title": "canary",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/cicd/",
	"title": "cicd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/deploy/",
	"title": "deploy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/roll-out/",
	"title": "roll-out",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pod/",
	"title": "pod",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/concepts/",
	"title": "concepts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/prometheus/",
	"title": "prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/utterance/",
	"title": "utterance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/federation/",
	"title": "federation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/canary-update/",
	"title": "canary-update",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/spinnaker/",
	"title": "spinnaker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/air-gaped/",
	"title": "air-gaped",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/minio/",
	"title": "minio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/instll/",
	"title": "instll",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/kubeadm/",
	"title": "kubeadm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/overview/",
	"title": "overview",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/hostname/",
	"title": "hostname",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/font/",
	"title": "font",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/halyard/",
	"title": "halyard",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/proxy/",
	"title": "proxy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/netplan/",
	"title": "netplan",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/static-ip/",
	"title": "static-ip",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu-18.04/",
	"title": "ubuntu-18.04",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/html/",
	"title": "html",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/pipeline/",
	"title": "pipeline",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ansible/",
	"title": "ansible",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/libvirt/",
	"title": "libvirt",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/preseed.cfg/",
	"title": "preseed.cfg",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/ubuntu-18.04-server/",
	"title": "ubuntu-18.04-server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/tags/boot-parameter/",
	"title": "boot parameter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://kimmj.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]